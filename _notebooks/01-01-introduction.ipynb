{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction\n",
    "============\n",
    "\n",
    "### [Neil D. Lawrence](http://inverseprobability.com)\n",
    "\n",
    "### 2021-01-21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abstract**: This lecture will give the background to what this course\n",
    "is about, and how it fits in to other material you can find on deep\n",
    "neural network models. It explains deep neural networks are, how they\n",
    "fit into the wider context of the field and why they are successful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Do not edit this file locally. -->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!---->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->\n",
    "<!--\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Course Overview\n",
    "---------------\n",
    "\n",
    "<span style=\"text-align:right\"><span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_deepnn/includes/overview-2020.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_deepnn/includes/overview-2020.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span></span>\n",
    "\n",
    "Deep Neural Networks is an eight we course that introduces you to the\n",
    "fundamental concepts behind deep learning with neural networks. Over the\n",
    "last decade, deep neural network models have been behind some of the\n",
    "most impressive feats in machine learning. From the [convolutional\n",
    "neural networks](https://en.wikipedia.org/wiki/AlexNet) that made\n",
    "breakthrough progess on the [ImageNet\n",
    "challenge](https://en.wikipedia.org/wiki/ImageNet), object detection and\n",
    "the image net challenge, and most recently with the [CASP14 Protein\n",
    "Folding result from\n",
    "DeepMind](https://predictioncenter.org/casp14/doc/CASP14_press_release.html).\n",
    "\n",
    "Welcome to this course, which is designed to introduce you to the\n",
    "principles and ideas behind deep neural network models.\n",
    "\n",
    "Due to widespread international interest in neural networks there is now\n",
    "a great deal of material to help you train and deploy these models. It\n",
    "is not our aim to substitute this material but to augment it with deeper\n",
    "understanding of the reasons why deep learning is successful and the\n",
    "context in which that success has occurred.\n",
    "\n",
    "You are strongly encouraged to explore that material. For example, you\n",
    "can find [Yann LeCun’s neural networks course from NYU\n",
    "here](https://atcold.github.io/pytorch-Deep-Learning/). For example,\n",
    "[here is their introductory\n",
    "session](https://atcold.github.io/pytorch-Deep-Learning/) with Yann\n",
    "giving the first lecture available here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.lib.display import YouTubeVideo\n",
    "YouTubeVideo('0bMe_vCZo30')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure: <i>Lecture from the NYU course on Deep Learning given by Yann\n",
    "LeCun, Alfredo Canziani and Mark Goldstein</i>\n",
    "\n",
    "The full playlist is [available\n",
    "here](https://www.youtube.com/playlist?list=PLLHTzKZzVU9eaEyErdV26ikyolxOsz6mq).\n",
    "\n",
    "<svg viewBox=\"0 0 200 200\" style=\"width:15%\">\n",
    "\n",
    "<defs> <clipPath id=\"clip0\">\n",
    "\n",
    "<style>\n",
    "circle {\n",
    "  fill: black;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<circle cx=\"100\" cy=\"100\" r=\"100\"/> </clipPath> </defs>\n",
    "\n",
    "<title>\n",
    "\n",
    "Ferenc Huszar\n",
    "\n",
    "</title>\n",
    "\n",
    "<image preserveAspectRatio=\"xMinYMin slice\" width=\"100%\" xlink:href=\"../slides/diagrams/people/ferenc-huszar.jpg\" clip-path=\"url(#clip0)\"/>\n",
    "\n",
    "</svg>\n",
    "<svg viewBox=\"0 0 200 200\" style=\"width:15%\">\n",
    "\n",
    "<defs> <clipPath id=\"clip1\">\n",
    "\n",
    "<style>\n",
    "circle {\n",
    "  fill: black;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<circle cx=\"100\" cy=\"100\" r=\"100\"/> </clipPath> </defs>\n",
    "\n",
    "<title>\n",
    "\n",
    "Nic Lane\n",
    "\n",
    "</title>\n",
    "\n",
    "<image preserveAspectRatio=\"xMinYMin slice\" width=\"100%\" xlink:href=\"../slides/diagrams/people/nic-lane.jpg\" clip-path=\"url(#clip1)\"/>\n",
    "\n",
    "</svg>\n",
    "<svg viewBox=\"0 0 200 200\" style=\"width:15%\">\n",
    "\n",
    "<defs> <clipPath id=\"clip2\">\n",
    "\n",
    "<style>\n",
    "circle {\n",
    "  fill: black;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<circle cx=\"100\" cy=\"100\" r=\"100\"/> </clipPath> </defs>\n",
    "\n",
    "<title>\n",
    "\n",
    "Neil Lawrence\n",
    "\n",
    "</title>\n",
    "\n",
    "<image preserveAspectRatio=\"xMinYMin slice\" width=\"100%\" xlink:href=\"../slides/diagrams/people/neil-lawrence.png\" clip-path=\"url(#clip2)\"/>\n",
    "\n",
    "</svg>\n",
    "\n",
    "Our course is taught by Ferenc Huszár, Nic Lane and Neil Lawrence.\n",
    "\n",
    "Alongside such teaching material there are frameworks such as PyTorch,\n",
    "that help you in constructing these models and deploying the training on\n",
    "GPUs.\n",
    "\n",
    "Neural network models are not new, and this is not the first wave of\n",
    "interest in them, it is the third wave. There are reasons why they have\n",
    "become more successful in this wave than in their first two\n",
    "incarnations. Those reasons include the context in which they were\n",
    "reintroduced. In particular, the wide availability of *data* and *fast\n",
    "compute* have been critical in their success.\n",
    "\n",
    "Our aim is to give you the understanding of that context, and therefore\n",
    "deepen your understanding of neural networks, how and *when* they should\n",
    "be used. The neural network is not a panacea, it does not solve all the\n",
    "challenges of machine learning (at least not in its current\n",
    "incarnation). But it is a powerful framework for flexible modelling of\n",
    "data. Just like any powerful framework, it is important to understand\n",
    "its strengths as well as its limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schedule\n",
    "--------\n",
    "\n",
    "-   Week 1:\n",
    "    1.  Introduction and Data. Lecturer: [Neil D.\n",
    "        Lawrence](http://inverseprobability.com/)\n",
    "    2.  Generalisation and Neural Networks. Lectuer: [Neil D.\n",
    "        Lawrence](http://inverseprobability.com/)\n",
    "-   Week 2:\n",
    "    1.  Automatic Differentiation. Lecturer: [Ferenc\n",
    "        Huszár](https://www.inference.vc/about/)\n",
    "    2.  Optimization and Stochastic Gradient Descent. Lecturer: [Ferenc\n",
    "        Huszár](https://www.inference.vc/about/)\n",
    "-   Week 3:\n",
    "    1.  Hardware. Lecturer: [Nic Lane](http://niclane.org/)\n",
    "    2.  Summary and Gap Filling: [Ferenc\n",
    "        Huszár](https://www.inference.vc/about/), [Neil D.\n",
    "        Lawrence](http://inverseprobability.com/), [Nic\n",
    "        Lane](http://niclane.org/)\n",
    "\n",
    "**Set Assignment 1 (30%)**\n",
    "\n",
    "-   Week 4:\n",
    "    1.  Convolutional Neural Networks: [Nic Lane](http://niclane.org/)\n",
    "    2.  Recurrent Neural Networks: [Ferenc\n",
    "        Huszár](https://www.inference.vc/about/)\n",
    "\n",
    "**Assignment 1 Submitted**\n",
    "\n",
    "-   Week 5:\n",
    "    1.  Sequence to Sequence and Attention: [Ferenc\n",
    "        Huszár](https://www.inference.vc/about/)\n",
    "    2.  Transformers: [Nic Lane](http://niclane.org/)\n",
    "\n",
    "**Set Assignment 2 (70%)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Special Topics\n",
    "--------------\n",
    "\n",
    "Weeks 6-8 will involve a series of guest lectures and discussion\n",
    "sessions to relate the fundamental material you’ve learnt about to how\n",
    "people are deploying these models in the real world and how they are\n",
    "innovating with these models to extend their capabilities.\n",
    "\n",
    "The two assignments will make up the entire mark of the module. They\n",
    "should be submitted via Moodle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is Machine Learning?\n",
    "=========================\n",
    "\n",
    "<span style=\"text-align:right\"><span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/what-is-ml.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/what-is-ml.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span></span>\n",
    "\n",
    "What is machine learning? At its most basic level machine learning is a\n",
    "combination of\n",
    "\n",
    "$$\\text{data} + \\text{model} \\stackrel{\\text{compute}}{\\rightarrow} \\text{prediction}$$\n",
    "\n",
    "where *data* is our observations. They can be actively or passively\n",
    "acquired (meta-data). The *model* contains our assumptions, based on\n",
    "previous experience. That experience can be other data, it can come from\n",
    "transfer learning, or it can merely be our beliefs about the\n",
    "regularities of the universe. In humans our models include our inductive\n",
    "biases. The *prediction* is an action to be taken or a categorization or\n",
    "a quality score. The reason that machine learning has become a mainstay\n",
    "of artificial intelligence is the importance of predictions in\n",
    "artificial intelligence. The data and the model are combined through\n",
    "computation.\n",
    "\n",
    "In practice we normally perform machine learning using two functions. To\n",
    "combine data with a model we typically make use of:\n",
    "\n",
    "**a prediction function** a function which is used to make the\n",
    "predictions. It includes our beliefs about the regularities of the\n",
    "universe, our assumptions about how the world works, e.g. smoothness,\n",
    "spatial similarities, temporal similarities.\n",
    "\n",
    "**an objective function** a function which defines the cost of\n",
    "misprediction. Typically it includes knowledge about the world’s\n",
    "generating processes (probabilistic objectives) or the costs we pay for\n",
    "mispredictions (empiricial risk minimization).\n",
    "\n",
    "The combination of data and model through the prediction function and\n",
    "the objective function leads to a *learning algorithm*. The class of\n",
    "prediction functions and objective functions we can make use of is\n",
    "restricted by the algorithms they lead to. If the prediction function or\n",
    "the objective function are too complex, then it can be difficult to find\n",
    "an appropriate learning algorithm. Much of the acdemic field of machine\n",
    "learning is the quest for new learning algorithms that allow us to bring\n",
    "different types of models and data together.\n",
    "\n",
    "A useful reference for state of the art in machine learning is the UK\n",
    "Royal Society Report, [Machine Learning: Power and Promise of Computers\n",
    "that Learn by\n",
    "Example](https://royalsociety.org/~/media/policy/projects/machine-learning/publications/machine-learning-report.pdf).\n",
    "\n",
    "You can also check my post blog post on [What is Machine\n",
    "Learning?](http://inverseprobability.com/2017/07/17/what-is-machine-learning).."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ingredients\n",
    "-----------\n",
    "\n",
    "<span style=\"text-align:right\"><span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/introduction.mdtmp.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/introduction.mdtmp.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span></span>\n",
    "\n",
    "So the three key ingredients of machine learning are a model, data and\n",
    "compute. Note, that this necessarily implies that an *algorithm* exists\n",
    "to combine the model with the data and that algorithm is what consumes\n",
    "the compute.\n",
    "\n",
    "So how do these ingredients pan out in our recipe for deep learning? To\n",
    "better understand this, we’re going to add more historical context and\n",
    "go back go 1997 when, here in Cambridge, there was a six month programme\n",
    "at the Isaac Newton Institute run on Machine Learning, Neural Networks\n",
    "and Generalisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cybernetics, Neural Networks and the Ratio Club\n",
    "-----------------------------------------------\n",
    "\n",
    "<span style=\"text-align:right\"><span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/cybernetics-ratio-club.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/cybernetics-ratio-club.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span></span>\n",
    "\n",
    "This is certainly not the first wave of excitment in neural networks.\n",
    "This history of neural networks predates the history of the computer,\n",
    "and papers on neural networks predate papers on the digital computer.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"30%\">\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/philosophy/Bertrand_Russell_1957.jpg\" style=\"width:100%\">\n",
    "\n",
    "</td>\n",
    "<td width=\"30%\">\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/ml/Lettvin_Pitts.jpg\" style=\"width:100%\">\n",
    "\n",
    "</td>\n",
    "<td width=\"30%\">\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/ml/warren_mcculloch.jpg\" style=\"width:100%\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Figure: <i>Bertrand Russell (1872-1970), Walter Pitts, *right*\n",
    "(1923-1969), Warren McCulloch (1898-1969)</i>\n",
    "\n",
    "Specifically, one of the first papers on neural networks was written by\n",
    "two collaborators from logic and psychology in 1943. [Walter\n",
    "Pitts](https://en.wikipedia.org/wiki/Walter_Pitts) was a child prodigy\n",
    "who read Russell and Whitehead’s *Principia Mathematica*. He felt he’d\n",
    "spotted some errors in the text and wrote to Russell in Cambridge, who\n",
    "replied inviting him for a visit. Pitts did not take up the offer\n",
    "because he was only 12 years old. But, three years later, when Russell\n",
    "took a sabbatical at the University of Chicago, Pitts left his home in\n",
    "Detroit and headed to Chicago to hear Russell speak. When Russell left\n",
    "Pitts stayed on studying logic. He never formally took a degree but just\n",
    "worked with whoever was available.\n",
    "\n",
    "[Warren\n",
    "McCulloch](https://en.wikipedia.org/wiki/Warren_Sturgis_McCulloch) was a\n",
    "psychologist who moved to the University of Chicago in 1941. Overlapping\n",
    "interests meant that he met Pitts, who was still living an intinerant\n",
    "lifestyle around the campus of the University. McCulloch invited Pitts\n",
    "to live with his family and they began collaborating on a simple model\n",
    "of the neuron and how neurons might interact. The dominant ‘theory of\n",
    "knowledge’ at the time was *logic* and their paper attempted to show how\n",
    "networks of neurons (or ir paper attempted to bridge the logical\n",
    "foundation. Their paper, *A Logical Calculus of the Ideas Immanent in\n",
    "Nervous Activity* (McCulloch and Pitts, 1943) was published in the\n",
    "middle of the Second World War. It modelled the neuron as a linear\n",
    "threshold, and described how networks of such neurons could create\n",
    "logical functions. The inspiration in the paper is clear, they make use\n",
    "of Rudolf Carmap’s Language II (Carnap, 1937) to represent their theorem\n",
    "and cite the second edition of Russell and Whitehead (Russell and\n",
    "Whitehead, 1925)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cybernetics\n",
    "-----------\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"30%\">\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/physics/james-clerk-maxwell.png\" style=\"width:100%\">\n",
    "\n",
    "</td>\n",
    "<td width=\"30%\">\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/physics/j-w-gibbs.jpg\" style=\"width:100%\">\n",
    "\n",
    "</td>\n",
    "<td width=\"30%\">\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/physics/Norbert_wiener.jpg\" style=\"width:100%\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Figure: <i>James Clerk Maxwell (1831-1879), Josiah Willard Gibbs\n",
    "(1839-1903), Norbert Wiener (1894-1964)</i>\n",
    "\n",
    "After the war, this work, along with McCulloch and Pitts, was at the\n",
    "heart of a movement known as *Cybernetics*. A term coined by [Norbert\n",
    "Wiener](https://en.wikipedia.org/wiki/Norbert_Wiener) (Wiener, 1948) to\n",
    "reflect the wealth of work on sensing and computing. Wiener chose the\n",
    "term as an alternative rendering of the word *governor*. Governor comes\n",
    "to us form latin, but is a corruption of the Greek κυβερνήτης meaning\n",
    "helmsman. Wiener’s choice of the term was a nod to the importance of\n",
    "James Clerk Maxwell’s work on understanding surging in James Watt’s\n",
    "steam engine governor (Maxwell, 1867). It reflected the importance that\n",
    "Wiener placed on *feedback* in these systems. From this strand of work\n",
    "came the field of *control theory*.\n",
    "\n",
    "Many of the constituent ideas of Cybernetics came from the war itself.\n",
    "Norbert Wiener, was a Professor of Applied Mathematics at MIT. He was\n",
    "another child prodigy who visited Russell in Cambridge having completed\n",
    "his PhD at Harvard by the age of 19. But Wiener was less keen on logic\n",
    "than McCulloch and Pitts, he looked to stochastic processes and\n",
    "probability theory as the key to intelligent decision making. He\n",
    "rejected the need for a ‘theory of knowledge’ and preferred to think of\n",
    "a ‘theory of ignorance’ which was inspired by statistical mechanics,\n",
    "Maxwell was also a originator of the field, but Wiener wrote of Josiah\n",
    "Willard Gibbs as being his inspiration.\n",
    "\n",
    "This nascent community was mainly based on those who were involved in\n",
    "war work. Wiener worked on radar systems for tracking aircraft (leading\n",
    "to the Wiener filter (Wiener, 1949)). In the UK researchers such as Jack\n",
    "Good, Alan Turing, Donald MacKay, Ross Ashby formed the *Ratio Club*. A\n",
    "group of scientists interested in how the brain works and how it might\n",
    "be modelled. Many of these scientists also worked on radar systems or\n",
    "code breaking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analogue and Digital\n",
    "--------------------\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/physics/dmaccrimmonmackay.jpg\" style=\"width:30%\">\n",
    "\n",
    "Figure: <i>Donald M. MacKay (1922-1987), an early member of the\n",
    "Cybernetics community and member of the Ratio Club.</i>\n",
    "\n",
    "Donald MacKay wrote of the influence that his own work on radar had on\n",
    "his interest in the brain.\n",
    "\n",
    "> … during the war I had worked on the theory of automated and\n",
    "> electronic computing and on the theory of information, all of which\n",
    "> are highly relevant to such things as automatic pilots and automatic\n",
    "> gun direction. I found myself grappling with problems in the design of\n",
    "> artificial sense organs for naval gun-directors and with the\n",
    "> principles on which electronic circuits could be used to simulate\n",
    "> situations in the external world so as to provide goal-directed\n",
    "> guidance for ships, aircraft, missiles and the like.\n",
    ">\n",
    "> Later in the 1940’s, when I was doing my Ph.D. work, there was much\n",
    "> talk of the brain as a computer and of the early digital computers\n",
    "> that were just making the headlines as “electronic brains.” As an\n",
    "> analogue computer man I felt strongly convinced that the brain,\n",
    "> whatever it was, was not a digital computer. I didn’t think it was an\n",
    "> analogue computer either in the conventional sense.\n",
    ">\n",
    "> But this naturally rubbed under my skin the question: well, if it is\n",
    "> not either of these, what kind of system is it? Is there any way of\n",
    "> following through the kind of analysis that is appropriate to their\n",
    "> artificial automata so as to understand better the kind of system the\n",
    "> human brain is? That was the beginning of my slippery slope into brain\n",
    "> research.\n",
    ">\n",
    "> *Behind the Eye* pg 40. Edited version of The 1986 Gifford Lectures\n",
    "> given by Donald M. MacKay and edited by Valerie MacKay\n",
    "\n",
    "Importantly, MacKay distinguishes between the *analogue* computer and\n",
    "the *digital* computer. As he mentions, his experience was with analogue\n",
    "machines. An analogue machine is *literally* an analogue. The radar\n",
    "systems that Wiener and MacKay both worked on were made up of electronic\n",
    "components such as resistors, capacitors and inductors, that together\n",
    "represented a physical system, such as an anti-aircraft gun and a plane.\n",
    "The design of the analogue computer required the engineer to simulate\n",
    "the real world in analogue electronics, using dualities that exist\n",
    "between e.g. mechanical circuits (mass, spring, damper) and electroni\n",
    "circuits (inductor, resistor, capacitor). The analogy between mass and a\n",
    "damper, between spring and a resistor and between capacitor and a damper\n",
    "works because the underlying mathematics is approximated with the same\n",
    "linear system: a second order differential equation. This mathematical\n",
    "analogy allowed the designer to map from the real world, through\n",
    "mathematics, to a virtual world where the components reflected the real\n",
    "world through analogy.\n",
    "\n",
    "This is a quite different from the approach that McCulloch and Pitts\n",
    "were taking with their paper on the logical calculus of the nervous\n",
    "system. They were attempting to map their model of the neuron onto\n",
    "logic. Logical reasoning was the mainstay of the contemporary\n",
    "understanding of intelligence. But the components they were considering\n",
    "were neurons, they could only map onto the logical world because their\n",
    "analogy for the neuron was so simple. An ‘on’ or ‘off’ linear threshold\n",
    "unit. Where the synapses of the neuron were compared to a threshold in\n",
    "the neuron. Firing occurs when the sum of input neurons cross a\n",
    "threshold in the receiving neuron. These networks can then be built\n",
    "together in cascades.\n",
    "\n",
    "In the late 1940s and early 1950s, Cyberneticists were also working on\n",
    "*digital computers*. The type of machines (such as Colossus, built by\n",
    "Tommy Flowers, and known to Turing) and the ENIAC. Indeed, by 1949\n",
    "Cambridge had built its own machine, the EDSAC in the Mathematical\n",
    "Laboratory, inspired by “draft of a report on the EDVAC” by von Neumann.\n",
    "\n",
    "Digital computers are themselves (at their heart) a series of analogue\n",
    "devices, but in the digital computer, the analogue is between\n",
    "collections of transistors and logic gates. Logic gates allow the\n",
    "computer to reconstruct *logical truth tables*, which Wittgenstein had\n",
    "popularised in his *Tractatus Logico-Philosophicus* (Wittgenstein,\n",
    "1922). The mathematical operations we need can be reconstructed through\n",
    "logic, so this ‘analogue machine’ which we call a *digital computer*\n",
    "becomes capable of directly computing the mathematics we’re interested\n",
    "in, rather than going via the analogue machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Perceptron\n",
    "--------------\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"30%\">\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/ml/w-ross-ashby.jpg\" style=\"width:100%\">\n",
    "\n",
    "</td>\n",
    "<td width=\"30%\">\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/physics/JohnvonNeumann-LosAlamos.gif\" style=\"width:100%\">\n",
    "\n",
    "</td>\n",
    "<td width=\"30%\">\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/Frank_Rosenblatt.jpg\" style=\"width:100%\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Figure: <i>W. Ross Ashby (1903-1972), John von Neumann (1903-1957),\n",
    "Frank Rosenblatt (1928-1971). *Photograph of W. Ross Ashby is Copyright\n",
    "W. Ross Ashby*.</i>\n",
    "\n",
    "So the early story of Cybernetics starts with the success of analogue\n",
    "control, and in the domain of neural networks, analogues built. Inspired\n",
    "by F. Ross Ashby’s (Ashby, 1952) ideas that suggested *random\n",
    "connections* and von Neumann (Neumann, 1956), who wrote about\n",
    "*probabilistic logics*, Frank Rosenblatt constructed the Perceptron\n",
    "(Rosenblatt, 1958). This was a deep neural network that recognised\n",
    "images from TV cameras.\n",
    "\n",
    "The perceptron created a great deal of interest, but it was rapidly\n",
    "eclipsed by the emerging *digital computer*. Perhaps as a marker as the\n",
    "increasing importance of the digital computer, the Apollo program\n",
    "(1961-1972) had a guidance computer that was a 16 bit digital machine\n",
    "for navigating to the moon. It implemented Kalman filters for guidance.\n",
    "In signal processing there’s a shift from the 1950s to the 1960s of\n",
    "reseachers moving from analogue designs to designs that are suitable for\n",
    "implementation of digital machines.\n",
    "\n",
    "That same shift was imposed on the Cybernetics community. Artificial\n",
    "Inteligence is often traced to the ‘summer research project’ proposed by\n",
    "John McCarthy at Dartmouth. But the proposal is not just notable for the\n",
    "introduction of the term, it is notable for the extent to which it marks\n",
    "a break with the Cybernetics community. Wiener’s name isn’t even\n",
    "mentioned in the proposal. And Cyberneticists either weren’t invited to,\n",
    "or couldn’t make, the event (with the notable exception of Warren\n",
    "McCulloch). Turing had died, and von Neumann was seriously ill with\n",
    "cancer. W. Ross Ashby was even in the US, but from his diaries there’s\n",
    "no trace of him attending. Donald MacKay was on the initial proposal,\n",
    "but didn’t make the event (perhaps because that summer his son,\n",
    "[Robert](https://warwick.ac.uk/fac/sci/maths/people/staff/robert_mackay/),\n",
    "was born).\n",
    "\n",
    "One of the great ironies of modern artificial intelligence is that it is\n",
    "almost wholly reliant on deep neural network methodologies for the\n",
    "recent breakthroughs. But the dawn of the term artificial intelligence\n",
    "is associated with a period of around three decades when those methods\n",
    "(and the community that originated them) was actively marginalised.\n",
    "\n",
    "There were many reasons for this, including personal enimities between\n",
    "Wiener and McCulloch, the untimely death of Frank Rosenblatt in a\n",
    "sailing accident. But regardless of these personal tragedies and tales\n",
    "of academic politics, the principal reason that neural networks were\n",
    "eclipsed was the dominance of the digital computer. From 1956 to 1986 we\n",
    "saw the rise of the computer from a tool for science and big business to\n",
    "a personal machine, available to individual researchers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Connectionists\n",
    "------------------\n",
    "\n",
    "<span style=\"text-align:right\"><span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/connectionist-revival.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/connectionist-revival.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span></span>\n",
    "\n",
    "By the early 1980s, some disillusionment was creeping into the\n",
    "artificial intelligence agenda that stemmed from the Dartmouth Meeting.\n",
    "At the same time, an emerging group was re-examining the ideas of the\n",
    "Cyberneticists. This group became known as the connectionists, because\n",
    "of their focus on neural network models with their myriad of\n",
    "connections.\n",
    "\n",
    "By the second half of the decade, some of their principles had come\n",
    "together to form a movement. Meetings including the Snowbird Workshop,\n",
    "Neural Informaton Processing Systems and the connectionist summer school\n",
    "provided a venue for these researchers to come together.\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/ml/connectionist-summer-school.jpg\" style=\"width:100%\">\n",
    "\n",
    "Figure: <i>Group photo from the 1986 Connectionists’ Summer School, held\n",
    "at CMU in July. Included in the photo are Richard Durbin, Terry\n",
    "Sejnowski, Geoff Hinton, Yann LeCun, Michael I. Jordan.</i>\n",
    "\n",
    "This was also the era of cheap(er) computing. Connectionists were able\n",
    "to implement *simulators* of neural networks in *minicomputers* such as\n",
    "DEC’s PDP-11 series. These allowed new architectures to be tried. Among\n",
    "them was backpropagation, popularised by the “Parallel Distributed\n",
    "Processing” books \\[Rumelhart:book86\\], these books formed the canonical\n",
    "ideas on which the connectionists based their research.\n",
    "\n",
    "> What makes people smarter than machines? They certainly are not\n",
    "> quicker or more precise. Yet people are far better at perceiving\n",
    "> objects in natural scenes and noting their relations, at understanding\n",
    "> language and retrieving contextually appropriate information from\n",
    "> memory, at making plans and carrying out contextually appropriate\n",
    "> actions, and at a wide range of other natural cognitive tasks. People\n",
    "> are also far better at learning to do these things more accurately and\n",
    "> fluently through processing experience.\n",
    ">\n",
    "> What is the basis for these differences? One answer, perhaps the\n",
    "> classic one we might expect from artificial intelligence, is\n",
    "> “software.” If we only had the right computer program, the argument\n",
    "> goes, we might be able to capture the fluidity and adaptability of\n",
    "> human information processing.\n",
    ">\n",
    "> Certainly this answer is partially correct. There have been great\n",
    "> breakthroughs in our understanding of cognition as a result of the\n",
    "> development of expressive high-level computer languages and powerful\n",
    "> algorithms. No doubt there will be more such breakthroughs in the\n",
    "> future. However, we do not think that software is the whole story.\n",
    ">\n",
    "> In our view, people are smarter than today’s computers because the\n",
    "> brain employs a basic computational architecture that is more suited\n",
    "> to deal with a central aspect of the natural information processing\n",
    "> tasks that people are so good at.\n",
    ">\n",
    "> J. L. McClelland, David E. Rumelhart and Geoffrey E. Hinton in\n",
    "> *Parallel Distributed Processing,* Rumelhart et al. (1986)\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/ml/pdp_cover.jpg\" style=\"width:40%\">\n",
    "\n",
    "Figure: <i>Cover of the Parallel Distributed Processing edited volume\n",
    "(Rumelhart et al., 1986).</i>\n",
    "\n",
    "This led to the second wave of neural network architectures. A new\n",
    "journal, *Neural Computation* was launched in 1989 to cater for this new\n",
    "field. It’s first volume contained a new architecture: the convolutional\n",
    "neural networks (Le Cun et al., 1989), developed for recognising hand\n",
    "written digits. It made the cover.\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/ml/nc_cover.jpg\" style=\"width:40%\">\n",
    "\n",
    "Figure: <i>Cover of *Neural Computation*, Volume 1, Issue 4 containing\n",
    "Le Cun et al. (1989). The cover shows examples from the U.S. Postal\n",
    "Service data set of handwritten digits.</i>\n",
    "\n",
    "It’s worth noting what compute and data that LeCun and collaborators had\n",
    "avaialble. Experiments were run on a SUN-4/260, with a CPU running at\n",
    "16.67 MHz and 128 MB of RAM. It’s an impressive machine for the day. The\n",
    "neural network had just under 10,000 parameters and the training data\n",
    "consisted of 7,291 digitized training images on a 16 x 16 grid, 2007\n",
    "images were retained for testing. The model had a 5% error rate on the\n",
    "test data.\n",
    "\n",
    "> The first several stages of processing in our previous system\n",
    "> (described in Denker et al. 1989) involved convolutions in which the\n",
    "> coefficients had been laboriously hand designed. In the present\n",
    "> system, the first two layers of the network are constrained to be\n",
    "> convolutional, but the system automatically learns the coefficients\n",
    "> that make up the kernels.\n",
    ">\n",
    "> Section 5.1 in Le Cun et al. (1989)\n",
    "\n",
    "The second half of the 1980s and the 1990s were a period of growth and\n",
    "innovation for the community. Recurrent neural networks that operated\n",
    "through time were produced including in 1997, the Long Short-Term Memory\n",
    "architecture (Hochreiter and Schmidhuber, 1997), a form of recurrent\n",
    "neural network that can deal with sequence data.\n",
    "\n",
    "<!--include{_ml/includes/what-does-machine-learning-do.md}-->\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/people/1997-08-02-neil-newton-institute.jpg\" style=\"width:80%\">\n",
    "\n",
    "Figure: <i>Neil standing outside the Newton Institute on 2nd August\n",
    "1997, just after arriving for “Generalisation in Neural Networks and\n",
    "Machine Learning,” [see page 26-30 of this\n",
    "report](http://www.newton.ac.uk/files/reports/annual/ini_annual_report_97-98.pdf).</i>\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/ml/le-net-5.gif\" style=\"width:60%\">\n",
    "\n",
    "Figure: <i>Gif animation of LeNet-5 in action.</i>\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/ml/le-net-translate.gif\" style=\"width:60%\">\n",
    "\n",
    "Figure: <i>Gif animation of LeNet-5 in action. Here the translation\n",
    "invariance of the network is being tested.</i>\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/ml/le-net-scale.gif\" style=\"width:60%\">\n",
    "\n",
    "Figure: <i>Gif animation of LeNet-5 in action, here the scale invariance\n",
    "of the network is being tested.</i>\n",
    "\n",
    "But at the same meeting Vladmir Vapnik was there as was Bernhard\n",
    "Scholkopf. Corinna Cortes, Bernard Boser, Isabelle Guyon and Vladmir\n",
    "Vapnik were all instrumental in developing the Support Vector Machine.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"30%\">\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/ml/corinna-cortes.jpg\" style=\"width:100%\">\n",
    "\n",
    "</td>\n",
    "<td width=\"30%\">\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/ml/isabelle-guyon.jpg\" style=\"width:100%\">\n",
    "\n",
    "</td>\n",
    "<td width=\"30%\">\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/ml/vladmir-vapnik.jpg\" style=\"width:100%\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Figure: <i>Corinna Cortes, Isabelle Guyon and Vladmir Vapnik. Three of\n",
    "the key people behind the support vector machine Cortes and Vapnik\n",
    "(1995). All were based at Bell Labs in the 1990s.</i>\n",
    "\n",
    "Also attending the summer school at the Newton Institute was Bernhard\n",
    "Schölkopf. He had shown that, on the same USPS digits data set, the\n",
    "support vector machine was able to achieve an error of 4.2% (Schölkopf\n",
    "et al., 1997). It was also mathematically more elegant than the neural\n",
    "network approaches. Even on larger data sets, by incorporating the\n",
    "translation invariance (Schölkopf et al., n.d.), the support vector\n",
    "machine was able to achieve similar error rates to convolutional neural\n",
    "networks.\n",
    "\n",
    "> This work points out the necessity of having flexible “network design”\n",
    "> software tools that ease the design of complex, specialized network\n",
    "> architectures\n",
    ">\n",
    "> From conclusions of Le Cun et al. (1989)\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"50%\">\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/ml/olga-russakovsky.png\" style=\"width:100%\">\n",
    "\n",
    "</td>\n",
    "<td width=\"50%\">\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/ml/fei-fei-li.png\" style=\"width:100%\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Figure: <i>Olga Russakovsky, Fei Fei Li. Olga and Fei Fei led the\n",
    "creation of the ImageNet database that enabled convolutional neural\n",
    "networks to show their true potential. The data base contains millions\n",
    "of images.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Third Wave\n",
    "--------------\n",
    "\n",
    "-   Data (many data, many classes)\n",
    "-   Compute (GPUs)\n",
    "-   Stochastic Gradient Descent\n",
    "-   Software (autograd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Domains of Use\n",
    "--------------\n",
    "\n",
    "-   Perception and Representation\n",
    "    1.  Speech\n",
    "    2.  Vision\n",
    "    3.  Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experience\n",
    "----------\n",
    "\n",
    "-   Bringing it together:\n",
    "    -   Unsupervised pre-training\n",
    "    -   Initialisation and RELU\n",
    "    -   A Zoo of methods and models\n",
    "\n",
    "-   Why do they generalize well?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "----------\n",
    "\n",
    "-   Understand the principles behind:\n",
    "    -   Generalization\n",
    "    -   Optimization\n",
    "    -   Implementation (hardware)\n",
    "-   Differen NN Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks!\n",
    "-------\n",
    "\n",
    "For more information on these subjects and more you might want to check\n",
    "the following resources.\n",
    "\n",
    "-   twitter: [@lawrennd](https://twitter.com/lawrennd)\n",
    "-   podcast: [The Talking Machines](http://thetalkingmachines.com)\n",
    "-   newspaper: [Guardian Profile\n",
    "    Page](http://www.theguardian.com/profile/neil-lawrence)\n",
    "-   blog:\n",
    "    [http://inverseprobability.com](http://inverseprobability.com/blog.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ashby, W.R., 1952. Design for a brain: The origin of adaptive behaviour.\n",
    "Chapman & Hall.\n",
    "\n",
    "Boser, B.E., Guyon, I.M., Vapnik, V.N., 1992. A training algorithm for\n",
    "optimal margin classifiers, in: Proceedings of the Fifth Annual Workshop\n",
    "on Computational Learning Theory, COLT ’92. Association for Computing\n",
    "Machinery, New York, NY, USA, pp. 144–152.\n",
    "<https://doi.org/10.1145/130385.130401>\n",
    "\n",
    "Carnap, R., 1937. Locigal syntax of language. Routledge, Trench, Trubner\n",
    "& Co Ltd.\n",
    "\n",
    "Cortes, C., Vapnik, V.N., 1995. Support vector networks. Machine\n",
    "Learning 20, 273–297. <https://doi.org/10.1007/BF00994018>\n",
    "\n",
    "Hochreiter, S., Schmidhuber, J., 1997. Long short-term memory. Neural\n",
    "Computation 9, 1735–1780. <https://doi.org/10.1162/neco.1997.9.8.1735>\n",
    "\n",
    "Le Cun, Y., Boser, B.E., Denker, J.S., Henderson, D., Howard, R.E.,\n",
    "Hubbard, W., Jackel, L.D., 1989. Backpropagation applied to handwritten\n",
    "zip code recognition. Neural Computation 1, 541–551.\n",
    "<https://doi.org/10.1162/neco.1989.1.4.541>\n",
    "\n",
    "Maxwell, J.C., 1867. On governors. Proceedings of the Royal Society of\n",
    "London 16, 270–283.\n",
    "\n",
    "McCulloch, W.S., Pitts, W., 1943. A logical calculus of the ideas\n",
    "immanent in nervous activity. Bulletin of Mathematical Biophysics 5,\n",
    "115–133. <https://doi.org/10.1007/BF02478259>\n",
    "\n",
    "Neumann, J. von, 1956. Probabilistic logics and the synthesis of\n",
    "reliable organisms from unreliable components, in: Shannon, C.E.,\n",
    "McCarthy, J. (Eds.), Automata Studies. Princeton University Press,\n",
    "Princeton.\n",
    "\n",
    "Rosenblatt, F., 1958. The perceptron: A probabilistic model for\n",
    "information storage and organization in the brain. Psychological Review\n",
    "65, 386–408. <https://doi.org/10.1037/h0042519>\n",
    "\n",
    "Rumelhart, D.E., McClelland, J.L., the PDP Research Group, 1986.\n",
    "Parallel distributed programming: Explorations in the microstructure of\n",
    "cognition. mit, Cambridge, MA.\n",
    "\n",
    "Russell, B., Whitehead, A.N., 1925. Principia mathematica, 2nd ed.\n",
    "Cambridge University Press.\n",
    "\n",
    "Schölkopf, B., Burges, C.J.C., Vapnik, V.N., n.d. Incorporating\n",
    "invariances in support vector learning machines. pp. 47–52.\n",
    "<https://doi.org/10.1007/3-540-61510-5_12>\n",
    "\n",
    "Schölkopf, B., Sung, K.-K., Burges, C.J.C., Girosi, F., Niyogi, P.,\n",
    "Poggio, T., Vapnik, V.N., 1997. Comparing support vector machines with\n",
    "Gaussian kernels to radial basis function classifiers. IEEE Transactions\n",
    "on Signal Processing 45, 2758–2765. <https://doi.org/10.1109/78.650102>\n",
    "\n",
    "Wiener, N., 1949. The extrapolation, interpolation and smoothing of\n",
    "stationary time series with engineering applications. wiley.\n",
    "\n",
    "Wiener, N., 1948. Cybernetics: Control and communication in the animal\n",
    "and the machine. MIT Press, Cambridge, MA.\n",
    "\n",
    "Wittgenstein, L., 1922. Tractatus logico-philosophicus. Kegan Paul."
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
