{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalization and Neural Networks\n",
    "\n",
    "### [Neil D. Lawrence](http://inverseprobability.com)\n",
    "\n",
    "### 2022-01-25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abstract**: This lecture will cover generalization in machine learning\n",
    "with a particular focus on neural architectures. We will review\n",
    "classical generalization and explore what’s different about neural\n",
    "network models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.cell .markdown}\n",
    "\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!---->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->\n",
    "<!--\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
    "class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_deepnn/includes/deepnn-notebook-setup.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_deepnn/includes/deepnn-notebook-setup.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "First we download some libraries and files to support the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 22})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--setupplotcode{import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_context('paper')\n",
    "sns.set_palette('colorblind')}-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pods\n",
    "\n",
    "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
    "class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_software/includes/pods-software.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_software/includes/pods-software.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "In Sheffield we created a suite of software tools for ‘Open Data\n",
    "Science.’ Open data science is an approach to sharing code, models and\n",
    "data that should make it easier for companies, health professionals and\n",
    "scientists to gain access to data science techniques.\n",
    "\n",
    "You can also check this blog post on [Open Data\n",
    "Science](http://inverseprobability.com/2014/07/01/open-data-science).\n",
    "\n",
    "The software can be installed using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from the command prompt where you can access your python installation.\n",
    "\n",
    "The code is also available on GitHub: <https://github.com/lawrennd/ods>\n",
    "\n",
    "Once `pods` is installed, it can be imported in the usual manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## notutils\n",
    "\n",
    "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
    "class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_software/includes/notutils-software.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_software/includes/notutils-software.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "This small package is a helper package for various notebook utilities\n",
    "used\n",
    "\n",
    "The software can be installed using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install notutils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from the command prompt where you can access your python installation.\n",
    "\n",
    "The code is also available on GitHub:\n",
    "<https://github.com/lawrennd/notutils>\n",
    "\n",
    "Once `notutils` is installed, it can be imported in the usual manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import notutils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mlai\n",
    "\n",
    "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
    "class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_software/includes/mlai-software.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_software/includes/mlai-software.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "The `mlai` software is a suite of helper functions for teaching and\n",
    "demonstrating machine learning algorithms. It was first used in the\n",
    "Machine Learning and Adaptive Intelligence course in Sheffield in 2013.\n",
    "\n",
    "The software can be installed using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mlai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from the command prompt where you can access your python installation.\n",
    "\n",
    "The code is also available on GitHub: <https://github.com/lawrennd/mlai>\n",
    "\n",
    "Once `mlai` is installed, it can be imported in the usual manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic Loss and Linear System\n",
    "\n",
    "We will consider a simplified system, to remind us of some of the linear\n",
    "algebra involved, and introduce some of the fundamental issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Loss\n",
    "\n",
    "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
    "class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/expected-loss.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/expected-loss.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "Our objective function so far has been the negative log likelihood,\n",
    "which we have minimized (via the sum of squares error) to obtain our\n",
    "model. However, there is an alternative perspective on an objective\n",
    "function, that of a *loss function*. A loss function is a cost function\n",
    "associated with the penalty you might need to pay for a particular\n",
    "incorrect decision. One approach to machine learning involves specifying\n",
    "a loss function and considering how much a particular model is likely to\n",
    "cost us across its lifetime. We can represent this with an expectation.\n",
    "If our loss function is given as $L(y, x, \\mathbf{ w})$ for a particular\n",
    "model that predicts $y$ given $x$ and $\\mathbf{ w}$ then we are\n",
    "interested in minimizing the expected loss under the likely distribution\n",
    "of $y$ and $x$. To understand this formally we define the *true*\n",
    "distribution of the data samples, $y$, $x$. This is a particular\n",
    "distribution that we don’t typically have access to. To represent it we\n",
    "define a variant of the letter ‘P,’ $\\mathbb{P}(y, x)$. If we genuinely\n",
    "pay $L(y, x, \\mathbf{ w})$ for every mistake we make, and the future\n",
    "test data is genuinely drawn from $\\mathbb{P}(y, x)$ then we can define\n",
    "our expected loss, or risk, to be, $$\n",
    "R(\\mathbf{ w}) = \\int L(y, x, \\mathbf{ w}) \\mathbb{P}(y, x) \\text{d}y\n",
    "\\text{d}x.\n",
    "$$ Of course, in practice, this value can’t be computed *but* it serves\n",
    "as a reminder of what it is we are aiming to minimize and under certain\n",
    "circumstances it can be approximated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample-Based Approximations\n",
    "\n",
    "A sample-based approximation to an expectation involves replacing the\n",
    "true expectation with a sum over samples from the distribution. $$\n",
    "\\int f(z) p(z) \\text{d}z\\approx \\frac{1}{s}\\sum_{i=1}^s f(z_i).\n",
    "$$ if $\\{z_i\\}_{i=1}^s$ are a set of $s$ independent and identically\n",
    "distributed samples from the distribution $p(z)$. This approximation\n",
    "becomes better for larger $s$, although the *rate of convergence* to the\n",
    "true integral will be very dependent on the distribution $p(z)$ *and*\n",
    "the function $f(z)$.\n",
    "\n",
    "That said, this means we can approximate our true integral with the sum,\n",
    "$$\n",
    "R(\\mathbf{ w}) \\approx \\frac{1}{n}\\sum_{i=1}^{n} L(y_i, x_i, \\mathbf{ w}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laplace’s Idea\n",
    "\n",
    "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
    "class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/linear-regression-log-likelihood.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/linear-regression-log-likelihood.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "Laplace had the idea to augment the observations by noise, that is\n",
    "equivalent to considering a probability density whose mean is given by\n",
    "the *prediction function*\n",
    "$$p\\left(y_i|x_i\\right)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{\\left(y_i-f\\left(x_i\\right)\\right)^{2}}{2\\sigma^2}\\right).$$\n",
    "\n",
    "This is known as *stochastic process*. It is a function that is\n",
    "corrupted by noise. Laplace didn’t suggest the Gaussian density for that\n",
    "purpose, that was an innovation from Carl Friederich Gauss, which is\n",
    "what gives the Gaussian density its name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Height as a Function of Weight\n",
    "\n",
    "In the standard Gaussian, parameterized by mean and variance, make the\n",
    "mean a linear function of an *input*.\n",
    "\n",
    "This leads to a regression model. $$\n",
    "\\begin{align*}\n",
    "  y_i=&f\\left(x_i\\right)+\\epsilon_i,\\\\\n",
    "         \\epsilon_i \\sim & \\mathcal{N}\\left(0,\\sigma^2\\right).\n",
    "  \\end{align*}\n",
    "$$\n",
    "\n",
    "Assume $y_i$ is height and $x_i$ is weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Olympic Marathon Data\n",
    "\n",
    "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
    "class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_datasets/includes/olympic-marathon-data.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_datasets/includes/olympic-marathon-data.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"70%\">\n",
    "\n",
    "-   Gold medal times for Olympic Marathon since 1896.\n",
    "-   Marathons before 1924 didn’t have a standardized distance.\n",
    "-   Present results using pace per km.\n",
    "-   In 1904 Marathon was badly organized leading to very slow times.\n",
    "\n",
    "</td>\n",
    "<td width=\"30%\">\n",
    "\n",
    "<img class=\"\" src=\"https://mlatcl.github.io/deepnn/./slides/diagrams//Stephen_Kiprotich.jpg\" style=\"width:100%\">\n",
    "<small>Image from Wikimedia Commons <http://bit.ly/16kMKHQ></small>\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "The first thing we will do is load a standard data set for regression\n",
    "modelling. The data consists of the pace of Olympic Gold Medal Marathon\n",
    "winners for the Olympics from 1896 to present. Let’s load in the data\n",
    "and plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pods.datasets.olympic_marathon_men()\n",
    "x = data['X']\n",
    "y = data['Y']\n",
    "\n",
    "offset = y.mean()\n",
    "scale = np.sqrt(y.var())\n",
    "yhat = (y - offset)/scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mlai.plot as plot\n",
    "import mlai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlim = (1875,2030)\n",
    "ylim = (2.5, 6.5)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "_ = ax.plot(x, y, 'r.',markersize=10)\n",
    "ax.set_xlabel('year', fontsize=20)\n",
    "ax.set_ylabel('pace min/km', fontsize=20)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "\n",
    "mlai.write_figure(filename='olympic-marathon.svg', \n",
    "                  directory='./datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://mlatcl.github.io/deepnn/./slides/diagrams//datasets/olympic-marathon.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Olympic marathon pace times since 1896.</i>\n",
    "\n",
    "Things to notice about the data include the outlier in 1904, in that\n",
    "year the Olympics was in St Louis, USA. Organizational problems and\n",
    "challenges with dust kicked up by the cars following the race meant that\n",
    "participants got lost, and only very few participants completed. More\n",
    "recent years see more consistently quick marathons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Example: Olympic Marathons\n",
    "\n",
    "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
    "class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/olympic-marathon-linear-regression.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/olympic-marathon-linear-regression.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "Note that `x` and `y` are not `pandas` data frames for this example,\n",
    "they are just arrays of dimensionality $n\\times 1$, where $n$ is the\n",
    "number of data.\n",
    "\n",
    "The aim of this lab is to have you coding linear regression in python.\n",
    "We will do it in two ways, once using iterative updates (coordinate\n",
    "ascent) and then using linear algebra. The linear algebra approach will\n",
    "not only work much better, it is also easy to extend to multiple input\n",
    "linear regression and *non-linear* regression using basis functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood: Iterative Solution\n",
    "\n",
    "Now we will take the maximum likelihood approach we derived in the\n",
    "lecture to fit a line, $y_i=mx_i + c$, to the data you’ve plotted. We\n",
    "are trying to minimize the error function: $$\n",
    "L(m, c) =  \\sum_{i=1}^n(y_i-mx_i-c)^2\n",
    "$$ with respect to $m$, $c$ and $\\sigma^2$. We can start with an initial\n",
    "guess for $m$,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = -0.4\n",
    "c = 80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use the maximum likelihood update to find an estimate for the\n",
    "offset, $c$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic Loss\n",
    "\n",
    "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
    "class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/linear-regression-direct-solution.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/linear-regression-direct-solution.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "Now we’ve identified the empirical risk with the loss, we’ll use\n",
    "$L(\\mathbf{ w})$ to represent our objective function. $$\n",
    "L(\\mathbf{ w}) = \\sum_{i=1}^n\\left(y_i - f(\\mathbf{ x}_i, \\mathbf{ w})\\right)^2\n",
    "$$ gives us our objective.\n",
    "\n",
    "In the case of the linear prediction function, we can substitute\n",
    "$f(\\mathbf{ x}_i, \\mathbf{ w}) = \\mathbf{ w}^\\top \\mathbf{ x}_i$. $$\n",
    "L(\\mathbf{ w}) = \\sum_{i=1}^n\\left(y_i - \\mathbf{ w}^\\top \\mathbf{ x}_i\\right)^2\n",
    "$$ To compute the gradient of the objective, we first expand the\n",
    "brackets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bracket Expansion\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "  L(\\mathbf{ w},\\sigma^2)  = & \\sum\n",
    "_{i=1}^{n}y_i^{2}- 2\\sum\n",
    "_{i=1}^{n}y_i\\mathbf{ w}^{\\top}\\mathbf{ x}_i\\\\&+\\sum\n",
    "_{i=1}^{n}\\mathbf{ w}^{\\top}\\mathbf{ x}_i\\mathbf{ x}_i^{\\top}\\mathbf{ w}\\\\\n",
    "    = & \\sum_{i=1}^{n}y_i^{2}-\n",
    "2 \\mathbf{ w}^\\top\\sum_{i=1}^{n}\\mathbf{ x}_iy_i\\\\&+\n",
    "\\mathbf{ w}^{\\top}\\left[\\sum\n",
    "_{i=1}^{n}\\mathbf{ x}_i\\mathbf{ x}_i^{\\top}\\right]\\mathbf{ w}.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution with Linear Algebra\n",
    "\n",
    "In this section we’re going compute the minimum of the quadratic loss\n",
    "with respect to the parameters. When we do this, we’ll also review\n",
    "*linear algebra*. We will represent all our errors and functions in the\n",
    "form of matrices and vectors.\n",
    "\n",
    "Linear algebra is just a shorthand for performing lots of\n",
    "multiplications and additions simultaneously. What does it have to do\n",
    "with our system then? Well, the first thing to note is that the classic\n",
    "linear function we fit for a one-dimensional regression has the form: $$\n",
    "f(x) = mx + c\n",
    "$$ the classical form for a straight line. From a linear algebraic\n",
    "perspective, we are looking for multiplications and additions. We are\n",
    "also looking to separate our parameters from our data. The data is the\n",
    "*givens*. In French the word is données literally translated means\n",
    "*givens* that’s great, because we don’t need to change the data, what we\n",
    "need to change are the parameters (or variables) of the model. In this\n",
    "function the data comes in through $x$, and the parameters are $m$ and\n",
    "$c$.\n",
    "\n",
    "What we’d like to create is a vector of parameters and a vector of data.\n",
    "Then we could represent the system with vectors that represent the data,\n",
    "and vectors that represent the parameters.\n",
    "\n",
    "We look to turn the multiplications and additions into a linear\n",
    "algebraic form, we have one multiplication ($m\\times c$) and one\n",
    "addition ($mx + c$). But we can turn this into an inner product by\n",
    "writing it in the following way, $$\n",
    "f(x) = m \\times x +\n",
    "c \\times 1,\n",
    "$$ in other words, we’ve extracted the unit value from the offset, $c$.\n",
    "We can think of this unit value like an extra item of data, because it\n",
    "is always given to us, and it is always set to 1 (unlike regular data,\n",
    "which is likely to vary!). We can therefore write each input data\n",
    "location, $\\mathbf{ x}$, as a vector $$\n",
    "\\mathbf{ x}= \\begin{bmatrix} 1\\\\ x\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Now we choose to also turn our parameters into a vector. The parameter\n",
    "vector will be defined to contain $$\n",
    "\\mathbf{ w}= \\begin{bmatrix} c \\\\ m\\end{bmatrix}\n",
    "$$ because if we now take the inner product between these two vectors we\n",
    "recover $$\n",
    "\\mathbf{ x}\\cdot\\mathbf{ w}= 1 \\times c + x \\times m = mx + c\n",
    "$$ In `numpy` we can define this vector as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the vector w\n",
    "w = np.zeros(shape=(2, 1))\n",
    "w[0] = m\n",
    "w[1] = c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us the equivalence between original operation and an\n",
    "operation in vector space. Whilst the notation here isn’t a lot shorter,\n",
    "the beauty is that we will be able to add as many features as we like\n",
    "and keep the same representation. In general, we are now moving to a\n",
    "system where each of our predictions is given by an inner product. When\n",
    "we want to represent a linear product in linear algebra, we tend to do\n",
    "it with the transpose operation, so since we have\n",
    "$\\mathbf{a}\\cdot\\mathbf{b} = \\mathbf{a}^\\top\\mathbf{b}$ we can write $$\n",
    "f(\\mathbf{ x}_i) = \\mathbf{ x}_i^\\top\\mathbf{ w}.\n",
    "$$ Where we’ve assumed that each data point, $\\mathbf{ x}_i$, is now\n",
    "written by appending a 1 onto the original vector $$\n",
    "\\mathbf{ x}_i = \\begin{bmatrix} \n",
    "1 \\\\\n",
    "x_i\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design Matrix\n",
    "\n",
    "We can do this for the entire data set to form a [*design\n",
    "matrix*](http://en.wikipedia.org/wiki/Design_matrix)\n",
    "$\\boldsymbol{ \\Phi}$, $$\n",
    "\\boldsymbol{ \\Phi}\n",
    "= \\begin{bmatrix} \n",
    "\\mathbf{ x}_1^\\top \\\\\\ \n",
    "\\mathbf{ x}_2^\\top \\\\\\ \n",
    "\\vdots \\\\\\\n",
    "\\mathbf{ x}_n^\\top\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "1 & x_1 \\\\\\\n",
    "1 & x_2 \\\\\\\n",
    "\\vdots\n",
    "& \\vdots \\\\\\\n",
    "1 & x_n\n",
    "\\end{bmatrix},\n",
    "$$ which in `numpy` can be done with the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Phi = np.hstack((np.ones_like(x), x))\n",
    "print(Phi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing the Objective with Linear Algebra\n",
    "\n",
    "When we think of the objective function, we can think of it as the\n",
    "errors where the error is defined in a similar way to what it was in\n",
    "Legendre’s day $y_i - f(\\mathbf{ x}_i)$, in statistics these errors are\n",
    "also sometimes called\n",
    "[*residuals*](http://en.wikipedia.org/wiki/Errors_and_residuals_in_statistics).\n",
    "So, we can think as the objective and the prediction function as two\n",
    "separate parts, first we have, $$\n",
    "L(\\mathbf{ w}) = \\sum_{i=1}^n(y_i - f(\\mathbf{ x}_i; \\mathbf{ w}))^2,\n",
    "$$ where we’ve made the function $f(\\cdot)$’s dependence on the\n",
    "parameters $\\mathbf{ w}$ explicit in this equation. Then we have the\n",
    "definition of the function itself, $$\n",
    "f(\\mathbf{ x}_i; \\mathbf{ w}) = \\mathbf{ x}_i^\\top \\mathbf{ w}.\n",
    "$$ Let’s look again at these two equations and see if we can identify\n",
    "any inner products. The first equation is a sum of squares, which is\n",
    "promising. Any sum of squares can be represented by an inner product, $$\n",
    "a = \\sum_{i=1}^{k} b^2_i = \\mathbf{b}^\\top\\mathbf{b}.\n",
    "$$ If we wish to represent $L(\\mathbf{ w})$ in this way, all we need to\n",
    "do is convert the sum operator to an inner product. We can get a vector\n",
    "from that sum operator by placing both $y_i$ and\n",
    "$f(\\mathbf{ x}_i; \\mathbf{ w})$ into vectors, which we do by defining $$\n",
    "\\mathbf{ y}= \\begin{bmatrix}y_1\\\\ y_2\\\\ \\vdots \\\\ y_n\\end{bmatrix}\n",
    "$$ and defining $$\n",
    "\\mathbf{ f}(\\mathbf{ x}_1; \\mathbf{ w}) = \\begin{bmatrix}f(\\mathbf{ x}_1; \\mathbf{ w})\\\\ f(\\mathbf{ x}_2; \\mathbf{ w})\\\\ \\vdots \\\\ f(\\mathbf{ x}_n; \\mathbf{ w})\\end{bmatrix}.\n",
    "$$ The second of these is a vector-valued function. This term may appear\n",
    "intimidating, but the idea is straightforward. A vector valued function\n",
    "is simply a vector whose elements are themselves defined as *functions*,\n",
    "i.e., it is a vector of functions, rather than a vector of scalars. The\n",
    "idea is so straightforward, that we are going to ignore it for the\n",
    "moment, and barely use it in the derivation. But it will reappear later\n",
    "when we introduce *basis functions*. So, we will for the moment ignore\n",
    "the dependence of $\\mathbf{ f}$ on $\\mathbf{ w}$ and\n",
    "$\\boldsymbol{ \\Phi}$ and simply summarise it by a vector of numbers $$\n",
    "\\mathbf{ f}= \\begin{bmatrix}f_1\\\\f_2\\\\\n",
    "\\vdots \\\\ f_n\\end{bmatrix}.\n",
    "$$ This allows us to write our objective in the folowing, linear\n",
    "algebraic form, $$\n",
    "L(\\mathbf{ w}) = (\\mathbf{ y}- \\mathbf{ f})^\\top(\\mathbf{ y}- \\mathbf{ f})\n",
    "$$ from the rules of inner products. But what of our matrix\n",
    "$\\boldsymbol{ \\Phi}$ of input data? At this point, we need to dust off\n",
    "[*matrix-vector\n",
    "multiplication*](http://en.wikipedia.org/wiki/Matrix_multiplication).\n",
    "Matrix multiplication is simply a convenient way of performing many\n",
    "inner products together, and it’s exactly what we need to summarize the\n",
    "operation $$\n",
    "f_i = \\mathbf{ x}_i^\\top\\mathbf{ w}.\n",
    "$$ This operation tells us that each element of the vector $\\mathbf{ f}$\n",
    "(our vector valued function) is given by an inner product between\n",
    "$\\mathbf{ x}_i$ and $\\mathbf{ w}$. In other words, it is a series of\n",
    "inner products. Let’s look at the definition of matrix multiplication,\n",
    "it takes the form $$\n",
    "\\mathbf{c} = \\mathbf{B}\\mathbf{a},\n",
    "$$ where $\\mathbf{c}$ might be a $k$ dimensional vector (which we can\n",
    "interpret as a $k\\times 1$ dimensional matrix), and $\\mathbf{B}$ is a\n",
    "$k\\times k$ dimensional matrix and $\\mathbf{a}$ is a $k$ dimensional\n",
    "vector ($k\\times 1$ dimensional matrix).\n",
    "\n",
    "The result of this multiplication is of the form $$\n",
    "\\begin{bmatrix}c_1\\\\c_2 \\\\ \\vdots \\\\\n",
    "a_k\\end{bmatrix} = \n",
    "\\begin{bmatrix} b_{1,1} & b_{1, 2} & \\dots & b_{1, k} \\\\\n",
    "b_{2, 1} & b_{2, 2} & \\dots & b_{2, k} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "b_{k, 1} & b_{k, 2} & \\dots & b_{k, k} \\end{bmatrix} \\begin{bmatrix}a_1\\\\a_2 \\\\\n",
    "\\vdots\\\\ c_k\\end{bmatrix} = \\begin{bmatrix} b_{1, 1}a_1 + b_{1, 2}a_2 + \\dots +\n",
    "b_{1, k}a_k\\\\\n",
    "b_{2, 1}a_1 + b_{2, 2}a_2 + \\dots + b_{2, k}a_k \\\\ \n",
    "\\vdots\\\\\n",
    "b_{k, 1}a_1 + b_{k, 2}a_2 + \\dots + b_{k, k}a_k\\end{bmatrix}.\n",
    "$$ We see that each element of the result, $\\mathbf{a}$ is simply the\n",
    "inner product between each *row* of $\\mathbf{B}$ and the vector\n",
    "$\\mathbf{c}$. Because we have defined each element of $\\mathbf{ f}$ to\n",
    "be given by the inner product between each *row* of the design matrix\n",
    "and the vector $\\mathbf{ w}$ we now can write the full operation in one\n",
    "matrix multiplication,\n",
    "\n",
    "$$\n",
    "\\mathbf{ f}= \\boldsymbol{ \\Phi}\\mathbf{ w}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = Phi@w # The @ sign performs matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining this result with our objective function, $$\n",
    "L(\\mathbf{ w}) = (\\mathbf{ y}- \\mathbf{ f})^\\top(\\mathbf{ y}- \\mathbf{ f})\n",
    "$$ we find we have defined the *model* with two equations. One equation\n",
    "tells us the form of our predictive function and how it depends on its\n",
    "parameters, the other tells us the form of our objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid = (y-f)\n",
    "E = np.dot(resid.T, resid) # matrix multiplication on a single vector is equivalent to a dot product.\n",
    "print(\"Error function is:\", E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective Optimization\n",
    "\n",
    "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
    "class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/linear-regression-objective-optimisation.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/linear-regression-objective-optimisation.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "Our *model* has now been defined with two equations: the prediction\n",
    "function and the objective function. Now we will use multivariate\n",
    "calculus to define an *algorithm* to fit the model. The separation\n",
    "between model and algorithm is important and is often overlooked. Our\n",
    "model contains a function that shows how it will be used for prediction,\n",
    "and a function that describes the objective function we need to optimize\n",
    "to obtain a good set of parameters.\n",
    "\n",
    "The model linear regression model we have described is still the same as\n",
    "the one we fitted above with a coordinate ascent algorithm. We have only\n",
    "played with the notation to obtain the same model in a matrix and vector\n",
    "notation. However, we will now fit this model with a different\n",
    "algorithm, one that is much faster. It is such a widely used algorithm\n",
    "that from the end user’s perspective it doesn’t even look like an\n",
    "algorithm, it just appears to be a single operation (or function).\n",
    "However, underneath the computer calls an algorithm to find the\n",
    "solution. Further, the algorithm we obtain is very widely used, and\n",
    "because of this it turns out to be highly optimized.\n",
    "\n",
    "Once again, we are going to try and find the stationary points of our\n",
    "objective by finding the *stationary points*. However, the stationary\n",
    "points of a multivariate function, are a little bit more complex to\n",
    "find. As before we need to find the point at which the gradient is zero,\n",
    "but now we need to use *multivariate calculus* to find it. This involves\n",
    "learning a few additional rules of differentiation (that allow you to do\n",
    "the derivatives of a function with respect to vector), but in the end it\n",
    "makes things quite a bit easier. We define vectorial derivatives as\n",
    "follows, $$\n",
    "\\frac{\\text{d}L(\\mathbf{ w})}{\\text{d}\\mathbf{ w}} =\n",
    "\\begin{bmatrix}\\frac{\\text{d}L(\\mathbf{ w})}{\\text{d}w_1}\\\\\\frac{\\text{d}L(\\mathbf{ w})}{\\text{d}w_2}\\end{bmatrix}.\n",
    "$$ where $\\frac{\\text{d}L(\\mathbf{ w})}{\\text{d}w_1}$ is the [partial\n",
    "derivative](http://en.wikipedia.org/wiki/Partial_derivative) of the\n",
    "error function with respect to $w_1$.\n",
    "\n",
    "Differentiation through multiplications and additions is relatively\n",
    "straightforward, and since linear algebra is just multiplication and\n",
    "addition, then its rules of differentiation are quite straightforward\n",
    "too, but slightly more complex than regular derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Derivatives\n",
    "\n",
    "We will need two rules of multivariate or *matrix* differentiation. The\n",
    "first is differentiation of an inner product. By remembering that the\n",
    "inner product is made up of multiplication and addition, we can hope\n",
    "that its derivative is quite straightforward, and so it proves to be. We\n",
    "can start by thinking about the definition of the inner product, $$\n",
    "\\mathbf{a}^\\top\\mathbf{z} = \\sum_{i} a_i\n",
    "z_i,\n",
    "$$ which if we were to take the derivative with respect to $z_k$ would\n",
    "simply return the gradient of the one term in the sum for which the\n",
    "derivative was non-zero, that of $a_k$, so we know that $$\n",
    "\\frac{\\text{d}}{\\text{d}z_k} \\mathbf{a}^\\top \\mathbf{z} = a_k\n",
    "$$ and by our definition for multivariate derivatives, we can simply\n",
    "stack all the partial derivatives of this form in a vector to obtain the\n",
    "result that $$\n",
    "\\frac{\\text{d}}{\\text{d}\\mathbf{z}}\n",
    "\\mathbf{a}^\\top \\mathbf{z} = \\mathbf{a}.\n",
    "$$ The second rule that’s required is differentiation of a ‘matrix\n",
    "quadratic.’ A scalar quadratic in $z$ with coefficient $c$ has the form\n",
    "$cz^2$. If $\\mathbf{z}$ is a $k\\times 1$ vector and $\\mathbf{C}$ is a\n",
    "$k \\times k$ *matrix* of coefficients then the matrix quadratic form is\n",
    "written as $\\mathbf{z}^\\top \\mathbf{C}\\mathbf{z}$, which is itself a\n",
    "*scalar* quantity, but it is a function of a *vector*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching Dimensions in Matrix Multiplications\n",
    "\n",
    "There’s a trick for telling a multiplication leads to a scalar result.\n",
    "When you are doing mathematics with matrices, it’s always worth pausing\n",
    "to perform a quick sanity check on the dimensions. Matrix multplication\n",
    "only works when the dimensions match. To be precise, the ‘inner’\n",
    "dimension of the matrix must match. What is the inner dimension? If we\n",
    "multiply two matrices $\\mathbf{A}$ and $\\mathbf{B}$, the first of which\n",
    "has $k$ rows and $\\ell$ columns and the second of which has $p$ rows and\n",
    "$q$ columns, then we can check whether the multiplication works by\n",
    "writing the dimensionalities next to each other, $$\n",
    "\\mathbf{A} \\mathbf{B} \\rightarrow (k \\times\n",
    "\\underbrace{\\ell)(p}_\\text{inner dimensions} \\times q) \\rightarrow (k\\times q).\n",
    "$$ The inner dimensions are the two inside dimensions, $\\ell$ and $p$.\n",
    "The multiplication will only work if $\\ell=p$. The result of the\n",
    "multiplication will then be a $k\\times q$ matrix: this dimensionality\n",
    "comes from the ‘outer dimensions.’ Note that matrix multiplication is\n",
    "not [*commutative*](http://en.wikipedia.org/wiki/Commutative_property).\n",
    "And if you change the order of the multiplication, $$\n",
    "\\mathbf{B} \\mathbf{A} \\rightarrow (\\ell \\times \\underbrace{k)(q}_\\text{inner dimensions} \\times p) \\rightarrow (\\ell \\times p).\n",
    "$$ Firstly, it may no longer even work, because now the condition is\n",
    "that $k=q$, and secondly the result could be of a different\n",
    "dimensionality. An exception is if the matrices are square matrices\n",
    "(e.g., same number of rows as columns) and they are both *symmetric*. A\n",
    "symmetric matrix is one for which $\\mathbf{A}=\\mathbf{A}^\\top$, or\n",
    "equivalently, $a_{i,j} = a_{j,i}$ for all $i$ and $j$.\n",
    "\n",
    "For applying and developing machine learning algorithms you should get\n",
    "familiar with working with matrices and vectors. You should have come\n",
    "across them before, but you may not have used them as extensively as we\n",
    "are doing now. It’s worth getting used to using this trick to check your\n",
    "work and ensure you know what the dimension of an output matrix should\n",
    "be. For our matrix quadratic form, it turns out that we can see it as a\n",
    "special type of inner product. $$\n",
    "\\mathbf{z}^\\top\\mathbf{C}\\mathbf{z} \\rightarrow (1\\times\n",
    "\\underbrace{k) (k}_\\text{inner dimensions}\\times k) (k\\times 1) \\rightarrow\n",
    "\\mathbf{b}^\\top\\mathbf{z}\n",
    "$$ where $\\mathbf{b} = \\mathbf{C}\\mathbf{z}$ so therefore the result is\n",
    "a scalar, $$\n",
    "\\mathbf{b}^\\top\\mathbf{z} \\rightarrow\n",
    "(1\\times \\underbrace{k) (k}_\\text{inner dimensions}\\times 1) \\rightarrow\n",
    "(1\\times 1)\n",
    "$$ where a $(1\\times 1)$ matrix is recognised as a scalar.\n",
    "\n",
    "This implies that we should be able to differentiate this form, and\n",
    "indeed the rule for its differentiation is slightly more complex than\n",
    "the inner product, but still quite simple, $$\n",
    "\\frac{\\text{d}}{\\text{d}\\mathbf{z}}\n",
    "\\mathbf{z}^\\top\\mathbf{C}\\mathbf{z}= \\mathbf{C}\\mathbf{z} + \\mathbf{C}^\\top\n",
    "\\mathbf{z}.\n",
    "$$ Note that in the special case where $\\mathbf{C}$ is symmetric then we\n",
    "have $\\mathbf{C} = \\mathbf{C}^\\top$ and the derivative simplifies to $$\n",
    "\\frac{\\text{d}}{\\text{d}\\mathbf{z}} \\mathbf{z}^\\top\\mathbf{C}\\mathbf{z}=\n",
    "2\\mathbf{C}\\mathbf{z}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differentiate the Objective\n",
    "\n",
    "First, we need to compute the full objective by substituting our\n",
    "prediction function into the objective function to obtain the objective\n",
    "in terms of $\\mathbf{ w}$. Doing this we obtain $$\n",
    "L(\\mathbf{ w})= (\\mathbf{ y}- \\boldsymbol{ \\Phi}\\mathbf{ w})^\\top (\\mathbf{ y}- \\boldsymbol{ \\Phi}\\mathbf{ w}).\n",
    "$$ We now need to differentiate this *quadratic form* to find the\n",
    "minimum. We differentiate with respect to the *vector* $\\mathbf{ w}$.\n",
    "But before we do that, we’ll expand the brackets in the quadratic form\n",
    "to obtain a series of scalar terms. The rules for bracket expansion\n",
    "across the vectors are similar to those for the scalar system giving, $$\n",
    "(\\mathbf{a} - \\mathbf{b})^\\top\n",
    "(\\mathbf{c} - \\mathbf{d}) = \\mathbf{a}^\\top \\mathbf{c} - \\mathbf{a}^\\top\n",
    "\\mathbf{d} - \\mathbf{b}^\\top \\mathbf{c} + \\mathbf{b}^\\top \\mathbf{d}\n",
    "$$ which substituting for $\\mathbf{a} = \\mathbf{c} = \\mathbf{ y}$ and\n",
    "$\\mathbf{b}=\\mathbf{d} = \\boldsymbol{ \\Phi}\\mathbf{ w}$ gives $$\n",
    "L(\\mathbf{ w})=\n",
    "\\mathbf{ y}^\\top\\mathbf{ y}- 2\\mathbf{ y}^\\top\\boldsymbol{ \\Phi}\\mathbf{ w}+\n",
    "\\mathbf{ w}^\\top\\boldsymbol{ \\Phi}^\\top\\boldsymbol{ \\Phi}\\mathbf{ w}\n",
    "$$ where we used the fact that\n",
    "$\\mathbf{ y}^\\top\\boldsymbol{ \\Phi}\\mathbf{ w}=\\mathbf{ w}^\\top\\boldsymbol{ \\Phi}^\\top\\mathbf{ y}$.\n",
    "\n",
    "Now we can use our rules of differentiation to compute the derivative of\n",
    "this form, which is, $$\n",
    "\\frac{\\text{d}}{\\text{d}\\mathbf{ w}}L(\\mathbf{ w})=- 2\\boldsymbol{ \\Phi}^\\top \\mathbf{ y}+\n",
    "2\\boldsymbol{ \\Phi}^\\top\\boldsymbol{ \\Phi}\\mathbf{ w},\n",
    "$$ where we have exploited the fact that\n",
    "$\\boldsymbol{ \\Phi}^\\top\\boldsymbol{ \\Phi}$ is symmetric to obtain this\n",
    "result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Use the equivalence between our vector and our matrix formulations of\n",
    "linear regression, alongside our definition of vector derivates, to\n",
    "match the gradients we’ve computed directly for\n",
    "$\\frac{\\text{d}L(c, m)}{\\text{d}c}$ and\n",
    "$\\frac{\\text{d}L(c, m)}{\\text{d}m}$ to those for\n",
    "$\\frac{\\text{d}L(\\mathbf{ w})}{\\text{d}\\mathbf{ w}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 Answer\n",
    "\n",
    "Write your answer to Exercise 1 here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Equation for Global Optimum\n",
    "\n",
    "We need to find the minimum of our objective function. Using our\n",
    "objective function, we can minimize for our parameter vector\n",
    "$\\mathbf{ w}$. Firstly, we seek stationary points by find parameter\n",
    "vectors that solve for when the gradients are zero, $$\n",
    "\\mathbf{0}=- 2\\boldsymbol{ \\Phi}^\\top\n",
    "\\mathbf{ y}+ 2\\boldsymbol{ \\Phi}^\\top\\boldsymbol{ \\Phi}\\mathbf{ w},\n",
    "$$ where $\\mathbf{0}$ is a *vector* of zeros. Rearranging this equation,\n",
    "we find the solution to be $$\n",
    "\\boldsymbol{ \\Phi}^\\top \\boldsymbol{ \\Phi}\\mathbf{ w}= \\boldsymbol{ \\Phi}^\\top\n",
    "\\mathbf{ y}\n",
    "$$ which is a matrix equation of the familiar form\n",
    "$\\mathbf{A}\\mathbf{x} = \\mathbf{b}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving the Multivariate System\n",
    "\n",
    "The solution for $\\mathbf{ w}$ can be written mathematically in terms of\n",
    "a matrix inverse of $\\boldsymbol{ \\Phi}^\\top\\boldsymbol{ \\Phi}$, but\n",
    "computation of a matrix inverse requires an algorithm to resolve it.\n",
    "You’ll know this if you had to invert, by hand, a $3\\times 3$ matrix in\n",
    "high school. From a numerical stability perspective, it is also best not\n",
    "to compute the matrix inverse directly, but rather to ask the computer\n",
    "to *solve* the system of linear equations given by $$\n",
    "\\boldsymbol{ \\Phi}^\\top\\boldsymbol{ \\Phi}\\mathbf{ w}= \\boldsymbol{ \\Phi}^\\top\\mathbf{ y}\n",
    "$$ for $\\mathbf{ w}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Linear Regression\n",
    "\n",
    "A major advantage of the new system is that we can build a linear\n",
    "regression on a multivariate system. The matrix calculus didn’t specify\n",
    "what the length of the vector $\\mathbf{ x}$ should be, or equivalently\n",
    "the size of the design matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hessian Matrix\n",
    "\n",
    "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
    "class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/linear-regression-hessian.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/linear-regression-hessian.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "We can also compute the [Hessian\n",
    "matrix](https://en.wikipedia.org/wiki/Hessian_matrix), the curvature of\n",
    "the loss function. To do this we take the second derivative of the loss\n",
    "function with respect to the parameter vector, $$\n",
    "\\frac{\\text{d}^2}{\\text{d}\\mathbf{ w}\\text{d}\\mathbf{ w}^\\top} L(\\mathbf{ w}) = 2\\boldsymbol{ \\Phi}^\\top\\boldsymbol{ \\Phi}.\n",
    "$$ So, we see that the curvature is only dependent on the design matrix.\n",
    "\n",
    "Note that for this linear model the curvature is *not* dependent on the\n",
    "values of the parameter vector, $\\mathbf{ w}$, or indeed on the\n",
    "*response* variables, $\\mathbf{ y}$. This is unusual, in general the\n",
    "curvature will depend on the parameters and the response variables. The\n",
    "linear model with quadratic loss is a special case because the overall\n",
    "loss function has a *quadratic form* which is the unique form with\n",
    "constant curvature across the whole space.\n",
    "\n",
    "This is one reason why linear models are so easy to work with.\n",
    "\n",
    "Because the curvature is constant everywhere, we know that the curvature\n",
    "at the minimum is given by $2\\boldsymbol{ \\Phi}^\\top\\boldsymbol{ \\Phi}$.\n",
    "\n",
    "From univariate calculus you might recall that the optimum is a maximum\n",
    "if the curvature is negative, and a minimum if the curvature is\n",
    "positive. A similar theorem holds for multivariate calculus, but now the\n",
    "curvature must be *positive definite* for the point to be a minimum. The\n",
    "constant curvature also shows us also that the minimum is *unique*.\n",
    "\n",
    "Positive definite means that for any two vectors, $\\mathbf{u}$ of unit\n",
    "length $\\mathbf{u}^\\top\\mathbf{u}$ we have that, $$\n",
    "\\mathbf{u}^\\top\\mathbf{A} \\mathbf{u} > 0 \\quad \\forall \\quad \\mathbf{u} \\quad \\text{with}\\quad \\mathbf{u}^\\top\\mathbf{u}=1\n",
    "$$ The matrix $\\boldsymbol{ \\Phi}^\\top\\boldsymbol{ \\Phi}$ (where we’ve\n",
    "dropped the 2) will satisfy this condition as long as the columns of\n",
    "$\\boldsymbol{ \\Phi}$ are *linearly independent* and the number of basis\n",
    "functions is less or equal to the number of data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigendecomposition of Hessian\n",
    "\n",
    "Applying a vector $\\mathbf{u}$ to the Hessian matrix gives us the\n",
    "curvature in a particular direction. So we can use this to look at the\n",
    "shape of the minimum by projecting onto the different directions,\n",
    "$\\mathbf{u}$.\n",
    "\n",
    "Recall the eigendecomposition of a matrix, $$\n",
    "\\mathbf{A}\\mathbf{u} = \\lambda\\mathbf{u}\n",
    "$$ If we allow $\\mathbf{u}_i$ to be an *eigenvector* of\n",
    "$\\boldsymbol{ \\Phi}^\\top\\boldsymbol{ \\Phi}\\mathbf{ w}$ then the\n",
    "curvature in that direction is given by the corresponding eigenvalue,\n",
    "$\\lambda_i$. $$\n",
    "\\boldsymbol{ \\Phi}^\\top\\boldsymbol{ \\Phi}\\mathbf{u}_i = \\lambda_i \\mathbf{u}_i.\n",
    "$$\n",
    "\n",
    "The eigendecomposition of the Hessian is a convenient representation of\n",
    "the nature of these minima. The principal eigenvector (the one\n",
    "associated with the largest eigenvalue), $\\mathbf{u}_1$ is associated\n",
    "with the direction of *highest curvature*. While the minor eigenvector\n",
    "shows us the flattest direction, where the curvature is smallest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nigeria NMIS Data\n",
    "\n",
    "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
    "class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_datasets/includes/nigeria-nmis-data.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_datasets/includes/nigeria-nmis-data.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "As an example data set we will use Nigerian Millennium Development Goals\n",
    "Information System Health Facility (The Office of the Senior Special\n",
    "Assistant to the President on the Millennium Development Goals\n",
    "(OSSAP-MDGs) and Columbia University, 2014). It can be found here\n",
    "<https://energydata.info/dataset/nigeria-nmis-education-facility-data-2014>.\n",
    "\n",
    "Taking from the information on the site,\n",
    "\n",
    "> The Nigeria MDG (Millennium Development Goals) Information System –\n",
    "> NMIS health facility data is collected by the Office of the Senior\n",
    "> Special Assistant to the President on the Millennium Development Goals\n",
    "> (OSSAP-MDGs) in partner with the Sustainable Engineering Lab at\n",
    "> Columbia University. A rigorous, geo-referenced baseline facility\n",
    "> inventory across Nigeria is created spanning from 2009 to 2011 with an\n",
    "> additional survey effort to increase coverage in 2014, to build\n",
    "> Nigeria’s first nation-wide inventory of health facility. The database\n",
    "> includes 34,139 health facilities info in Nigeria.\n",
    ">\n",
    "> The goal of this database is to make the data collected available to\n",
    "> planners, government officials, and the public, to be used to make\n",
    "> strategic decisions for planning relevant interventions.\n",
    ">\n",
    "> For data inquiry, please contact Ms. Funlola Osinupebi, Performance\n",
    "> Monitoring & Communications, Advisory Power Team, Office of the Vice\n",
    "> President at funlola.osinupebi@aptovp.org\n",
    ">\n",
    "> To learn more, please visit\n",
    "> <http://csd.columbia.edu/2014/03/10/the-nigeria-mdg-information-system-nmis-takes-open-data-further/>\n",
    ">\n",
    "> Suggested citation: Nigeria NMIS facility database (2014), the Office\n",
    "> of the Senior Special Assistant to the President on the Millennium\n",
    "> Development Goals (OSSAP-MDGs) & Columbia University\n",
    "\n",
    "For ease of use we’ve packaged this data set in the `pods` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pods.datasets.nigeria_nmis()['Y']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can access the data directly with the following\n",
    "commands.\n",
    "\n",
    "``` python\n",
    "import urllib.request\n",
    "urllib.request.urlretrieve('https://energydata.info/dataset/f85d1796-e7f2-4630-be84-79420174e3bd/resource/6e640a13-cab4-457b-b9e6-0336051bac27/download/healthmopupandbaselinenmisfacility.csv', 'healthmopupandbaselinenmisfacility.csv')\n",
    "\n",
    "import pandas as pd\n",
    "data = pd.read_csv('healthmopupandbaselinenmisfacility.csv')\n",
    "```\n",
    "\n",
    "Once it is loaded in the data can be summarized using the `describe`\n",
    "method in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In python and the Jupyter notebook it is possible to see a list of all\n",
    "possible functions and attributes by typing the name of the object\n",
    "followed by `.<Tab>` for example in the above case if we type\n",
    "`data.<Tab>` it show the columns available (these are attributes in\n",
    "pandas dataframes) such as `num_nurses_fulltime`, and also functions,\n",
    "such as `.describe()`.\n",
    "\n",
    "For functions we can also see the documentation about the function by\n",
    "following the name with a question mark. This will open a box with\n",
    "documentation at the bottom which can be closed with the x button."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mlai\n",
    "import mlai.plot as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_figsize)\n",
    "ax.plot(data.longitude, data.latitude, 'ro', alpha=0.01)\n",
    "ax.set_xlabel('longitude')\n",
    "ax.set_ylabel('latitude')\n",
    "\n",
    "mlai.write_figure('nigerian-health-facilities.png', directory='./ml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img class=\"\" src=\"https://mlatcl.github.io/deepnn/./slides/diagrams//ml/nigerian-health-facilities.png\" style=\"width:60%\">\n",
    "\n",
    "Figure: <i>Location of the over thirty-four thousand health facilities\n",
    "registered in the NMIS data across Nigeria. Each facility plotted\n",
    "according to its latitude and longitude.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Regression on Nigeria NMIS Data\n",
    "\n",
    "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
    "class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/nigeria-nmis-linear-regression.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/nigeria-nmis-linear-regression.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "Now we will build a design matrix based on the numeric features that\n",
    "include the number of nurses, and the number of midwives, the latitude\n",
    "and longitude. These are our *covariates*. The response variable will be\n",
    "the number of doctors. We build the design matrix as follows:\n",
    "\n",
    "Bias as an additional feature.\n",
    "\n",
    "First we select the covariates and response variables and drop any rows\n",
    "where there are missing values using the `pandas` `dropna` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariates = ['num_nurses_fulltime', 'num_nursemidwives_fulltime', 'latitude', 'longitude']\n",
    "response = ['num_doctors_fulltime']\n",
    "data_without_missing = data[covariates + response].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how many rows we have dropped and have a quick sanity check\n",
    "on our new data frame with `len`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data) - len(data_without_missing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that 2,735 of the entries had missing values in one of our\n",
    "variables of interest.\n",
    "\n",
    "You may also want to use `describe` or other functions to explore the\n",
    "new data frame.\n",
    "\n",
    "Now let’s perform a linear regression. But this time, we will create a\n",
    "pandas data frame for the result so we can store it in a form that we\n",
    "can visualize easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Phi = data_without_missing[covariates]\n",
    "Phi['Eins'] = 1 # add a column for the offset\n",
    "y = data_without_missing[response]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = pd.DataFrame(data=np.linalg.solve(Phi.T@Phi, Phi.T@y),  # solve linear regression here\n",
    "                 index = Phi.columns,  # columns of Phi become rows of w\n",
    "                 columns=['regression_coefficient']) # the column of Phi is the value of regression coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the residuals to see how good our estimates are. First we\n",
    "create a pandas data frame containing the predictions and use it to\n",
    "compute the residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = pd.DataFrame(data=(Phi@w).values, columns=['num_doctors_fulltime'])\n",
    "resid = y-ypred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s look at the residuals. We can use `describe` to get a summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that while the standard deviation of our residuals is around\n",
    "3, (this is equivalent to a root mean square error). The smallest and\n",
    "largest residual sow there are some significant outliers that our\n",
    "regression isn’t picking up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mlai.plot as plot\n",
    "import mlai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "resid.hist(ax=ax,bins=200)\n",
    "ax.set_xlim((-10,10))\n",
    "mlai.write_figure(filename='nigeria-nmis-num-doctors-residuals.svg', \n",
    "                  directory='./ml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://mlatcl.github.io/deepnn/./slides/diagrams//ml/nigeria-nmis-num-doctors-residuals.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Residual values for the ratings from the prediction of the\n",
    "movie rating given the data from the film.</i>\n",
    "\n",
    "Which shows our model *hasn’t* yet done a great job of representation,\n",
    "because the spread of values is large. We can check what the rating is\n",
    "dominated by in terms of regression coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking our regression coefficients, we see that the number of doctors\n",
    "is positively influenced by the number of nurses and the number of\n",
    "midwives. The latitude and longitude have a smaller effect. The bias\n",
    "term (‘eins’) is a small positive offset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aside\n",
    "\n",
    "Just for informational purposes, the actual approach used in software\n",
    "for fitting a linear model *should* be a QR decomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution with QR Decomposition\n",
    "\n",
    "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
    "class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/qr-decomposition-regression.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/qr-decomposition-regression.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "Performing a solve instead of a matrix inverse is the more numerically\n",
    "stable approach, but we can do even better. A\n",
    "[QR-decomposition](http://en.wikipedia.org/wiki/QR_decomposition) of a\n",
    "matrix factorizes it into a matrix which is an orthogonal matrix\n",
    "$\\mathbf{Q}$, so that $\\mathbf{Q}^\\top \\mathbf{Q} = \\mathbf{I}$. And a\n",
    "matrix which is upper triangular, $\\mathbf{R}$. $$\n",
    "\\boldsymbol{ \\Phi}^\\top \\boldsymbol{ \\Phi}\\boldsymbol{\\beta} =\n",
    "\\boldsymbol{ \\Phi}^\\top \\mathbf{ y}\n",
    "$$ $$\n",
    "(\\mathbf{Q}\\mathbf{R})^\\top\n",
    "(\\mathbf{Q}\\mathbf{R})\\boldsymbol{\\beta} = (\\mathbf{Q}\\mathbf{R})^\\top\n",
    "\\mathbf{ y}\n",
    "$$ $$\n",
    "\\mathbf{R}^\\top (\\mathbf{Q}^\\top \\mathbf{Q}) \\mathbf{R}\n",
    "\\boldsymbol{\\beta} = \\mathbf{R}^\\top \\mathbf{Q}^\\top \\mathbf{ y}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{R}^\\top \\mathbf{R} \\boldsymbol{\\beta} = \\mathbf{R}^\\top \\mathbf{Q}^\\top\n",
    "\\mathbf{ y}\n",
    "$$ $$\n",
    "\\mathbf{R} \\boldsymbol{\\beta} = \\mathbf{Q}^\\top \\mathbf{ y}\n",
    "$$\n",
    "\n",
    "This is a more numerically stable solution because it removes the need\n",
    "to compute $\\boldsymbol{ \\Phi}^\\top\\boldsymbol{ \\Phi}$ as an\n",
    "intermediate. Computing $\\boldsymbol{ \\Phi}^\\top\\boldsymbol{ \\Phi}$ is a\n",
    "bad idea because it involves squaring all the elements of\n",
    "$\\boldsymbol{ \\Phi}$ and thereby potentially reducing the numerical\n",
    "precision with which we can represent the solution. Operating on\n",
    "$\\boldsymbol{ \\Phi}$ directly preserves the numerical precision of the\n",
    "model.\n",
    "\n",
    "This can be more particularly seen when we begin to work with *basis\n",
    "functions* in the next session. Some systems that can be resolved with\n",
    "the QR decomposition cannot be resolved by using solve directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q, R = np.linalg.qr(Phi)\n",
    "w = sp.linalg.solve_triangular(R, Q.T@y) \n",
    "w = pd.DataFrame(w, index=Phi.columns)\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basis Function Models\n",
    "\n",
    "We are reviewing models that are *linear* in the parameters. Very often\n",
    "we are interested in *non-linear* predictions. We can make models that\n",
    "are linear in the parameters and given non-linear predictions by\n",
    "introducing non-linear *basis functions*. A common example is the\n",
    "polynomial basis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Basis\n",
    "\n",
    "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
    "class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/polynomial-basis.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/polynomial-basis.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "The polynomial basis combines higher order polynomials together to\n",
    "create the function. For example, the fourth order polynomial has five\n",
    "components to its basis function. $$\n",
    "\\phi_j(x) = x^j\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -n mlai.polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mlai.plot as plot\n",
    "import mlai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "f, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "loc =[[0, 1.4,],\n",
    "      [0, -0.7],\n",
    "      [0.75, -0.2],\n",
    "     [-0.75, -0.2],\n",
    "     [-0.75, 2]]\n",
    "text =['$\\phi(x) = 1$',\n",
    "       '$\\phi(x) = x$',\n",
    "       '$\\phi(x) = x^2$',\n",
    "       '$\\phi(x) = x^3$',\n",
    "       '$\\phi(x) = x^4$']\n",
    "\n",
    "plot.basis(mlai.polynomial, x_min=-1.3, x_max=1.3, \n",
    "           fig=f, ax=ax, loc=loc, text=text, num_basis=5,\n",
    "           diagrams='./ml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://mlatcl.github.io/deepnn/./slides/diagrams//ml/polynomial_basis004.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The set of functions which are combined to form a\n",
    "*polynomial* basis.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods\n",
    "from ipywidgets import IntSlider\n",
    "import notutils as nu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nu.display_plots('polynomial_basis{num_basis:0>3}.svg', \n",
    "                            directory='./ml', \n",
    "                            num_basis=IntSlider(0,0,4,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nu.display_prediction(basis=mlai.polynomial, num_basis=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions Derived from Polynomial Basis\n",
    "\n",
    "$$\n",
    "f(x) = {\\color{red}{w_0}} + {\\color{magenta}{w_1 x}} + {\\color{blue}{w_2 x^2}} + {\\color{green}{w_3 x^3}} + {\\color{cyan}{w_4 x^4}}\n",
    "$$\n",
    "\n",
    "<img src=\"https://mlatcl.github.io/deepnn/./slides/diagrams//ml/polynomial_function002.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>A random combination of functions from the polynomial\n",
    "basis.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods\n",
    "from ipywidgets import IntSlider\n",
    "import notutils as nu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nu.display_plots('polynomial_function{func_num:0>3}.svg', \n",
    "                            directory='./ml', \n",
    "                            func_num=IntSlider(0,0,2,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions from this model, $$\n",
    "f(x) = w_0 + w_1 x+ w_2 x^2 + w_3 x^3 + w_4 x^4\n",
    "$$ are *linear* in the parameters, $\\mathbf{ w}$, but *non-linear* in\n",
    "the input $x^3$. Here we are showing a polynomial basis for a\n",
    "1-dimensional input, $x$, but basis functions can also be constructed\n",
    "for multidimensional inputs, $\\mathbf{ x}$.}\n",
    "\n",
    "In the neural network models, the “RELU function” is normally used as a\n",
    "basis function, but for illustration we will continue with the\n",
    "polynomial basis for these linear models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Olympic Marathon Data\n",
    "\n",
    "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
    "class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_datasets/includes/olympic-marathon-data.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_datasets/includes/olympic-marathon-data.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"70%\">\n",
    "\n",
    "-   Gold medal times for Olympic Marathon since 1896.\n",
    "-   Marathons before 1924 didn’t have a standardized distance.\n",
    "-   Present results using pace per km.\n",
    "-   In 1904 Marathon was badly organized leading to very slow times.\n",
    "\n",
    "</td>\n",
    "<td width=\"30%\">\n",
    "\n",
    "<img class=\"\" src=\"https://mlatcl.github.io/deepnn/./slides/diagrams//Stephen_Kiprotich.jpg\" style=\"width:100%\">\n",
    "<small>Image from Wikimedia Commons <http://bit.ly/16kMKHQ></small>\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "The first thing we will do is load a standard data set for regression\n",
    "modelling. The data consists of the pace of Olympic Gold Medal Marathon\n",
    "winners for the Olympics from 1896 to present. Let’s load in the data\n",
    "and plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pods.datasets.olympic_marathon_men()\n",
    "x = data['X']\n",
    "y = data['Y']\n",
    "\n",
    "offset = y.mean()\n",
    "scale = np.sqrt(y.var())\n",
    "yhat = (y - offset)/scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mlai.plot as plot\n",
    "import mlai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlim = (1875,2030)\n",
    "ylim = (2.5, 6.5)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "_ = ax.plot(x, y, 'r.',markersize=10)\n",
    "ax.set_xlabel('year', fontsize=20)\n",
    "ax.set_ylabel('pace min/km', fontsize=20)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "\n",
    "mlai.write_figure(filename='olympic-marathon.svg', \n",
    "                  directory='./datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://mlatcl.github.io/deepnn/./slides/diagrams//datasets/olympic-marathon.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Olympic marathon pace times since 1896.</i>\n",
    "\n",
    "Things to notice about the data include the outlier in 1904, in that\n",
    "year the Olympics was in St Louis, USA. Organizational problems and\n",
    "challenges with dust kicked up by the cars following the race meant that\n",
    "participants got lost, and only very few participants completed. More\n",
    "recent years see more consistently quick marathons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Fits to Olympic Marthon Data\n",
    "\n",
    "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
    "class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/olympic-marathon-polynomial.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/olympic-marathon-polynomial.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the polynomial basis function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -n mlai.polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial(x, num_basis=4, data_limits=[-1., 1.]):\n",
    "    \"Polynomial basis\"\n",
    "    centre = data_limits[0]/2. + data_limits[1]/2.\n",
    "    span = data_limits[1] - data_limits[0]\n",
    "    z = np.asarray(x, dtype=float) - centre\n",
    "    z = 2*z/span   # scale the inputs to be within -1, 1 where polynomials are well behaved\n",
    "    Phi = np.zeros((x.shape[0], num_basis))\n",
    "    for i in range(num_basis):\n",
    "        Phi[:, i:i+1] = z**i\n",
    "    return Phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we include the solution for the linear regression through\n",
    "QR-decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basis_fit(Phi, y):\n",
    "    \"Use QR decomposition to fit the basis.\"\"\"\n",
    "    Q, R = np.linalg.qr(Phi)\n",
    "    return sp.linalg.solve_triangular(R, Q.T@y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_args = {'num_basis':2, # two basis functions (1 and x)\n",
    "             'data_limits':xlim}\n",
    "Phi = polynomial(x, **poly_args)\n",
    "w = basis_fit(Phi, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make some predictions for the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pred = np.linspace(xlim[0], xlim[1], 400)[:, np.newaxis]\n",
    "Phi_pred = polynomial(x_pred, **poly_args)\n",
    "f_pred = Phi_pred@w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mlai.plot as plot\n",
    "import mlai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "_ = ax.plot(x, y, 'r.',markersize=10)\n",
    "ax.set_xlabel('year', fontsize=20)\n",
    "ax.set_ylabel('pace min/km', fontsize=20)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "\n",
    "_ = ax.plot(x_pred, f_pred, 'b-', linewidth=2)\n",
    "\n",
    "mlai.write_figure(filename='olympic-marathon-polynomial-2.svg', \n",
    "                  directory='./ml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://mlatcl.github.io/deepnn/./slides/diagrams//ml/olympic-marathon-polynomial-2.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Fit of a 1-degree polynomial (a linear model) to the Olympic\n",
    "marathon data.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cubic Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_args = {'num_basis':4, # four basis: 1, x, x^2, x^3\n",
    "             'data_limits':xlim}\n",
    "Phi = polynomial(x, **poly_args)\n",
    "w = basis_fit(Phi, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Phi_pred = polynomial(x_pred, **poly_args)\n",
    "f_pred = Phi_pred@w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mlai.plot as plot\n",
    "import mlai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "_ = ax.plot(x, y, 'r.',markersize=10)\n",
    "ax.set_xlabel('year', fontsize=20)\n",
    "ax.set_ylabel('pace min/km', fontsize=20)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "\n",
    "_ = ax.plot(x_pred, f_pred, 'b-', linewidth=2)\n",
    "\n",
    "mlai.write_figure(filename='olympic-marathon-polynomial-4.svg', \n",
    "                  directory='./ml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://mlatcl.github.io/deepnn/./slides/diagrams//ml/olympic-marathon-polynomial-4.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Fit of a 3-degree polynomial (a cubic model) to the Olympic\n",
    "marathon data.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9th Degree Polynomial Fit\n",
    "\n",
    "Now we’ll try a 9th degree polynomial fit to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_args = {'num_basis':10, # basis up to x^9\n",
    "             'data_limits':xlim}\n",
    "Phi = polynomial(x, **poly_args)\n",
    "w = basis_fit(Phi, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Phi_pred = polynomial(x_pred, **poly_args)\n",
    "f_pred = Phi_pred@w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mlai.plot as plot\n",
    "import mlai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "_ = ax.plot(x, y, 'r.',markersize=10)\n",
    "ax.set_xlabel('year', fontsize=20)\n",
    "ax.set_ylabel('pace min/km', fontsize=20)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "\n",
    "_ = ax.plot(x_pred, f_pred, 'b-', linewidth=2)\n",
    "\n",
    "mlai.write_figure(filename='olympic-marathon-polynomial-10.svg', \n",
    "                  directory='./ml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://mlatcl.github.io/deepnn/./slides/diagrams//ml/olympic-marathon-polynomial-10.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Fit of a 9-degree polynomial to the Olympic marathon\n",
    "data.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16th Degree Polynomial Fit\n",
    "\n",
    "Now we’ll try a 16th degree polynomial fit to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_args = {'num_basis':17, # basis up to x^16\n",
    "             'data_limits':xlim}\n",
    "Phi = polynomial(x, **poly_args)\n",
    "w = basis_fit(Phi, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Phi_pred = polynomial(x_pred, **poly_args)\n",
    "f_pred = Phi_pred@w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mlai.plot as plot\n",
    "import mlai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "_ = ax.plot(x, y, 'r.',markersize=10)\n",
    "ax.set_xlabel('year', fontsize=20)\n",
    "ax.set_ylabel('pace min/km', fontsize=20)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "\n",
    "_ = ax.plot(x_pred, f_pred, 'b-', linewidth=2)\n",
    "\n",
    "mlai.write_figure(filename='olympic-marathon-polynomial-17.svg', \n",
    "                  directory='./ml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://mlatcl.github.io/deepnn/./slides/diagrams//ml/olympic-marathon-polynomial-17.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Fit of a 16-degree polynomial to the Olympic marathon\n",
    "data.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 26th Degree Polynomial Fit\n",
    "\n",
    "Now we’ll try a 26th degree polynomial fit to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_args = {'num_basis':27, # basis up to x^26\n",
    "             'data_limits':xlim}\n",
    "Phi = polynomial(x, **poly_args)\n",
    "w = basis_fit(Phi, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Phi_pred = polynomial(x_pred, **poly_args)\n",
    "f_pred = Phi_pred@w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mlai.plot as plot\n",
    "import mlai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "_ = ax.plot(x, y, 'r.',markersize=10)\n",
    "ax.set_xlabel('year', fontsize=20)\n",
    "ax.set_ylabel('pace min/km', fontsize=20)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "\n",
    "_ = ax.plot(x_pred, f_pred, 'b-', linewidth=2)\n",
    "\n",
    "mlai.write_figure(filename='olympic-marathon-polynomial-27.svg', \n",
    "                  directory='./ml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://mlatcl.github.io/deepnn/./slides/diagrams//ml/olympic-marathon-polynomial-27.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Fit of a 26-degree polynomial to the Olympic marathon\n",
    "data.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bootstrap\n",
    "\n",
    "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
    "class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/the-bootstrap.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/the-bootstrap.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "Bootstrap sampling (Efron, 1979) is an approach to assessing the\n",
    "sensitivity of the model to different variations on a data set. In an\n",
    "ideal world, we’d like to be able to look at different realisations from\n",
    "the original data generating distribution $\\mathbb{P}(y, \\mathbf{ x})$,\n",
    "but this is not available to us.\n",
    "\n",
    "In bootstrap sampling, we take the sample we have, $$\n",
    "\\mathbf{ y}, \\mathbf{X}\\sim \\mathbb{P}(y, \\mathbf{ x})\n",
    "$$ and resample from that data, rather than from the true distribution.\n",
    "So, we have a new data set, $\\hat{\\mathbf{ y}}$, $\\hat{\\mathbf{X}}$\n",
    "which is sampled from the original *with* replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap(X):\n",
    "    \"Return a bootstrap sample from a data set.\"\n",
    "    n = X.shape[0]\n",
    "    ind = np.random.choice(n, n, replace=True) # Sample randomly with replacement.\n",
    "    return X[ind, :]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap and Olympic Marathon Data\n",
    "\n",
    "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
    "class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/olympic-marathon-bootstrap-polynomial.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/olympic-marathon-bootstrap-polynomial.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "First we define a function to bootstrap resample our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap(X, y):\n",
    "    \"Return a bootstrap sample from a data set.\"\n",
    "    n = X.shape[0]\n",
    "    ind = np.random.choice(n, n, replace=True) # Sample randomly with replacement.\n",
    "    return X[ind, :], y[ind, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bootstraps = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_fit(Phi, y, size):\n",
    "    W = np.zeros((Phi.shape[1], size))\n",
    "    for i in range(size):\n",
    "        Phi_hat, y_hat = bootstrap(Phi, y)\n",
    "        W[:, i:i+1] = basis_fit(Phi_hat, y_hat)\n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_args = {'num_basis':2, # two basis functions (1 and x)\n",
    "             'data_limits':xlim}\n",
    "Phi = polynomial(x, **poly_args)\n",
    "W_hat = bootstrap_fit(Phi, y, num_bootstraps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make some predictions for the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pred = np.linspace(xlim[0], xlim[1], 400)[:, np.newaxis]\n",
    "Phi_pred = polynomial(x_pred, **poly_args)\n",
    "f_pred = Phi_pred@W_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mlai.plot as plot\n",
    "import mlai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "_ = ax.plot(x, y, 'r.',markersize=10)\n",
    "ax.set_xlabel('year', fontsize=20)\n",
    "ax.set_ylabel('pace min/km', fontsize=20)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "\n",
    "_ = ax.plot(x_pred, f_pred, 'b-', linewidth=2)\n",
    "\n",
    "mlai.write_figure(filename='olympic-marathon-bootstrap-polynomial-2.svg', \n",
    "                  directory='./ml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://mlatcl.github.io/deepnn/./slides/diagrams//ml/olympic-marathon-bootstrap-polynomial-2.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Fit of a 1 degree polynomial (a linear model) to the olympic\n",
    "marathon data.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cubic Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_args = {'num_basis':4, # four basis: 1, x, x^2, x^3\n",
    "             'data_limits':xlim}\n",
    "Phi = polynomial(x, **poly_args)\n",
    "W_hat = bootstrap_fit(Phi, y, num_bootstraps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Phi_pred = polynomial(x_pred, **poly_args)\n",
    "f_pred = Phi_pred@W_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mlai.plot as plot\n",
    "import mlai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "_ = ax.plot(x, y, 'r.',markersize=10)\n",
    "ax.set_xlabel('year', fontsize=20)\n",
    "ax.set_ylabel('pace min/km', fontsize=20)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "\n",
    "_ = ax.plot(x_pred, f_pred, 'b-', linewidth=2)\n",
    "\n",
    "mlai.write_figure(filename='olympic-marathon-bootstrap-polynomial-4.svg', \n",
    "                  directory='./ml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://mlatcl.github.io/deepnn/./slides/diagrams//ml/olympic-marathon-bootstrap-polynomial-4.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Fit of a 3 degree polynomial (a cubic model) to the olympic\n",
    "marathon data.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9th Degree Polynomial Fit\n",
    "\n",
    "Now we’ll try a 9th degree polynomial fit to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_args = {'num_basis':10, # basis up to x^9\n",
    "             'data_limits':xlim}\n",
    "Phi = polynomial(x, **poly_args)\n",
    "W_hat = bootstrap_fit(Phi, y, num_bootstraps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Phi_pred = polynomial(x_pred, **poly_args)\n",
    "f_pred = Phi_pred@W_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mlai.plot as plot\n",
    "import mlai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "_ = ax.plot(x, y, 'r.',markersize=10)\n",
    "ax.set_xlabel('year', fontsize=20)\n",
    "ax.set_ylabel('pace min/km', fontsize=20)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "\n",
    "_ = ax.plot(x_pred, f_pred, 'b-', linewidth=2)\n",
    "\n",
    "mlai.write_figure(filename='olympic-marathon-bootstrap-polynomial-10.svg', \n",
    "                  directory='./ml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://mlatcl.github.io/deepnn/./slides/diagrams//ml/olympic-marathon-bootstrap-polynomial-10.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Fit of a 9 degree polynomial to the olympic marathon\n",
    "data.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16th Degree Polynomial Fit\n",
    "\n",
    "Now we’ll try a 16th degree polynomial fit to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_args = {'num_basis':17, # basis up to x^16\n",
    "             'data_limits':xlim}\n",
    "Phi = polynomial(x, **poly_args)\n",
    "W_hat = bootstrap_fit(Phi, y, num_bootstraps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Phi_pred = polynomial(x_pred, **poly_args)\n",
    "f_pred = Phi_pred@W_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mlai.plot as plot\n",
    "import mlai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "_ = ax.plot(x, y, 'r.',markersize=10)\n",
    "ax.set_xlabel('year', fontsize=20)\n",
    "ax.set_ylabel('pace min/km', fontsize=20)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "\n",
    "_ = ax.plot(x_pred, f_pred, 'b-', linewidth=2)\n",
    "\n",
    "mlai.write_figure(filename='olympic-marathon-bootstrap-polynomial-17.svg', \n",
    "                  directory='./ml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://mlatcl.github.io/deepnn/./slides/diagrams//ml/olympic-marathon-bootstrap-polynomial-17.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Fit of a 16 degree polynomial to the olympic marathon\n",
    "data.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias Variance Decomposition\n",
    "\n",
    "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
    "class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/bias-variance-dilemma.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/bias-variance-dilemma.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "One of Breiman’s ideas for improving predictive performance is known as\n",
    "bagging (Breiman, 1996). The idea is to train a number of models on the\n",
    "data such that they overfit (high variance). Then average the\n",
    "predictions of these models. The models are trained on different\n",
    "bootstrap samples (Efron, 1979) and their predictions are aggregated\n",
    "giving us the acronym, Bagging. By combining decision trees with\n",
    "bagging, we recover random forests (Breiman, 2001).\n",
    "\n",
    "Bias and variance can also be estimated through Efron’s bootstrap\n",
    "(Efron, 1979), and the traditional view has been that there’s a form of\n",
    "Goldilocks effect, where the best predictions are given by the model\n",
    "that is ‘just right’ for the amount of data available. Not to simple,\n",
    "not too complex. The idea is that bias decreases with increasing model\n",
    "complexity and variance increases with increasing model complexity.\n",
    "Typically plots begin with the Mummy bear on the left (too much bias)\n",
    "end with the Daddy bear on the right (too much variance) and show a dip\n",
    "in the middle where the Baby bear (just) right finds themselves.\n",
    "\n",
    "The Daddy bear is typically positioned at the point where the model can\n",
    "exactly interpolate the data. For a generalized linear model (McCullagh\n",
    "and Nelder, 1989), this is the point at which the number of parameters\n",
    "is equal to the number of data[1].\n",
    "\n",
    "The bias-variance decomposition (Geman et al., 1992) considers the\n",
    "expected test error for different variations of the *training data*\n",
    "sampled from, $\\mathbb{P}(\\mathbf{ x}, y)$ $$\\begin{align*}\n",
    "R(\\mathbf{ w}) = & \\int \\left(y- f^*(\\mathbf{ x})\\right)^2 \\mathbb{P}(y, \\mathbf{ x}) \\text{d}y\\text{d}\\mathbf{ x}\\\\\n",
    "& \\triangleq \\mathbb{E}\\left[ \\left(y- f^*(\\mathbf{ x})\\right)^2 \\right].\n",
    "\\end{align*}$$\n",
    "\n",
    "This can be decomposed into two parts, $$\n",
    "\\begin{align*}\n",
    "\\mathbb{E}\\left[ \\left(y- f(\\mathbf{ x})\\right)^2 \\right] = & \\text{bias}\\left[f^*(\\mathbf{ x})\\right]^2  + \\text{variance}\\left[f^*(\\mathbf{ x})\\right]  +\\sigma^2,\n",
    "\\end{align*}\n",
    "$$ where the bias is given by $$\n",
    "  \\text{bias}\\left[f^*(\\mathbf{ x})\\right] =\n",
    "\\mathbb{E}\\left[f^*(\\mathbf{ x})\\right] - f(\\mathbf{ x})\n",
    "$$ and it summarizes error that arises from the model’s inability to\n",
    "represent the underlying complexity of the data. For example, if we were\n",
    "to model the marathon pace of the winning runner from the Olympics by\n",
    "computing the average pace across time, then that model would exhibit\n",
    "*bias* error because the reality of Olympic marathon pace is it is\n",
    "changing (typically getting faster).\n",
    "\n",
    "The variance term is given by $$\n",
    "  \\text{variance}\\left[f^*(\\mathbf{ x})\\right] = \\mathbb{E}\\left[\\left(f^*(\\mathbf{ x}) - \\mathbb{E}\\left[f^*(\\mathbf{ x})\\right]\\right)^2\\right].\n",
    "  $$ The variance term is often described as arising from a model that\n",
    "is too complex, but we must be careful with this idea. Is the model\n",
    "really too complex relative to the real world that generates the data?\n",
    "The real world is a complex place, and it is rare that we are\n",
    "constructing mathematical models that are more complex than the world\n",
    "around us. Rather, the ‘too complex’ refers to ability to estimate the\n",
    "parameters of the model given the data we have. Slight variations in the\n",
    "training set cause changes in prediction.\n",
    "\n",
    "Models that exhibit high variance are sometimes said to ‘overfit’ the\n",
    "data whereas models that exhibit high bias are sometimes described as\n",
    "‘underfitting’ the data.\n",
    "\n",
    "Also related on generalization error is the so called ‘no free lunch\n",
    "theorem,’ which refers to our inability to decide what a better learning\n",
    "algorithm is without making assumptions about the data (Wolpert, 1996)\n",
    "(see also Wolpert (2002)).\n",
    "\n",
    "[1] Assuming we are ignoring parameters in the link function and the\n",
    "distribution function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
    "class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/linear-regression-regularisation.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/linear-regression-regularisation.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "The solution to the linear system is given by solving, $$\n",
    "\\boldsymbol{ \\Phi}^\\top\\boldsymbol{ \\Phi}\\mathbf{ w}= \\boldsymbol{ \\Phi}^\\top\\mathbf{ y}\n",
    "$$ for $\\mathbf{ w}$.\n",
    "\n",
    "But if $\\boldsymbol{ \\Phi}^\\top\\boldsymbol{ \\Phi}$ is not full rank,\n",
    "this system cannot be solved. This is reflective of an *underdetermined\n",
    "system* of equations. There are *infinite* solutions. This happens when\n",
    "there are more basis functions than data points, in effect the number of\n",
    "data we have is not enough to determine the parameters.\n",
    "\n",
    "Thinking about this in terms of the Hessian, if\n",
    "$\\boldsymbol{ \\Phi}^\\top\\boldsymbol{ \\Phi}$ is not full rank, then we\n",
    "are no longer at a minimum. We are in a trough, because *not full rank*\n",
    "implies that there are fewer eigenvectors than dimensions, in effect for\n",
    "those dimensions where there is no eigenvector, the objective function\n",
    "is ‘flat.’ It is ambivalent to changes in parameters. This implies there\n",
    "are infinite valid solutions.\n",
    "\n",
    "One solution to this problem is to regularize the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coefficient Shrinkage\n",
    "\n",
    "Coefficient shrinkage is a technique where the parameters of the of the\n",
    "model are ‘encouraged’ to be small. In practice this is normally done by\n",
    "augmenting the objective function with a term that keeps the parameters\n",
    "low, typically by penalizing a norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tikhonov Regularization\n",
    "\n",
    "In neural network models this approach is sometimes called ‘weight\n",
    "decay.’ At every gradient step we reduce the value of the weight a\n",
    "little. This idea comes from an approach called Tikhonov regularization\n",
    "(Tikhonov and Arsenin, 1977), where the objective function is augmented\n",
    "by the L2 norm of the weights, $$\n",
    "L(\\mathbf{ w}) = (\\mathbf{ y}- \\mathbf{ f})^\\top(\\mathbf{ y}- \\mathbf{ f}) + \\alpha\\left\\Vert \\mathbf{W} \\right\\Vert_2^2\n",
    "$$ with some weighting $\\alpha >0$. This has the effect of changing the\n",
    "Hessian at the minimum to $$\n",
    "\\boldsymbol{ \\Phi}^\\top\\boldsymbol{ \\Phi}+ \\alpha \\mathbf{I}\n",
    "$$ Which is always full rank. The minimal eigenvalues are now given by\n",
    "$\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso\n",
    "\n",
    "Other techniques for regularization based on a norm of the parameters\n",
    "include the Lasso (Tibshirani, 1996), which is an L1 norm of the\n",
    "parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splines, Functions, Hilbert Kernels\n",
    "\n",
    "Regularization of the parameters has the desired effect of making the\n",
    "solution viable, but it can sometimes be difficult to interpret,\n",
    "particularly if the parameters don’t have any inherent meaning (like in\n",
    "a neural network). An alternative approach is to regularize the\n",
    "function, $\\mathbf{ f}$, directly, (see e.g., Kimeldorf and Wahba (1970)\n",
    "and Wahba (1990)). This is the approach taken by *spline models* which\n",
    "use energy-based regularization for $f(\\cdot)$ and also *kernel methods*\n",
    "such as the support vector machine (Schölkopf and Smola, 2001)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Noise\n",
    "\n",
    "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
    "class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/training-with-noise-tikhonov-regularisation.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/training-with-noise-tikhonov-regularisation.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "In practice, and across the last two waves of neural networks, other\n",
    "techniques for regularization have been used which can be seen as\n",
    "perturbing the neural network in some way. For example, in dropout\n",
    "(Srivastava et al., 2014), different basis functions are eliminated from\n",
    "the gradient computation at each gradient update.\n",
    "\n",
    "Many of these perturbations have some form of regularizing effect. The\n",
    "exact nature of the effect is not always easy to characterize, but in\n",
    "some cases, we can assess how these manipulations effect the model. For\n",
    "example, Bishop (1995) analyzed training with ‘noisy inputs’ and showed\n",
    "conditions under which it’s equivalent to Tikhonov regularization.\n",
    "\n",
    "But in general, these approaches can have different interpretations and\n",
    "they’ve also been related to ensemble learning (e.g. related to\n",
    "*bagging* or Bayesian approaches).\n",
    "\n",
    "<!--include{_ml/includes/bayesian-interpretation-of-regularisation.md}-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shallow and Deep Learning\n",
    "\n",
    "So far, we have been talking about *linear models* or *shallow learning*\n",
    "as we might think of it. Let’s pause for a moment and consider a *fully\n",
    "connected* deep neural network model to relate the two ideas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Network\n",
    "\n",
    "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
    "class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_deepnn/includes/deep-neural-network.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_deepnn/includes/deep-neural-network.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install daft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "# Comment for google colab (no latex available)\n",
    "#matplotlib.rc('text', usetex=True)\n",
    "#matplotlib.rcParams['text.latex.preamble']=[r\"\\usepackage{amsmath}\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlai.plot as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot.deep_nn(diagrams='./deepgp/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://mlatcl.github.io/deepnn/./slides/diagrams//deepgp/deep-nn2.svg\" class=\"\" width=\"70%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>A deep neural network. Input nodes are shown at the bottom.\n",
    "Each hidden layer is the result of applying an affine transformation to\n",
    "the previous layer and placing through an activation function.</i>\n",
    "\n",
    "Mathematically, each layer of a neural network is given through\n",
    "computing the activation function, $\\phi(\\cdot)$, contingent on the\n",
    "previous layer, or the inputs. In this way the activation functions, are\n",
    "composed to generate more complex interactions than would be possible\n",
    "with any single layer. $$\n",
    "\\begin{align*}\n",
    "    \\mathbf{ h}_{1} &= \\phi\\left(\\mathbf{W}_1 \\mathbf{ x}\\right)\\\\\n",
    "    \\mathbf{ h}_{2} &=  \\phi\\left(\\mathbf{W}_2\\mathbf{ h}_{1}\\right)\\\\\n",
    "    \\mathbf{ h}_{3} &= \\phi\\left(\\mathbf{W}_3 \\mathbf{ h}_{2}\\right)\\\\\n",
    "    f&= \\mathbf{ w}_4 ^\\top\\mathbf{ h}_{3}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Under our basis function perspective, we can see that our deep neural\n",
    "network is mathematical composition of basis function models. Each layer\n",
    "contains a separate basis function set, so $$\n",
    " f(\\mathbf{ x}; \\mathbf{W})  =  \\mathbf{ w}_4 ^\\top\\phi\\left(\\mathbf{W}_3 \\phi\\left(\\mathbf{W}_2\\phi\\left(\\mathbf{W}_1 \\mathbf{ x}\\right)\\right)\\right).\n",
    "$$\n",
    "\n",
    "In this course there are two reasons for looking at the shallow model.\n",
    "Firstly, it is easier to introduce the concepts of regularisation in the\n",
    "linear model regime. Secondly, the matrix forms we see, e.g.,\n",
    "expressions like $\\boldsymbol{ \\Phi}^\\top \\boldsymbol{ \\Phi}$, appear in\n",
    "both models.\n",
    "\n",
    "For deep learning, we can no longer optimize the parameters of the model\n",
    "through solving a linear system[1]. Instead, we need to turn to\n",
    "non-linear optimization algorithms. For deep learning, that’s typically\n",
    "stochastic gradient descent.\n",
    "\n",
    "While it’s possible to compute the Hessian in a neural network, Bishop\n",
    "(1992), we also find that it varies across the parameter space and will\n",
    "not normally be positive definite. In practice, the number of parameters\n",
    "is normally so large that storing the Hessian is impossible (it has\n",
    "quadratic cost in the number of weights/parameters) due to memory\n",
    "constraints.\n",
    "\n",
    "This means that while the theory of minima in optimization is well\n",
    "understood, empirical experiments with large neural networks are hard\n",
    "and the lessons of small models do not all translate to the very large\n",
    "systems.\n",
    "\n",
    "We can stay within the framework of linear models but take a step closer\n",
    "to neural network models by introducing functions that are non-linear in\n",
    "the inputs, $\\mathbf{ x}$, known as *basis functions*.\n",
    "\n",
    "[1] Apart from the last layer of parmeters in models with quadratic loss\n",
    "functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overparameterised Systems\n",
    "\n",
    "If we could examine the Hessian of a neural network at its minimum, we\n",
    "can speculate about what we would find. In particular, we would find\n",
    "that it would have very many low (or negative) eigenvalues in many\n",
    "directions. This is indicative of the parameters being *badly\n",
    "determined* because of the neural network model being heavily\n",
    "*overparameterized*. So how does it generalize?\n",
    "\n",
    "Simply put, there is not enough regularization encoded in the objective\n",
    "function of the neural network models we are using to explain the\n",
    "generalization performance. There must be something in the algorithms we\n",
    "are using that causes these highly overparameterized models to\n",
    "generalise well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double Descent\n",
    "\n",
    "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
    "class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_deepnn/includes/double-descent.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_deepnn/includes/double-descent.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "<img class=\"\" src=\"https://mlatcl.github.io/deepnn/./slides/diagrams//ml/double-descent.png\" style=\"width:100%\">\n",
    "\n",
    "Figure: <i>*Left* traditional perspective on generalization. There is a\n",
    "sweet spot of operation where the training error is still non-zero.\n",
    "Overfitting occurs when the variance increases. *Right* The double\n",
    "descent phenomenon, the modern models operate in an interpolation regime\n",
    "where they reconstruct the training data fully but are well regularized\n",
    "in their interpolations for test data. Figure from Belkin et al.\n",
    "(2019).</i>\n",
    "\n",
    "But the modern empirical finding is that when we move beyond Daddy bear,\n",
    "into the dark forest of the massively overparameterized model we can\n",
    "achieve good generalization. Indeed, recent work is showing that large\n",
    "language models are even *memorizing* data (Carlini et al., 2020) like\n",
    "non-parametric models do.\n",
    "\n",
    "As Zhang et al. (2017) starkly illustrated with their random labels\n",
    "experiment, within the dark forest there are some terrible places, big\n",
    "bad wolves of overfitting that will gobble up your model. But as\n",
    "empirical evidence shows there is also a safe and hospitable Grandma’s\n",
    "house where these highly overparameterized models are safely consumed.\n",
    "Fundamentally, it must be about the route you take through the forest,\n",
    "and the precautions you take to ensure the wolf doesn’t see where you’re\n",
    "going and beat you to the door.\n",
    "\n",
    "There are two implications of this empirical result. Firstly, that there\n",
    "is a great deal of new theory that needs to be developed. Secondly, that\n",
    "theory is now obliged to conflate two aspects to modelling that we\n",
    "generally like to keep separate: the model and the algorithm.\n",
    "\n",
    "Classical statistical theory around predictive generalization focusses\n",
    "specifically on the class of models that is being used for data fitting.\n",
    "Historically, whether that theory follows a Fisher-aligned estimation\n",
    "approach (see e.g., Vapnik (1998)) or model-based Bayesian approach (see\n",
    "e.g., Ghahramani (2015)), neither is fully equipped to deal with these\n",
    "new circumstances because, to continue our rather tortured analogy,\n",
    "these theories provide us with a characterization of the *destination*\n",
    "of the algorithm, and seek to ensure that we reach that destination.\n",
    "Modern machine learning requires theories of the *journey* and what our\n",
    "route through the forest should be.\n",
    "\n",
    "Crucially, the destination is always associated with 100% accuracy on\n",
    "the training set. An objective that is always achievable for the\n",
    "overparameterized model.\n",
    "\n",
    "Intuitively, it seems that a highly overparameterized model places\n",
    "Grandma’s house on the edge of the dark forest. Making it easily and\n",
    "quickly accessible to the algorithm. The larger the model, the more\n",
    "exposed Grandma’s house becomes. Perhaps this is due to some form of\n",
    "blessing of dimensionality brings Grandma’s house closer to the edge of\n",
    "the forest in a high dimensional setting. Really, we should think of\n",
    "Grandma’s house as a low dimensional manifold of destinations that are\n",
    "safe. A path through the forest where the wolf of overfitting doesn’t\n",
    "venture. In the GLM case, we know already that when the number of\n",
    "parameters matches the number of data there is precisely one location in\n",
    "parameter space where accuracy on the training data is 100%. Our\n",
    "previous misunderstanding of generalization stemmed from the fact that\n",
    "(seemingly) it is highly unlikely that this single point is a good place\n",
    "to be from the perspective of generalization. Additionally, it is often\n",
    "difficult to find. Finding the precise polynomial coefficients in a\n",
    "least squares regression to exactly fit the basis to a small data set\n",
    "such as the Olympic marathon data requires careful consideration of the\n",
    "numerical properties and an orthogonalization of the design matrix\n",
    "(Lawson and Hanson, 1995).\n",
    "\n",
    "It seems that with a highly overparameterized model, these locations\n",
    "become easier to find and they provide good generalization properties.\n",
    "In machine learning this is known as the “double descent phenomenon”\n",
    "(see e.g., Belkin et al. (2019)).\n",
    "\n",
    "See also this talk by Misha Belkin:\n",
    "<http://www.ipam.ucla.edu/abstract/?tid=15552&pcode=GLWS4> and these\n",
    "related papers <https://www.pnas.org/content/116/32/15849.short>,\n",
    "<https://www.pnas.org/content/117/20/10625>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Tangent Kernel\n",
    "\n",
    "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
    "class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_deepnn/includes/neural-tangent-kernel.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_deepnn/includes/neural-tangent-kernel.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "Another approach to analysis exploits the fact that optimization is\n",
    "occurring in a very high dimensional parameter space. By considering\n",
    "initializations that involve small random weights (known as the NTK\n",
    "initialization) and noting that small updates in the learning mean that\n",
    "the model doesn’t move far from this initialization (Jacot et al.,\n",
    "2018).\n",
    "\n",
    "For very wide neural networks, when these conditions are fulfilled, the\n",
    "network can be approximately represented by a *kernel* known as the\n",
    "neural tangent kernel. A kernel is a regularizer that operates in\n",
    "*function space* rather than *feature space*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization in Optimization\n",
    "\n",
    "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
    "class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_deepnn/includes/regularisation-in-optimisation.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_deepnn/includes/regularisation-in-optimisation.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "Another interesting theoretical direction is to study the path that\n",
    "neural network algorithms take when finding the optima. For certain\n",
    "simple linear systems, you can analytically study the ‘gradient flow.’\n",
    "\n",
    "Neural networks are normally trained by (stochastic) gradient descent.\n",
    "This is a discrete optimization algorithm where at each point, a step in\n",
    "the direction of the (approximate) gradient is taken.\n",
    "\n",
    "Gradient flow replaces this discrete update with a differential\n",
    "equation, where the step at any point is an exact gradient update. As a\n",
    "result, the path of the optimization can be studied as a *differential\n",
    "equation*.\n",
    "\n",
    "By making assumptions about the initialization, the optimum that\n",
    "gradient flow will find can be characterised. For a highly\n",
    "overparameterized linear model, Gunasekar et al. (2017) show in matrix\n",
    "factorization, that for particular initializations, the optimum will be\n",
    "a *global* optimum of the objective that minimizes the L2-norm.\n",
    "\n",
    "By reparameterizing the linear model so that each $w_i = u_i^2 - v_i^2$\n",
    "and optimising in the space defined by $\\mathbf{u}$ and $\\mathbf{v}$\n",
    "Woodworth et al. (2020) show that the L1 norm is found.\n",
    "\n",
    "Other papers have looked at *deep linear models* (Arora et al., 2019)\n",
    "where $$\n",
    "f(\\mathbf{ x}; \\mathbf{W}) = \\mathbf{W}_4 \\mathbf{W}_3 \\mathbf{W}_2 \\mathbf{W}_1 \\mathbf{ x}.\n",
    "$$ In these models, a gradient flow analysis shows that the model finds\n",
    "solutions where the linear mapping, $$\n",
    "\\mathbf{W}= \\mathbf{W}_4 \\mathbf{W}_3 \\mathbf{W}_2 \\mathbf{W}_1 \n",
    "$$ is very low rank. This is highly suggestive of another type of\n",
    "regularization that could be occurring in deep neural networks. Low rank\n",
    "parameter matrices mean that the effective capacity of the neural\n",
    "network is reduced. Indeed, empirical observations of the rank of deep\n",
    "nets trained on data suggest that they may be finding such solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thanks!\n",
    "\n",
    "For more information on these subjects and more you might want to check\n",
    "the following resources.\n",
    "\n",
    "-   twitter: [@lawrennd](https://twitter.com/lawrennd)\n",
    "-   podcast: [The Talking Machines](http://thetalkingmachines.com)\n",
    "-   newspaper: [Guardian Profile\n",
    "    Page](http://www.theguardian.com/profile/neil-lawrence)\n",
    "-   blog:\n",
    "    [http://inverseprobability.com](http://inverseprobability.com/blog.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "bootstrap\n",
    "\n",
    "David Hogg’s lecture\n",
    "<https://speakerdeck.com/dwhgg/linear-regression-with-huge-numbers-of-parameters>\n",
    "\n",
    "The Deep Bootstrap\n",
    "<https://twitter.com/PreetumNakkiran/status/1318007088321335297?s=20>\n",
    "\n",
    "Aki Vehtari on Leave One Out Uncertainty:\n",
    "<https://arxiv.org/abs/2008.10296> (check for his references)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arora, S., Cohen, N., Golowich, N., Hu, W., 2019. A convergence analysis\n",
    "of gradient descent for deep linear neural networks, in: International\n",
    "Conference on Learning Representations.\n",
    "\n",
    "Belkin, M., Hsu, D., Ma, S., Soumik Mandal, and, 2019. Reconciling\n",
    "modern machine-learning practice and the classical bias-variance\n",
    "trade-off. Proc. Natl. Acad. Sci. USA 116, 15849–15854.\n",
    "\n",
    "Bishop, C.M., 1995. Training with noise is equivalent to Tikhonov\n",
    "regularization. Neural Computation 7, 108–116.\n",
    "<https://doi.org/10.1162/neco.1995.7.1.108>\n",
    "\n",
    "Bishop, C.M., 1992. Exact calculation of the hessian matrix for the\n",
    "multilayer perceptron. Neural Computation 4, 494–501.\n",
    "<https://doi.org/10.1162/neco.1992.4.4.494>\n",
    "\n",
    "Breiman, L., 2001. Random forests. Mach. Learn. 45, 5–32.\n",
    "<https://doi.org/10.1023/A:1010933404324>\n",
    "\n",
    "Breiman, L., 1996. Bagging predictors. Machine Learning 24, 123–140.\n",
    "<https://doi.org/10.1007/BF00058655>\n",
    "\n",
    "Carlini, N., Tramèr, F., Wallace, E., Jagielski, M., Herbert-Voss, A.,\n",
    "Lee, K., Roberts, A., Brown, T., Song, D., Erlingsson, U., Oprea, A.,\n",
    "Raffel, C., 2020. Extracting training data from large language models.\n",
    "\n",
    "Efron, B., 1979. Bootstrap methods: Another look at the jackkife. Annals\n",
    "of Statistics 7, 1–26.\n",
    "\n",
    "Geman, S., Bienenstock, E., Doursat, R., 1992. Neural networks and the\n",
    "bias/variance dilemma. Neural Computation 4, 1–58.\n",
    "<https://doi.org/10.1162/neco.1992.4.1.1>\n",
    "\n",
    "Ghahramani, Z., 2015. Probabilistic machine learning and artificial\n",
    "intelligence. Nature 452–459.\n",
    "\n",
    "Gunasekar, S., Woodworth, B., Bhojanapalli, S., Neyshabur, B., Srebro,\n",
    "N., 2017. Implicit regularization in matrix factorization.\n",
    "\n",
    "Jacot, A., Gabriel, F., Hongler, C., 2018. Neural tangent kernel:\n",
    "Convergence and generalization in neural networks, in: Bengio, S.,\n",
    "Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., Garnett, R.\n",
    "(Eds.), Advances in Neural Information Processing Systems. Curran\n",
    "Associates, Inc., pp. 8571–8580.\n",
    "\n",
    "Kimeldorf, G.S., Wahba, G., 1970. A correspondence between Bayesian\n",
    "estimation of stochastic processes and smoothing by splines. Annals of\n",
    "Mathematical Statistics 41, 495–502.\n",
    "\n",
    "Lawson, C.L., Hanson, R.J., 1995. Solving least squares problems. SIAM.\n",
    "<https://doi.org/10.1137/1.9781611971217>\n",
    "\n",
    "McCullagh, P., Nelder, J.A., 1989. Generalized linear models, 2nd ed.\n",
    "Chapman; Hall.\n",
    "\n",
    "Schölkopf, B., Smola, A.J., 2001. Learning with kernels. mit, Cambridge,\n",
    "MA.\n",
    "\n",
    "Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I.,\n",
    "Salakhutdinov, R., 2014. Dropout: A simple way to prevent neural\n",
    "networks from overfitting. Journal of Machine Learning Research 15,\n",
    "1929–1958.\n",
    "\n",
    "The Office of the Senior Special Assistant to the President on the\n",
    "Millennium Development Goals (OSSAP-MDGs), Columbia University, 2014.\n",
    "Nigeria NMIS facility database.\n",
    "\n",
    "Tibshirani, R., 1996. Regression shrinkage and selection via the lasso.\n",
    "Journal of the Royal Statistical Society. Series B (Methodological) 58,\n",
    "267–288.\n",
    "\n",
    "Tikhonov, A.N., Arsenin, V.Y., 1977. Solutions of ill-posed problems. V.\n",
    "H. Winston, Washington, DC.\n",
    "\n",
    "Vapnik, V.N., 1998. Statistical learning theory. wiley, New York.\n",
    "\n",
    "Wahba, G., 1990. Spline models for observational data, First. ed. SIAM.\n",
    "<https://doi.org/10.1137/1.9781611970128>\n",
    "\n",
    "Wolpert, D.H., 2002. The supervised learning no-free-lunch theorems, in:\n",
    "Roy, R., Köppen, M., Ovaska, S., Furuhashi, T., Hoffmann, F. (Eds.),\n",
    "Soft Computing and Industry. Springer, London, pp. 25–.\n",
    "<https://doi.org/10.1007/978-1-4471-0123-9_3>\n",
    "\n",
    "Wolpert, D.H., 1996. The lack of a priori distinctions between learning\n",
    "algorithms. Neural Computation 8.\n",
    "<https://doi.org/10.1162/neco.1996.8.7.1341>\n",
    "\n",
    "Woodworth, B., Gunasekar, S., Lee, J.D., Moroshko, E., Savarese, P.,\n",
    "Golan, I., Soudry, D., Srebro, N., 2020. Kernel and rich regimes in\n",
    "overparametrized models.\n",
    "\n",
    "Zhang, C., Bengio, S., Hardt, M., Recht, B., Vinyals, O., 2017.\n",
    "Understanding deep learning requires rethinking generalization, in:\n",
    "https://openreview.net/forum?id=Sy8gdB9xx (Ed.), International\n",
    "Conference on Learning Representations."
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
