{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalization and Neural Networks\n",
    "==================================\n",
    "\n",
    "### [Neil D. Lawrence](http://inverseprobability.com)\n",
    "\n",
    "### 2021-01-26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abstract**: This lecture will cover generalization in machine learning\n",
    "with a particular focus on neural architectures. We will review\n",
    "classical generalization and explore what’s different about neural\n",
    "network models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Do not edit this file locally. -->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!---->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->\n",
    "<!--\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup\n",
    "-----\n",
    "\n",
    "First we download some libraries and files to support the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve('https://raw.githubusercontent.com/lawrennd/talks/gh-pages/ndlml.py','ndlml.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve('https://raw.githubusercontent.com/lawrennd/talks/gh-pages/teaching_plots.py','teaching_plots.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve('https://raw.githubusercontent.com/lawrennd/talks/gh-pages/gp_tutorial.py','gp_tutorial.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 22})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--setupplotcode{import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_context('paper')\n",
    "sns.set_palette('colorblind')}-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pods\n",
    "----\n",
    "\n",
    "In Sheffield we created a suite of software tools for ‘Open Data\n",
    "Science’. Open data science is an approach to sharing code, models and\n",
    "data that should make it easier for companies, health professionals and\n",
    "scientists to gain access to data science techniques.\n",
    "\n",
    "You can also check this blog post on [Open Data\n",
    "Science](http://inverseprobability.com/2014/07/01/open-data-science).\n",
    "\n",
    "The software can be installed using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade git+https://github.com/sods/ods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from the command prompt where you can access your python installation.\n",
    "\n",
    "The code is also available on github:\n",
    "<a href=\"https://github.com/sods/ods\" class=\"uri\">https://github.com/sods/ods</a>\n",
    "\n",
    "Once `pods` is installed, it can be imported in the usual manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias Variance Decomposition\n",
    "---------------------------\n",
    "\n",
    "The bias-variance decomposition considers the expected test error for\n",
    "different variations of the *training data* sampled from,\n",
    "$\\Pr(\\mathbf{ y}, y)$ $$\n",
    "\\mathbb{E}\\left[ \\left(y- f^*(\\mathbf{ y})\\right)^2 \\right].\n",
    "$$ This can be decomposed into two parts, $$\n",
    "\\mathbb{E}\\left[ \\left(y- f(\\mathbf{ y})\\right)^2 \\right] = \\text{bias}\\left[f^*(\\mathbf{ y})\\right]^2 + \\text{variance}\\left[f^*(\\mathbf{ y})\\right] +\\sigma^2,\n",
    "$$ where the bias is given by $$\n",
    "  \\text{bias}\\left[f^*(\\mathbf{ y})\\right] =\n",
    "\\mathbb{E}\\left[f^*(\\mathbf{ y})\\right] * f(\\mathbf{ y})\n",
    "$$ and it summarizes error that arises from the model’s inability to\n",
    "represent the underlying complexity of the data. For example, if we were\n",
    "to model the marathon pace of the winning runner from the Olympics by\n",
    "computing the average pace across time, then that model would exhibit\n",
    "*bias* error because the reality of Olympic marathon pace is it is\n",
    "changing (typically getting faster).\n",
    "\n",
    "The variance term is given by $$\n",
    "  \\text{variance}\\left[f^*(\\mathbf{ y})\\right] = \\mathbb{E}\\left[\\left(f^*(\\mathbf{ y}) - \\mathbb{E}\\left[f^*(\\mathbf{ y})\\right]\\right)^2\\right].\n",
    "  $$ The variance term is often described as arising from a model that\n",
    "is too complex, but we have to be careful with this idea. Is the model\n",
    "really too complex relative to the real world that generates the data?\n",
    "The real world is a complex place, and it is rare that we are\n",
    "constructing mathematical models that are more complex than the world\n",
    "around us. Rather, the ‘too complex’ refers to ability to estimate the\n",
    "parameters of the model given the data we have. Slight variations in the\n",
    "training set cause changes in prediction.\n",
    "\n",
    "Models that exhibit high variance are sometimes said to ‘overfit’ the\n",
    "data whereas models that exhibit high bias are sometimes described as\n",
    "‘underfitting’ the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias vs Variance Error Plots\n",
    "----------------------------\n",
    "\n",
    "Helper function for sampling data from two different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(per_cluster=30):\n",
    "    \"\"\"Create a randomly sampled data set\n",
    "    \n",
    "    :param per_cluster: number of points in each cluster\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    scale = 3\n",
    "    prec = 1/(scale*scale)\n",
    "    pos_mean = [[-1, 0],[0,0.5],[1,0]]\n",
    "    pos_cov = [[prec, 0.], [0., prec]]\n",
    "    neg_mean = [[0, -0.5],[0,-0.5],[0,-0.5]]\n",
    "    neg_cov = [[prec, 0.], [0., prec]]\n",
    "    for mean in pos_mean:\n",
    "        X.append(np.random.multivariate_normal(mean=mean, cov=pos_cov, size=per_class))\n",
    "        y.append(np.ones((per_class, 1)))\n",
    "    for mean in neg_mean:\n",
    "        X.append(np.random.multivariate_normal(mean=mean, cov=neg_cov, size=per_class))\n",
    "        y.append(np.zeros((per_class, 1)))\n",
    "    return np.vstack(X), np.vstack(y).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function for plotting the decision boundary of the SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_contours(ax, cl, xx, yy, **params):\n",
    "    \"\"\"Plot the decision boundaries for a classifier.\n",
    "\n",
    "    :param ax: matplotlib axes object\n",
    "    :param cl: a classifier\n",
    "    :param xx: meshgrid ndarray\n",
    "    :param yy: meshgrid ndarray\n",
    "    :param params: dictionary of params to pass to contourf, optional\n",
    "    \"\"\"\n",
    "    Z = cl.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Plot decision boundary and regions\n",
    "    out = ax.contour(xx, yy, Z, \n",
    "                     levels=[-1., 0., 1], \n",
    "                     colors='black', \n",
    "                     linestyles=['dashed', 'solid', 'dashed'])\n",
    "    out = ax.contourf(xx, yy, Z, \n",
    "                     levels=[Z.min(), 0, Z.max()], \n",
    "                     colors=[[0.5, 1.0, 0.5], [1.0, 0.5, 0.5]])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve('https://raw.githubusercontent.com/lawrennd/talks/gh-pages/mlai.py','mlai.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlai\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_boundary_plot(models, X, y, axs, filename, directory, titles, xlim, ylim):\n",
    "    \"\"\"Plot a decision boundary on the given axes\n",
    "    \n",
    "    :param axs: the axes to plot on.\n",
    "    :param models: the SVM models to plot\n",
    "    :param titles: the titles for each axis\n",
    "    :param X: input training data\n",
    "    :param y: target training data\"\"\"\n",
    "    for ax in axs.flatten():\n",
    "        ax.clear()\n",
    "    X0, X1 = X[:, 0], X[:, 1]\n",
    "    if xlim is None:\n",
    "        xlim = [X0.min()-1, X0.max()+1]\n",
    "    if ylim is None:\n",
    "        ylim = [X1.min()-1, X1.max()+1]\n",
    "    xx, yy = np.meshgrid(np.arange(xlim[0], xlim[1], 0.02),\n",
    "                         np.arange(ylim[0], ylim[1], 0.02))\n",
    "    for cl, title, ax in zip(models, titles, axs.flatten()):\n",
    "        plot_contours(ax, cl, xx, yy,\n",
    "                      cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "        ax.plot(X0[y==1], X1[y==1], 'r.', markersize=10)\n",
    "        ax.plot(X0[y==0], X1[y==0], 'g.', markersize=10)\n",
    "        ax.set_xlim(xlim)\n",
    "        ax.set_ylim(ylim)\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "        ax.set_title(title)\n",
    "        mlai.write_figure(filename,\n",
    "                          directory=directory,\n",
    "                          figure=fig,\n",
    "                          transparent=True)\n",
    "    return xlim, ylim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "font = {'family' : 'sans',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 22}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of SVM and fit the data. \n",
    "C = 100.0  # SVM regularization parameter\n",
    "gammas = [0.001, 0.01, 0.1, 1]\n",
    "\n",
    "\n",
    "per_class=30\n",
    "num_samps = 20\n",
    "# Set-up 2x2 grid for plotting.\n",
    "fig, ax = plt.subplots(1, 4, figsize=(10,3))\n",
    "xlim=None\n",
    "ylim=None\n",
    "for samp in range(num_samps):\n",
    "    X, y=create_data(per_class)\n",
    "    models = []\n",
    "    titles = []\n",
    "    for gamma in gammas:\n",
    "        models.append(svm.SVC(kernel='rbf', gamma=gamma, C=C))\n",
    "        titles.append('$\\gamma={}$'.format(gamma))\n",
    "    models = (cl.fit(X, y) for cl in models)\n",
    "    xlim, ylim = decision_boundary_plot(models, X, y, \n",
    "                           axs=ax, \n",
    "                           filename='bias-variance{samp:0>3}.svg'.format(samp=samp), \n",
    "                           directory='./ml'\n",
    "                           titles=titles,\n",
    "                          xlim=xlim,\n",
    "                          ylim=ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods\n",
    "from ipywidgets import IntSlider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('bias-variance{samp:0>3}.svg', \n",
    "                            directory='./ml', \n",
    "                            samp=IntSlider(0,0,10,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!---->\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/ml/bias-variance000.png\" style=\"width:80%\"><img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/ml/bias-variance010.png\" style=\"width:80%\">\n",
    "\n",
    "Figure: <i>In each figure the simpler model is on the left, and the more\n",
    "complex model is on the right. Each fit is done to a different version\n",
    "of the data set. The simpler model is more consistent in its errors\n",
    "(bias error), whereas the more complex model is varying in its errors\n",
    "(variance error).</i>\n",
    "\n",
    "Bias variance dilemma\n",
    "<a href=\"https://www.mitpressjournals.org/doi/abs/10.1162/neco.1992.4.1.1\" class=\"uri\">https://www.mitpressjournals.org/doi/abs/10.1162/neco.1992.4.1.1</a>\n",
    "\n",
    "bootstrap\n",
    "\n",
    "Bootstrap Predication and Bayesian Misspecified Models:\n",
    "<a href=\"https://www.jstor.org/stable/3318894#metadata_info_tab_contents\" class=\"uri\">https://www.jstor.org/stable/3318894#metadata_info_tab_contents</a>\n",
    "\n",
    "Edwin Fong and Chris Holmes: On the Marginal Likelihood and Cross\n",
    "Validation\n",
    "<a href=\"https://arxiv.org/abs/1905.08737\" class=\"uri\">https://arxiv.org/abs/1905.08737</a>\n",
    "\n",
    "The lack of a priori distinction between learning algorithms (No free\n",
    "lunch)\n",
    "<a href=\"https://www.mitpressjournals.org/doi/abs/10.1162/neco.1996.8.7.1341\" class=\"uri\">https://www.mitpressjournals.org/doi/abs/10.1162/neco.1996.8.7.1341</a>\n",
    "<a href=\"https://link.springer.com/chapter/10.1007/978-1-4471-0123-9_3\" class=\"uri\">https://link.springer.com/chapter/10.1007/978-1-4471-0123-9_3</a>\n",
    "\n",
    "David Hogg’s lecture\n",
    "<a href=\"https://speakerdeck.com/dwhgg/linear-regression-with-huge-numbers-of-parameters\" class=\"uri\">https://speakerdeck.com/dwhgg/linear-regression-with-huge-numbers-of-parameters</a>\n",
    "\n",
    "Belkin on Bias/Variance\n",
    "<a href=\"https://www.pnas.org/content/116/32/15849.short\" class=\"uri\">https://www.pnas.org/content/116/32/15849.short</a>\n",
    "<a href=\"https://www.pnas.org/content/117/20/10625\" class=\"uri\">https://www.pnas.org/content/117/20/10625</a>\n",
    "\n",
    "Belkin Talk:\n",
    "<a href=\"http://www.ipam.ucla.edu/abstract/?tid=15552&amp;pcode=GLWS4\" class=\"uri\">http://www.ipam.ucla.edu/abstract/?tid=15552&amp;pcode=GLWS4</a>\n",
    "\n",
    "The Deep Bootstrap\n",
    "<a href=\"https://twitter.com/PreetumNakkiran/status/1318007088321335297?s=20\" class=\"uri\">https://twitter.com/PreetumNakkiran/status/1318007088321335297?s=20</a>\n",
    "\n",
    "Aki Vehtari on Leave One Out Uncertainty:\n",
    "<a href=\"https://arxiv.org/abs/2008.10296\" class=\"uri\">https://arxiv.org/abs/2008.10296</a>\n",
    "(check for his references).\n",
    "\n",
    "Large models and memorisation:\n",
    "<a href=\"https://arxiv.org/abs/2012.07805\" class=\"uri\">https://arxiv.org/abs/2012.07805</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double Descent\n",
    "--------------\n",
    "\n",
    "One of Breiman’s ideas for improving predictive performance is known as\n",
    "bagging (Breiman, 1996). The idea is to train a number of models on the\n",
    "data such that they overfit (high variance). Then average the\n",
    "predictions of these models. The models are trained on different\n",
    "bootstrap samples (Efron, 1979) and their predictions are aggregated\n",
    "giving us the acronym, Bagging. By combining decision trees with\n",
    "bagging, we recover random forests (Breiman, 2001).\n",
    "\n",
    "Bias and variance can also be estimated through Efron’s bootstrap\n",
    "(Efron, 1979), and the traditional view has been that there’s a form of\n",
    "Goldilocks effect, where the best predictions are given by the model\n",
    "that is ‘just right’ for the amount of data available. Not to simple,\n",
    "not too complex. The idea is that bias decreases with increasing model\n",
    "complexity and variance increases with increasing model complexity.\n",
    "Typically plots begin with the Mummy bear on the left (too much bias)\n",
    "end with the Daddy bear on the right (too much variance) and show a dip\n",
    "in the middle where the Baby bear (just) right finds themselves.\n",
    "\n",
    "The Daddy bear is typically positioned at the point where the model is\n",
    "able to exactly interpolate the data. For a generalized linear model\n",
    "(McCullagh and Nelder, 1989), this is the point at which the number of\n",
    "parameters is equal to the number of data[1]. But the modern empirical\n",
    "finding is that when we move beyond Daddy bear, into the dark forest of\n",
    "the massively overparameterized model we can achieve good\n",
    "generalization.\n",
    "\n",
    "As Zhang et al. (2017) starkly illustrated with their random labels\n",
    "experiment, within the dark forest there are some terrible places, big\n",
    "bad wolves of overfitting that will gobble up your model. But as\n",
    "empirical evidence shows there is also a safe and hospitable Grandma’s\n",
    "house where these highly overparameterized models are safely consumed.\n",
    "Fundamentally, it must be about the route you take through the forest,\n",
    "and the precautions you take to ensure the wolf doesn’t see where you’re\n",
    "going and beat you to the door.\n",
    "\n",
    "There are two implications of this empirical result. Firstly, that there\n",
    "is a great deal of new theory that needs to be developed. Secondly, that\n",
    "theory is now obliged to conflate two aspects to modelling that we\n",
    "generally like to keep separate: the model and the algorithm.\n",
    "\n",
    "Classical statistical theory around predictive generalization focusses\n",
    "specifically on the class of models that is being used for data fitting.\n",
    "Historically, whether that theory follows a Fisher-aligned estimation\n",
    "approach (see e.g. Vapnik (1998)) or model-based Bayesian approach (see\n",
    "e.g. Ghahramani (2015)), neither is fully equipped to deal with these\n",
    "new circumstances because, to continue our rather tortured analogy,\n",
    "these theories provide us with a characterization of the *destination*\n",
    "of the algorithm, and seek to ensure that we reach that destination.\n",
    "Modern machine learning requires theories of the *journey* and what our\n",
    "route through the forest should be.\n",
    "\n",
    "Crucially, the destination is always associated with 100% accuracy on\n",
    "the training set. An objective that is always achievable for the\n",
    "overparameterized model.\n",
    "\n",
    "Intuitively, it seems that a highly overparameterized model places\n",
    "Grandma’s house on the edge of the dark forest. Making it easily and\n",
    "quickly accessible to the algorithm. The larger the model, the more\n",
    "exposed Grandma’s house becomes. Perhaps this is due to some form of\n",
    "blessing of dimensionality brings Grandma’s house closer to the edge of\n",
    "the forest in a high dimensional setting. Really, we should think of\n",
    "Grandma’s house as a low dimensional manifold of destinations that are\n",
    "safe. A path through the forest where the wolf of overfitting doesn’t\n",
    "venture. In the GLM case, we know already that when the number of\n",
    "parameters matches the number of data there is precisely one location in\n",
    "parameter space where accuracy on the training data is 100%. Our\n",
    "previous misunderstanding of generalization stemmed from the fact that\n",
    "(seemingly) it is highly unlikely that this single point is a good place\n",
    "to be from the perspective of generalization. Additionally, it is often\n",
    "difficult to find. Finding the precise polynomial coefficients in a\n",
    "least squares regression to exactly fit the basis to a small data set\n",
    "such as the Olympic marathon data requires careful consideration of the\n",
    "numerical properties and an orthogonalization of the design matrix\n",
    "(Lawson and Hanson, 1995).\n",
    "\n",
    "It seems that with a highly overparameterized model, these locations\n",
    "become easier to find and they provide good generalization properties.\n",
    "In machine learning this is known as the “double descent phenomenon”\n",
    "(see e.g. Belkin et al. (2019)).\n",
    "\n",
    "[1] Assuming we are ignoring parameters in the link function and the\n",
    "distribution function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks!\n",
    "-------\n",
    "\n",
    "For more information on these subjects and more you might want to check\n",
    "the following resources.\n",
    "\n",
    "-   twitter: [@lawrennd](https://twitter.com/lawrennd)\n",
    "-   podcast: [The Talking Machines](http://thetalkingmachines.com)\n",
    "-   newspaper: [Guardian Profile\n",
    "    Page](http://www.theguardian.com/profile/neil-lawrence)\n",
    "-   blog:\n",
    "    [http://inverseprobability.com](http://inverseprobability.com/blog.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Belkin, M., Hsu, D., Ma, S., Soumik Mandal, 2019. Reconciling modern\n",
    "machine-learning practice and the classical bias-variance trade-off.\n",
    "Proc. Natl. Acad. Sci. USA 116, 15849–15854.\n",
    "\n",
    "Breiman, L., 2001. Random forests. Mach. Learn. 45, 5–32.\n",
    "<https://doi.org/10.1023/A:1010933404324>\n",
    "\n",
    "Breiman, L., 1996. Bagging predictors. Machine Learning 24, 123–140.\n",
    "<https://doi.org/10.1007/BF00058655>\n",
    "\n",
    "Efron, B., 1979. Bootstrap methods: Another look at the jackkife. Annals\n",
    "of Statistics 7, 1–26.\n",
    "\n",
    "Ghahramani, Z., 2015. Probabilistic machine learning and artificial\n",
    "intelligence. Nature 452–459.\n",
    "\n",
    "Lawson, C.L., Hanson, R.J., 1995. Solving least squares problems. SIAM.\n",
    "<https://doi.org/10.1137/1.9781611971217>\n",
    "\n",
    "McCullagh, P., Nelder, J.A., 1989. Generalized linear models, 2nd ed.\n",
    "Chapman; Hall.\n",
    "\n",
    "Vapnik, V.N., 1998. Statistical learning theory. wiley, New York.\n",
    "\n",
    "Zhang, C., Bengio, S., Hardt, M., Recht, B., Vinyals, O., 2017.\n",
    "Understanding deep learning requires rethinking generalization, in:\n",
    "https://openreview.net/forum?id=Sy8gdB9xx (Ed.), International\n",
    "Conference on Learning Representations."
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
