---
title: "Optimization: Stochastic Gradient Descent and ADAM"
venue: "Computer Laboratory, William Gates Building, Room TBA"
abstract: "<p>This lecture will cover the Robins Munroe approach to optimizatin, differing it from classical approaches and highlighting its advantages for big data.</p>"
author:
- given: Neil D.
  family: Lawrence
  url: http://inverseprobability.com
  institute: 
  twitter: 
  gscholar: 
  orchid: 
date: 2021-02-02
published: 2021-02-02
week: 2
reveal: 02-optimization-stochastic-gradient-descent-and-adam-I.slides.html
layout: lecture
categories:
- notes
---


<div style="display:none">
  $${% include talk-notation.tex %}$$
</div>

<script src="/talks/figure-magnify.js"></script>
<script src="/talks/figure-animate.js"></script>
    
<div id="modal-frame" class="modal">
  <span class="close" onclick="closeMagnify()">&times;</span>
  <div class="modal-figure">
    <div class="figure-frame">
      <div class="modal-content" id="modal01"></div>
      <!--<img class="modal-content" id="object01">-->
    </div>
    <div class="caption-frame" id="modal-caption"></div>
  </div>
</div>	  

<p>Related publications and links will appear here.</p>

