---
title: "Generalization and Neural Networks"
venue: "Computer Laboratory, William Gates Building, 15 J. J. Thomson Avenue"
abstract: "<p>This lecture will cover generalization in machine learning with a particular focus on neural architectures. We will review classical generalization and explore what’s different about neural network models.</p>"
author:
- given: Neil D.
  family: Lawrence
  url: http://inverseprobability.com
  institute: 
  twitter: 
  gscholar: 
  orcid: 
date: 2021-01-21
published: 2021-01-21
time: "14:00"
week: 1
session: 2
layout: lecture
categories:
- notes
---



<ul>
<li>Classical models of generalisation</li>
<li>Double descent</li>
<li>Generalisation as a function of the algorithm (route up table mountain)</li>
<li>Conflating algoirthm with model</li>
</ul>
<p>Bias variance dilemma <a href="https://www.mitpressjournals.org/doi/abs/10.1162/neco.1992.4.1.1" class="uri">https://www.mitpressjournals.org/doi/abs/10.1162/neco.1992.4.1.1</a></p>
<p>The lack of a priori distinction between learning algorithms (No free lunch) <a href="https://www.mitpressjournals.org/doi/abs/10.1162/neco.1996.8.7.1341" class="uri">https://www.mitpressjournals.org/doi/abs/10.1162/neco.1996.8.7.1341</a> <a href="https://link.springer.com/chapter/10.1007/978-1-4471-0123-9_3" class="uri">https://link.springer.com/chapter/10.1007/978-1-4471-0123-9_3</a></p>
<p>Belkin on Bias/Variance <a href="https://www.pnas.org/content/116/32/15849.short" class="uri">https://www.pnas.org/content/116/32/15849.short</a> <a href="https://www.pnas.org/content/117/20/10625" class="uri">https://www.pnas.org/content/117/20/10625</a></p>
<p>Belkin Talk: <a href="http://www.ipam.ucla.edu/abstract/?tid=15552&amp;pcode=GLWS4" class="uri">http://www.ipam.ucla.edu/abstract/?tid=15552&amp;pcode=GLWS4</a></p>
<p>the Deep Bootstrap https://twitter.com/PreetumNakkiran/status/1318007088321335297?s=20</p>
<p>David Hogg’s lecture https://speakerdeck.com/dwhgg/linear-regression-with-huge-numbers-of-parameters</p>

