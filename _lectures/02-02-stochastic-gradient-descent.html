---
title: "Optimization and Stochastic Gradient Descent"
venue: "LT1, William Gates Building"
abstract: "<p>This lecture will cover stochastic gradient descent.</p>"
author:
- given: Ferenc
  family: Huszár
  url: https://www.inference.vc/about/
  institute: 
  twitter: 
  gscholar: 
  orcid: 
edit_url: https://github.com/lawrennd/talks/edit/gh-pages/_deepnn/stochastic-gradient-descent.md
date: 2022-02-01
published: 2022-02-01
time: "14:00"
week: 2
session: 2
reveal: 02-02-stochastic-gradient-descent.slides.html
hackmdslides: fhuszar/Hy69Wvrg_
hackmdnotes: fhuszar/rJWAWC7gO
youtube: "GDyD8KwSfvk"
edit_url: https://github.com/lawrennd/talks/edit/gh-pages/_deepnn/stochastic-gradient-descent.md
layout: lecture
categories:
- notes
---



<!-- Do not edit this file locally. -->
<!---->
<!-- Do not edit this file locally. -->
<!-- Do not edit this file locally. -->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
<!--

-->
<p>You can find the <a href="https://hackmd.io/@fhuszar/Hy69Wvrg_">slides here</a> and the <a href="https://hackmd.io/@fhuszar/rJWAWC7gO">notes here</a>.</p>
<h2 id="empirical-risk-minimization-via-gradient-descent">Empirical Risk Minimization via gradient descent</h2>
<p><span class="math display">\[
\mathbf{w}_{t+1} = \mathbf{w}_t - \eta \nabla_\mathbf{w} \hat{L}(\mathbf{w_t}, \mathcal{D})
\]</span></p>
<p>Calculating the gradient: * takes time to cycle through whole dataset * limited memory on GPU * is wasteful: <span class="math inline">\(\hat{L}\)</span> is a sum, CLT applies</p>
<h2 id="stochastic-gradient-descent">Stochastic gradient descent</h2>
<p><span class="math display">\[
\mathbf{w}_{t+1} = \mathbf{w}_t - \eta \nabla_\mathbf{w} \hat{L}(\mathbf{w_t}, \mathcal{D}_t)
\]</span></p>
<p>where <span class="math inline">\(\mathcal{D}_t\)</span> is a random subset (minibatch) of <span class="math inline">\(\mathcal{D}\)</span>.</p>
<p>Also known as minibatch-SGD.</p>
<h2 id="does-it-converge">Does it converge?</h2>
<p>Unbiased gradient estimator:</p>
<p><span class="math display">\[
\mathbb{E}[\hat{L}(\mathbf{w}, \mathcal{D}_t)] = \hat{L}(\mathbf{w}, \mathcal{D})
\]</span></p>
<ul>
<li>empirical risk does not increase in expectation</li>
<li><span class="math inline">\(\hat{L}(\mathbf{w}_t)\)</span> is a supermartingale</li>
<li>Doob’s martingale convergence theorem: a.s. convergence.</li>
</ul>
<h2 id="does-it-behave-the-same-way">Does it behave the same way?</h2>
<p><img src="https://i.imgur.com/xRYHk0m.png%20=1200x" /></p>
<h2 id="analysis-of-mean-iterate">Analysis of mean iterate</h2>
<p><img src="https://i.imgur.com/9j85UIv.png" /></p>
<p>(<a href="https://arxiv.org/abs/2101.12176">Smith et al, 2021</a>) “On the Origin of Implicit Regularization in Stochastic Gradient Descent”</p>
<h2 id="analysis-of-the-mean-iterate">Analysis of the mean iterate</h2>
<p><span class="math display">\[
\mathbf{w}_{t+1} = \mathbf{w}_t - \eta \nabla_\mathbf{w} \hat{L}(\mathbf{w_t}, \mathcal{D}_t)
\]</span></p>
<p>mean iterate in SGD:</p>
<p><span class="math display">\[
\mu_t = \mathbb{E}[\mathbf{w}_t]
\]</span></p>
<h2 id="implicit-regularization-in-sgd">Implicit regularization in SGD</h2>
<p>(<a href="https://arxiv.org/abs/2101.12176">Smith et al, 2021</a>): mean iterate approximated as continuous gradient flow:</p>
<p><span class="math display">\[
\small
\dot{\mu}(t) = -\eta \nabla_\mathbf{w}\tilde{L}_{SGD}(\mu(t), \mathcal{D})
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\small
\tilde{L}_{SGD}(\mathbf{w}, \mathcal{D}) = \tilde{L}_{GD}(\mathbf{w}, \mathcal{D}) + \frac{\eta}{4}\mathbb{E}\|\nabla_\mathbf{w}\hat{L}(\mathbf{w}, \mathcal{D_t}) - \nabla_\mathbf{w}\hat{L}(\mathbf{w}, \mathcal{D})\|^2
\]</span></p>
<h2 id="implicit-regularization-in-sgd-1">Implicit regularization in SGD</h2>
<p>(<a href="https://arxiv.org/abs/2101.12176">Smith et al, 2021</a>): mean iterate approximated as continuous gradient flow:</p>
<p><span class="math display">\[
\small
\dot{\mu}(t) = -\eta \nabla_\mathbf{w}\tilde{L}_{SGD}(\mu(t), \mathcal{D})
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\small
\tilde{L}_{SGD}(\mathbf{w}, \mathcal{D}) = \tilde{L}_{GD}(\mathbf{w}, \mathcal{D}) + \frac{\eta}{4}\underbrace{\mathbb{E}\|\nabla_\mathbf{w}\hat{L}(\mathbf{w}, \mathcal{D_t}) - \nabla_\mathbf{w}\hat{L}(\mathbf{w}, \mathcal{D})\|^2}_{\text{variance of gradients}}
\]</span></p>
<h2 id="revisiting-cartoon-example">Revisiting cartoon example</h2>
<p><img src="https://i.imgur.com/xRYHk0m.png%20=1200x" /></p>
<h2 id="limitations-of-this-analysis">Limitations of this analysis</h2>
<p><img src="https://i.imgur.com/4Zyb3vy.png" /></p>
<h2 id="variants-of-sgd-adam">Variants of SGD: Adam</h2>
<p>Motivation: * momentum * adapting to the average gradient norm</p>
<h2 id="adam-algorithm">Adam algorithm</h2>
<p><img src="https://i.imgur.com/MpiCllk.png" /></p>
<h2 id="is-adam-any-good">Is Adam any good?</h2>
<p><img src="https://i.imgur.com/0yelxmm.png" /></p>
<h2 id="remember-cartoon-example">Remember cartoon example</h2>
<p><img src="https://i.imgur.com/xRYHk0m.png%20=1200x" /></p>
<p><img src="https://i.imgur.com/xBNd5Qk.png" /></p>
<p><img src="https://i.imgur.com/q9tTe7i.png" /></p>
<h2 id="sgd-summary">SGD summary</h2>
<ul>
<li>gradient noise is a feature not bug</li>
<li>SGD avoids regions with high gradient noise</li>
<li>this may help with generalization</li>
<li>improved SGD, like Adam, may not help</li>
<li>an optimization algorithm can be “too good”</li>
</ul>
<!--No Free Lunch for Optimization: <https://ti.arc.nasa.gov/m/profile/dhw/papers/78.pdf> <https://link.springer.com/chapter/10.1007%2F978-3-030-12767-1_5>
Survey of Optimization methods for DeepNNs: <https://arxiv.org/abs/2007.01547>


Related publications and links will appear here.

* SGD (why it works, high variance estimator etc)
* Adam
* RMS PropMixed mode-->

