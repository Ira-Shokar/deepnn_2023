---
title: "Optimization and Stochastic Gradient Descent"
venue: "Zoom"
abstract: "<p>This lecture will cover stochastic gradient descent.</p>"
author:
- given: Ferenc
  family: Husz√°r
  url: https://www.inference.vc/about/
  institute: 
  twitter: 
  gscholar: 
  orcid: 
edit_url: https://github.com/lawrennd/talks/edit/gh-pages/_deepnn/stochastic-gradient-descent.md
date: 2021-02-02
published: 2021-02-02
time: "14:00"
week: 2
session: 2
youtube: "GDyD8KwSfvk"
edit_url: https://github.com/lawrennd/talks/edit/gh-pages/_deepnn/stochastic-gradient-descent.md
layout: lecture
categories:
- notes
---



<p>You can find the <a href="https://hackmd.io/@fhuszar/Hy69Wvrg_">slides here</a> and the <a href="https://hackmd.io/@fhuszar/rJWAWC7gO">notes here</a>.</p>
<!--No Free Lunch for Optimization: <https://ti.arc.nasa.gov/m/profile/dhw/papers/78.pdf> <https://link.springer.com/chapter/10.1007%2F978-3-030-12767-1_5>
Survey of Optimization methods for DeepNNs: <https://arxiv.org/abs/2007.01547>


Related publications and links will appear here.

* SGD (why it works, high variance estimator etc)
* Adam
* RMS PropMixed mode-->

