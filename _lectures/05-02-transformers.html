---
title: "Transformers"
venue: "Zoom"
abstract: "<p>This lecture will introduce transformers.</p>"
author:
- given: Nic
  family: Lane
  url: http://niclane.org/
  institute: 
  twitter: 
  gscholar: 
  orcid: 
date: 2021-02-23
published: 2021-02-23
time: "14:00"
week: 5
session: 2
reveal: 05-02-transformers.slides.html
youtube: "wXgWXDpVrM4"
layout: lecture
categories:
- notes
---



<!-- Do not edit this file locally. -->
<!---->
<!-- Do not edit this file locally. -->
<!-- Do not edit this file locally. -->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
<!--

-->
<h2 id="plan-for-the-day">Plan for the Day</h2>
<ul>
<li><strong>Attention, Attention!</strong></li>
<li>Transformers: Attention is All You Need.</li>
<li>Transformer extensions:
<ul>
<li>Reformers - alternative attention operator</li>
<li>Linformers - linear complexity</li>
<li>Transformer training difficulty<br />
</li>
<li>Switch Transformers</li>
</ul></li>
<li>Going beyond NLP
<ul>
<li>Image is Worth 16x16 Words</li>
<li>AlphaFold 2</li>
</ul></li>
</ul>
<h2 id="think-back-to-last-week-seq2seq">Think back to last week: Seq2Seq</h2>
<ul>
<li><strong>Encoder</strong> compresses inut into a number of hidden notes - the state vector.</li>
<li><strong>Decoder</strong> , initialized with the context vector, generates the output (translation)</li>
</ul>
<center>
<div class="centered" style="">
<img class="" src="../slides/diagrams/transformers/encoder-decoder-example.png" width="1300px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<ul>
<li><strong>Memory, and gradient stability, remain the key limitation even when LSTMs and GRUs are used.</strong></li>
</ul>
<h2 id="neural-machine-translation-by-jointly-learning-to-align-and-translate">Neural Machine Translation by Jointly Learning to Align and Translate</h2>
<center>
<table>
<tr>
<ul>
<div class="centered" style="">
<img class="" src="../slides/diagrams/transformers/encoder-decoder-example.png" width="1200px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</ul>
<ul>
<div class="centered" style="">
<img class="" src="../slides/diagrams/transformers/LSTM.png" width="700px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</ul>
</tr>
</table>
<br />

</center>
<h2 id="learning-alignment---the-first-attention">Learning Alignment - the first Attention</h2>
<div style="text-align:right">
<div class="centered" style="">
<img class="" src="../slides/diagrams/transformers/rnnAttention.png" width="600px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<ul>
<li>Fixed-length context vector was a key performance bottleneck in 2015.</li>
<li>Bahdanau et al. (2015) propose to (soft-)search for parts of a source <br>sentence that are relevant to predicting a target word.</li>
<li>The method, based on the Cho et al. (2014) Encoder–Decoder:
</li>
<ul>
<li><strong>Encoder:</strong> bi-directional RNN.</li>
<li><strong>Decoder:</strong> gated RNN.</li>
<li><strong>Innovation:</strong> each time-step gets its own separate context vector: <span class="math display">\[\large c_t = \sum_{j=1}^{\top} ( \alpha_{t,j} h_t)\]</span> <span class="math display">\[\large \alpha_{t,j} = \frac{\exp(e_{t,j})} {\sum_{k=1}^{\top}\exp(e_{t,k})}\]</span> <span class="math display">\[\large e_{t,j} = f(s_{t-1}, h_j)\]</span></li>
</ul></li>
</ul>
<p><em>Function <span class="math inline">\(f\)</span> implemented as a fully connected layer.</em></p>
<h2 id="attention-attention">Attention, Attention!</h2>
<center>
<div class="centered" style="">
<img class="" src="../slides/diagrams/transformers/learned_attention.png" width="1100px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<h2 id="plan-for-the-day-1">Plan for the Day</h2>
<ul>
<li>Attention, Attention!</li>
<li><strong>Transformers: Attention is All You Need.</strong></li>
<li>Transformer extensions:
<ul>
<li>Reformers - alternative attention operator</li>
<li>Linformers - linear complexity</li>
<li>Transformer training difficulty<br />
</li>
<li>Switch Transformers</li>
</ul></li>
<li>Going beyond NLP
<ul>
<li>Image is Worth 16x16 Words</li>
<li>AlphaFold 2</li>
</ul></li>
</ul>
<h2 id="attention-is-all-you-need">Attention is All You Need</h2>
<center>
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/transformers/attention.jpg" width="1300px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<h2 id="attention-is-all-you-need-1">Attention is All You Need</h2>
<div style="text-align:right">
<div class="centered" style="">
<img class="" src="../slides/diagrams/transformers/scaled.png" width="1200px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<ul>
<li>Scaled dot-product attention (no parameters):</li>
</ul>
<p><span class="math display">\[ \text{attention}(Q,K,V)=\text{softmax}\left(\frac{QK^{\top}}{\sqrt{d_{K}}}\right) V\]</span></p>
<h2 id="attention-is-all-you-need-2">Attention is All You Need</h2>
<center>
<div class="centered" style="">
<img class="" src="../slides/diagrams/transformers/QKV.png" width="1600px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<h2 id="attention-is-all-you-need-3">Attention is All You Need</h2>
<div style="text-align:right">
<div class="centered" style="">
<img class="" src="../slides/diagrams/transformers/multihead.png" width="1100px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<ul>
<li>Scaled dot-product attention (no parameters):</li>
</ul>
<p><span class="math display">\[ \text{attention}(Q,K,V)=\text{softmax}\left(\frac{QK^{\top}}{\sqrt{d_{K}}}\right) V\]</span></p>
<ul>
<li>Multi-head attention (parameters: <span class="math inline">\(W_0, W_1^Q, W_1^K, W_1^V, \dots\)</span>): <span class="math inline">\(\text{MultiHead}(Q,K,V)=\text{concat}(\text{head}_{1},\dots,\text{head}_{h}) W_{0}\)</span> <span class="math inline">\(\text{head}_i=\text{attention}(QW_i^Q, KW_i^K, VW_i^V)\)</span></li>
</ul>
<h2 id="attention-is-all-you-need-the-transformer">Attention is All You Need: the Transformer</h2>
<center>
<table>
<tr>
<td>
<div class="centered" style="">
<img class="" src="../slides/diagrams/transformers/rnnAttention.png" width="575px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td>
<div class="centered" style="">
<img class="" src="../slides/diagrams/transformers/transformer.png" width="675px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</center>
<h2 id="attention-is-all-you-need-4">Attention is All You Need</h2>
<center>
<em>The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German <br> and English-to-French newstest 2014 tests at a fraction of the training cost.</em>
</center>
<center>
<div class="centered" style="">
<img class="" src="../slides/diagrams/transformers/transformer_perf.png" width="1500px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<h2 id="plan-for-the-day-2">Plan for the Day</h2>
<ul>
<li>Attention, Attention!</li>
<li>Transformers: Attention is All You Need.</li>
<li><strong>Transformer extensions:</strong>
<ul>
<li>Reformers - alternative attention operator</li>
<li>Linformers - linear complexity</li>
<li>Transformer training difficulty<br />
</li>
<li>Switch Transformers</li>
</ul></li>
<li>Going beyond NLP
<ul>
<li>Image is Worth 16x16 Words</li>
<li>AlphaFold 2</li>
</ul></li>
</ul>
<h2 id="reformers">Reformers</h2>
<center>
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/transformers/reformers.jpg" width="1300px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<h2 id="reformers-locality-sensitive-hashing">Reformers: Locality Sensitive Hashing</h2>
<center>
<em>Rather than attending all-to-all, split the sequence up. Kitaev et al. (2020) employ a hashing scheme first proposed by Andoni et al. (2015).</em>
</center>
<center>
<div class="centered" style="">
<img class="" src="../slides/diagrams/transformers/reformer_mechanism.png" width="1500px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<h2 id="reformers-performance">Reformers performance</h2>
<center>
<div class="centered" style="">
<img class="" src="../slides/diagrams/transformers/reformer_performance.png" width="1400px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<center>
<em>Loss: log-likelihood and perplexity expressed in bits per dimension.</em>
</center>
<h2 id="linformers">Linformers</h2>
<center>
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/transformers/linformers.jpg" width="1300px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<h2 id="linformers-1">Linformers</h2>
<ul>
<li><p>Basic attention (no parameters):</p>
<p><span class="math display">\[ \text{attention}(Q,K,V)=\underbrace{\text{softmax}\left(\frac{QK^{\top}}{\sqrt{d_{K}}}\right)}_\text{P} V\]</span></p></li>
<li><p>Wang et al. prove theoretically and check empirically that P is low rank.</p></li>
</ul>
<center>
<div class="centered" style="">
<img class="" src="../slides/diagrams/transformers/linformer_eigen.png" width="1600px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<h2 id="linformers-2">Linformers</h2>
<center>
<em>The idea is very simple - add a simple projection between the weighted K, Q and their joint dot product. <br>This fixes to a constant the dimension of the matrices entering the self-attention mechanism.</em>
</center>
<center>
<div class="centered" style="">
<img class="" src="../slides/diagrams/transformers/linformer_arch.png" width="1200px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<h2 id="linformers-performance">Linformers performance</h2>
<center>
<div class="centered" style="">
<img class="" src="../slides/diagrams/transformers/linformer_performance.png" width="1400px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<h2 id="training-transformers">Training Transformers</h2>
<center>
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/transformers/training.jpg" width="1300px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<h2 id="layer-normalization-in-transformers">Layer Normalization in Transformers</h2>
<center>
<div class="centered" style="">
<img class="" src="../slides/diagrams/transformers/layerNorm.png" width="1400px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<h2 id="training-transformers-1">Training Transformers</h2>
<table>
<tr>
<th>
<center>
Unbalanced Gradients
</center>
</th>
<th>
<center>
Amplification-induced Instability
</center>
</th>
</tr>
<tr>
<td>
<div class="centered" style="">
<img class="" src="../slides/diagrams/transformers/training_sec3.png" width="1200px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td>
<div class="centered" style="">
<img class="" src="../slides/diagrams/transformers/training_sec4.png" width="1050px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
<h2 id="switch-transformers">Switch Transformers</h2>
<center>
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/transformers/switch.jpg" width="1300px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<h2 id="switch-transformers-1">Switch Transformers</h2>
<center>
<div class="centered" style="">
<img class="" src="../slides/diagrams/transformers/switch.png" width="1200px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<h2 id="switch-transformers---speed-up">Switch Transformers - speed-up</h2>
<center>
<div class="centered" style="">
<img class="" src="../slides/diagrams/transformers/switch_speed.png" width="1200px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<h2 id="switch-transformers---multi-task-performance">Switch Transformers - multi-task performance</h2>
<center>
<div class="centered" style="">
<img class="" src="../slides/diagrams/transformers/switch_multitask.png" width="1900px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<h2 id="plan-for-the-day-3">Plan for the Day</h2>
<ul>
<li>Attention, Attention!</li>
<li>Transformers: Attention is All You Need.</li>
<li>Transformer extensions:
<ul>
<li>Reformers - alternative attention operator</li>
<li>Linformers - linear complexity</li>
<li>Transformer training difficulty<br />
</li>
<li>Switch Transformers</li>
</ul></li>
<li><strong>Going beyond NLP</strong>
<ul>
<li>Image is Worth 16x16 Words</li>
<li>AlphaFold 2</li>
</ul></li>
</ul>
<h2 id="image-is-worth-16x16-words-embedding-and-architecture">Image is Worth 16x16 Words Embedding and Architecture</h2>
<center>
<div class="centered" style="">
<img class="" src="../slides/diagrams/transformers/image16wordsModel.png" width="1400px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<h2 id="image-is-worth-16x16-words">Image is Worth 16x16 Words</h2>
<center>
<div class="centered" style="">
<img class="" src="../slides/diagrams/transformers/image16wordsViz.png" width="2000px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<h2 id="image-is-worth-16x16-words-1">Image is Worth 16x16 Words</h2>
<center>
<div class="centered" style="">
<img class="" src="../slides/diagrams/transformers/image16wordsRes.png" width="1100px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<ul>
<center>
<div class="centered" style="">
<img class="" src="../slides/diagrams/transformers/image16wordsRes2.png" width="800px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</ul>
<ul>
<center>
<div class="centered" style="">
<img class="" src="../slides/diagrams/transformers/image16wordsRes3.png" width="800px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</ul>
<h2 id="alphafold---the-protein-folding-problem">AlphaFold - the protein folding problem</h2>
<center>
<div class="centered" style="">
<img class="" src="../slides/diagrams/transformers/protein.gif" width="1600px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<h2 id="convolutional-solution-alphafold-1">Convolutional solution: AlphaFold 1</h2>
<table>
<tr>
<td>
<center>
<div class="centered" style="">
<img class="" src="../slides/diagrams/transformers/AlphaFold1a.png" width="1100px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</td>
<td>
<center>
<div class="centered" style="">
<img class="" src="../slides/diagrams/transformers/AlphaFold1.png" width="1000px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</td>
</tr>
</table>
<h2 id="transformer-based-solution-alphafold-2">Transformer-based solution: AlphaFold 2</h2>
<table>
<tr>
<td>
<center>
<div class="centered" style="">
<img class="" src="../slides/diagrams/transformers/AlphaFold.png" width="1800px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</td>
<td>
<center>
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/transformers/AlphaFold_performance.jpg" width="1200px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</td>
</tr>
</table>
<h2 id="summary-of-the-day">Summary of the Day</h2>
<ul>
<li>Attention, Attention!</li>
<li>Transformers: Attention is All You Need.</li>
<li>Transformer extensions:
<ul>
<li>Reformers - alternative attention operator</li>
<li>Linformers - linear complexity</li>
<li>Transformer training difficulty<br />
</li>
<li>Switch Transformers</li>
</ul></li>
<li>Going beyond NLP
<ul>
<li>Image is Worth 16x16 Words</li>
<li>AlphaFold 2</li>
</ul></li>
</ul>

