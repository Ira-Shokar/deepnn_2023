---
title: "Representation and Transfer Learning"
abstract: "<p>In this lecture Ferenc will introduce us to the notions behind representation and transfer learning.</p>"
author:
- given: Ferenc
  family: Huszár
  url: https://www.inference.vc/about/
  institute: 
  twitter: 
  gscholar: 
  orcid: 
edit_url: https://github.com/lawrennd/talks/edit/gh-pages/_deepnn/representation.md
date: 2022-03-01
published: 2022-03-01
time: "14:00"
week: 6
session: 2
reveal: 06-02-representation.slides.html
hackmdslides: fhuszar/r1HxvooMd#/
youtube: "OTcrDGQQcZY"
edit_url: https://github.com/lawrennd/talks/edit/gh-pages/_deepnn/representation.md
layout: lecture
categories:
- notes
---



<!-- Do not edit this file locally. -->
<!---->
<!-- Do not edit this file locally. -->
<!-- Do not edit this file locally. -->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
<!--

-->
<h2 id="unsupervised-learning">Unsupervised learning</h2>
<ul>
<li>observations <span class="math inline">\(x_1, x_2, \ldots\)</span></li>
<li>drawn i.i.d. from some <span class="math inline">\(p_\mathcal{D}\)</span></li>
<li>can we learn something from this?</li>
</ul>
<h2 id="unsupervised-learning-goals">Unsupervised learning goals</h2>
<ul>
<li>can we learn something from this?
<ul>
<li>a model of data distribution <span class="math inline">\(p_\theta(x) \approx p_{\mathcal{D}}(x)\)</span>
<ul>
<li>compression</li>
<li>data reconstruction</li>
<li>sampling/generation</li>
</ul></li>
<li>a representation <span class="math inline">\(z=g_\theta(x)\)</span> or <span class="math inline">\(q_{\theta}(z\vert x)\)</span>
<ul>
<li>downstream classification task</li>
<li>data visualisation</li>
</ul></li>
</ul></li>
</ul>
<h2 id="ul-as-distribution-modeling">UL as distribution modeling</h2>
<ul>
<li>defines goal as modeling <span class="math inline">\(p_\theta(x)\approx p_\mathcal{D}(x)\)</span></li>
<li><span class="math inline">\(\theta\)</span>: parameters</li>
<li>maximum likelihood estimation: <span class="math display">\[
  \theta^{ML} = \operatorname{argmax}_\theta \sum_{x_i \in \mathcal{D}} \log p_\theta(x_i)
\]</span></li>
</ul>
<h2 id="deep-learning-for-modelling-distributions">Deep learning for modelling distributions</h2>
<ul>
<li>auto-regressive models (e.g. RNNs)
<ul>
<li><span class="math inline">\(p_{\theta}(x_{1:T}) = \prod_{t=1}^T p_\theta(x_t\vert x_{1:t-1})\)</span></li>
</ul></li>
<li>implicit distributions (e.g. GANs)
<ul>
<li>x = <span class="math inline">\(g_\theta(z), z\sim \mathcal{N}(0, I)\)</span></li>
</ul></li>
<li>flow models (e.g. RealNVP)
<ul>
<li>like above but <span class="math inline">\(g_\theta(z)\)</span> invertible</li>
</ul></li>
<li>latent variable models (LVMs, e.g. VAE)
<ul>
<li><span class="math inline">\(p_\theta(x) = \int p_\theta(x, z) dz\)</span></li>
</ul></li>
</ul>
<h2 id="latent-variable-models">Latent variable models</h2>
<p><span class="math display">\[
p_\theta(x) = \int p_\theta(x, z) dz
\]</span></p>
<h2 id="latent-variable-models-1">Latent variable models</h2>
<p><span class="math display">\[
p_\theta(x) = \int p_\theta(x\vert z) p_\theta(z) dz
\]</span></p>
<h2 id="motivation-1">Motivation 1</h2>
<p><strong>“it makes sense”</strong></p>
<ul>
<li>describes data in terms of a generative process</li>
<li>e.g. object properties, locations</li>
<li>learnt <span class="math inline">\(z\)</span> often interpretable</li>
<li>causal reasoning often needs latent variables</li>
</ul>
<h2 id="motivation-2">Motivation 2</h2>
<p><strong>manifold assumption</strong></p>
<ul>
<li>high-dimensional data</li>
<li>doesn’t occupy all the space</li>
<li>concentrated along low-dimensional manifold</li>
<li><span class="math inline">\(z \approx\)</span> intrinsic coordinates within the manifold</li>
</ul>
<h2 id="motivation-3">Motivation 3</h2>
<p><strong>from simple to complicated</strong></p>
<p><span class="math display">\[
p_\theta(x) = \int p_\theta(x\vert z) p_\theta(z) dz
\]</span></p>
<h2 id="motivation-3-1">Motivation 3</h2>
<p><strong>from simple to complicated</strong></p>
<p><span class="math display">\[
\underbrace{p_\theta(x)}_\text{complicated} = \int \underbrace{p_\theta(x\vert z) }_\text{simple}\underbrace{p_\theta(z)}_\text{simple} dz
\]</span></p>
<h2 id="motivation-3-2">Motivation 3</h2>
<p><strong>from simple to complicated</strong></p>
<p><span class="math display">\[
\underbrace{p_\theta(x)}_\text{complicated} = \int \underbrace{\mathcal{N}\left(x; \mu_\theta(z), \operatorname{diag}(\sigma_\theta(z)) \right)}_\text{simple}\underbrace{\mathcal{N}(z; 0, I)}_\text{simple} dz
\]</span></p>
<h2 id="motivation-4">Motivation 4</h2>
<p><strong>variational learning</strong></p>
<ul>
<li>evaluating <span class="math inline">\(p_\theta(x)\)</span> is hard
<ul>
<li>learning is hard</li>
</ul></li>
<li>evaluating <span class="math inline">\(p_\theta(z\vert x)\)</span> is hard
<ul>
<li>inference is hard</li>
</ul></li>
<li>variational framework:
<ul>
<li>approximate learning</li>
<li>approximate inference</li>
</ul></li>
</ul>
<center>
<div class="centered" style="">
<img class="" src="https://i.imgur.com/rmgzOHJ.png" width="600px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<p><a href="https://arxiv.org/abs/1906.02691">(Kingma and Welling, 2019)</a> Variational Autoencoder</p>
<h2 id="variational-autoencoder">Variational autoencoder</h2>
<ul>
<li>Decoder: <span class="math inline">\(p_\theta(x\vert z) = \mathcal{N}(\mu_\theta(z), \sigma_n I)\)</span></li>
<li>Encoder: <span class="math inline">\(q_\psi(z\vert x) = \mathcal{N}(\mu_\psi(z), \sigma_\psi(z))\)</span></li>
<li>Prior: <span class="math inline">\(p_\theta(z)=\mathcal{N}(0, I)\)</span></li>
</ul>
<h2 id="variational-encoder-interpretable-z">Variational encoder: interpretable <span class="math inline">\(z\)</span></h2>
<center>
<div class="centered" style="">
<img class="" src="https://i.imgur.com/kDgc74S.png" width="" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<h1 id="self-supervised-learning">Self-supervised learning</h1>
<h2 id="basic-idea">basic idea</h2>
<ul>
<li>turn unsupervised problem into supervised one</li>
<li>turn datapoints <span class="math inline">\(x_i\)</span> into input-output pairs</li>
<li>called auxiliary or pretext task</li>
<li>learn to solve auxiliary task</li>
<li>transfer representation leaned to the downstream task</li>
</ul>
<h2 id="example-jigsaw-puzzles">example: jigsaw puzzles</h2>
<center>
<div class="centered centered" style="">
<img class="" src="https://i.imgur.com/VtCWtrq.jpg" width="" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<p><a href="https://arxiv.org/abs/1603.09246">(Noroozi and Favaro, 2016)</a></p>
<h2 id="data-efficiency-in-downstream-task">Data-efficiency in downstream task</h2>
<center>
<div class="centered" style="">
<img class="" src="https://i.imgur.com/bX3BzNx.png" width="570px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<p><a href="https://proceedings.icml.cc/static/paper_files/icml/2020/3694-Paper.pdf">(Hènaff et al, 2020)</a></p>
<h2 id="linearity-in-downstream-task">Linearity in downstream task</h2>
<center>
<div class="centered" style="">
<img class="" src="https://i.imgur.com/4YiDM38.png" width="570px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<p><a href="https://arxiv.org/abs/2002.05709">(Chen et al, 2020)</a></p>
<h2 id="several-self-supervised-methods">Several self-supervised methods</h2>
<ul>
<li>auto-encoding</li>
<li>denoising auto-encoding</li>
<li>pseudo-likelihood</li>
<li>instance classification</li>
<li>contrastive learning</li>
<li>masked language models</li>
</ul>
<h2 id="example-instance-classification">Example: instance classification</h2>
<ul>
<li>pick random data index <span class="math inline">\(i\)</span></li>
<li>randomly transform image <span class="math inline">\(x_i\)</span>: <span class="math inline">\(T(x_i)\)</span></li>
<li>auxilliary task: guess data index <span class="math inline">\(i\)</span> from transformed input <span class="math inline">\(T(x_i)\)</span></li>
<li>difficulty: N-way classification</li>
</ul>
<h2 id="example-contrastive-learning">Example: contrastive learning</h2>
<ul>
<li>pick random <span class="math inline">\(y\)</span></li>
<li>if <span class="math inline">\(y=1\)</span> pick two random images <span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span></li>
<li>if <span class="math inline">\(y=0\)</span> use same image twice <span class="math inline">\(x_1=x_2\)</span></li>
<li>aux task: predict <span class="math inline">\(y\)</span> from <span class="math inline">\(f_\theta(T_1(x_1)), f_\theta(T_2(x_2))\)</span></li>
</ul>
<h2 id="example-masked-language-models">Example: Masked Language Models</h2>
<center>
<div class="centered" style="">
<img class="" src="https://i.imgur.com/HeRgOXp.png" width="" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<p><small>image credit: (<a href="https://arxiv.org/pdf/1901.07291.pdf">Lample and Conneau, 2019</a>)</small></p>
<h2 id="bert">BERT</h2>
<center>
<div class="centered centered" style="">
<img class="" src="https://i.imgur.com/zeA1Mix.jpg" width="" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<h1 id="why-should-any-of-this-work">Why should any of this work?</h1>
<p><strong>Predicting What you Already Know Helps: Provable Self-Supervised Learning</strong></p>
<p><a href="https://arxiv.org/abs/2008.01064">(Lee et al, 2020)</a></p>
<h2 id="provable-self-supervised-learning">Provable Self-Supervised Learning</h2>
<p>Assumptions:</p>
<ul>
<li>observable <span class="math inline">\(X\)</span> decomposes into <span class="math inline">\(X_1, X_2\)</span></li>
<li>pretext: only given <span class="math inline">\((X_1, X_2)\)</span> pairs</li>
<li>downstream: we will want to predict <span class="math inline">\(Y\)</span></li>
<li><span class="math inline">\(X_1 \perp \!\!\! \perp X_2 \vert Y, Z\)</span></li>
<li>(+1 additional strong assumption)</li>
</ul>
<h2 id="provable-self-supervised-learning-1">Provable Self-Supervised Learning</h2>
<center>
<div class="centered" style="">
<img class="" src="https://i.imgur.com/8SomFq9.png" width="" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<p><span class="math inline">\(X_1 \perp \!\!\! \perp X_2 \vert Y, Z\)</span></p>
<h2 id="provable-self-supervised-learning-2">Provable Self-Supervised Learning</h2>
<center>
<div class="centered" style="">
<img class="" src="https://i.imgur.com/K754eB2.png" width="" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<h2 id="provable-self-supervised-learning-3">Provable Self-Supervised Learning</h2>
<center>
<div class="centered" style="">
<img class="" src="https://i.imgur.com/SuCKgJq.png" width="" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<p><span class="math display">\[
X_1 \perp \!\!\! \perp  X_2 \vert Y, Z
\]</span></p>
<h2 id="provable-self-supervised-learning-4">Provable Self-Supervised Learning</h2>
<center>
<div class="centered" style="">
<img class="" src="https://i.imgur.com/SuCKgJq.png" width="" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<p><span class="math display">\[
👀 \perp \!\!\! \perp  👄 \vert \text{age}, \text{gender}, \text{ethnicity}
\]</span></p>
<h2 id="provable-self-supervised-learning-5">Provable Self-Supervised Learning</h2>
<p>If <span class="math inline">\(X_1 \perp \!\!\! \perp X_2 \vert Y\)</span>, then</p>
<p><span class="math display">\[
\mathbb{E}[X_2 \vert X_1] = \sum_k \mathbb{E}[X_2\vert Y=k] \mathbb{P}[Y=k\vert X_1 = x_1]
\]</span></p>
<h2 id="provable-self-supervised-learning-6">Provable Self-Supervised Learning</h2>
<p><span class="math display">\[\begin{align}
&amp;\mathbb{E}[X_2 \vert X_1=x_1] = \\
&amp;\left[\begin{matrix} \mathbb{E}[X_2\vert Y=1], \ldots, \mathbb{E}[X_2\vert Y=k]\end{matrix}\right] \left[\begin{matrix} \mathbb{P}[Y=1\vert X_1=x_1]\\ \vdots \\ \mathbb{P}[Y=k\vert X_1=x_1]\end{matrix}\right]
\end{align}\]</span></p>
<h2 id="provable-self-supervised-learning-7">Provable Self-Supervised Learning</h2>
<p><span class="math display">\[\begin{align}
&amp;\mathbb{E}[X_2 \vert X_1=x_1] = \\
&amp;\underbrace{\left[\begin{matrix} \mathbb{E}[X_2\vert Y=1], \ldots, \mathbb{E}[X_2\vert Y=k]\end{matrix}\right]}_\mathbf{A}\left[\begin{matrix} \mathbb{P}[Y=1\vert X_1=x_1]\\ \vdots \\ \mathbb{P}[Y=k\vert X_1=x_1]\end{matrix}\right]
\end{align}\]</span></p>
<h2 id="provable-self-supervised-learning-8">Provable Self-Supervised Learning</h2>
<p><span class="math display">\[
\mathbb{E}[X_2 \vert X_1=x_1] = \mathbf{A}\left[\begin{matrix} \mathbb{P}[Y=1\vert X_1=x_1]\\ \vdots \\ \mathbb{P}[Y=k\vert X_1=x_1]\end{matrix}\right]
\]</span></p>
<h2 id="provable-self-supervised-learning-9">Provable Self-Supervised Learning</h2>
<p><span class="math display">\[
\mathbf{A}^\dagger \mathbb{E}[X_2 \vert X_1=x_1] = \left[\begin{matrix} \mathbb{P}[Y=1\vert X_1=x_1]\\ \vdots \\ \mathbb{P}[Y=k\vert X_1=x_1]\end{matrix}\right]
\]</span></p>
<h2 id="provable-self-supervised-learning-10">Provable Self-Supervised Learning</h2>
<p><span class="math display">\[
\mathbf{A}^\dagger \underbrace{\mathbb{E}[X_2 \vert X_1=x_1]}_\text{pretext task} = \underbrace{\left[\begin{matrix} \mathbb{P}[Y=1\vert X_1=x_1]\\ \vdots \\ \mathbb{P}[Y=k\vert X_1=x_1]\end{matrix}\right]}_\text{downstream task}
\]</span></p>
<h2 id="provable-self-supervised-learning-summary">Provable self-supervised learning summary</h2>
<ul>
<li>under assumptions of conditional independence</li>
<li>(and that matrix <span class="math inline">\(\mathbf{A}\)</span> is full rank)</li>
<li><span class="math inline">\(\mathbb{P}[Y|x_1]\)</span> is in linear span of <span class="math inline">\(\mathbb{E}[X_2\vert x_1]\)</span></li>
<li>All we need is linear model on top of <span class="math inline">\(\mathbb{E}[X_2\vert x_1]\)</span></li>
<li>note: <span class="math inline">\(\mathbb{P}[Y|x_1, x_2]\)</span> would be really optimal</li>
</ul>
<h1 id="recap">Recap</h1>
<h2 id="variational-learning">Variational learning</h2>
<p><span class="math display">\[
\theta^\text{ML} = \operatorname{argmax}_\theta \sum_{x_i \in \mathcal{D}} \log p_\theta(x_i)
\]</span></p>
<h2 id="variational-learning-1">Variational learning</h2>
<p><span class="math display">\[
\mathcal{L}(\theta, \psi) = \sum_{x_i \in \mathcal{D}} \log  p_\theta(x_i) - \operatorname{KL}[q_\psi(z\vert x_i) \| p_\theta(z\vert x_i)]
\]</span></p>
<h2 id="variational-learning-2">Variational learning</h2>
<p><span class="math display">\[
\mathcal{L}(\theta, \psi) = \sum_{x_i \in \mathcal{D}} \log p_\theta(x_i) + \mathbb{E}_{z\sim q_\psi} \log \frac{p_\theta(z\vert x_i)}{q_\psi(z\vert x_i)}
\]</span></p>
<h2 id="variational-learning-3">Variational learning</h2>
<p><span class="math display">\[
\mathcal{L}(\theta, \psi) = \sum_{x_i \in \mathcal{D}} \mathbb{E}_{z\sim q_\psi} \log \frac{p_\theta(z\vert x_i) p_\theta(x_i)}{q_\psi(z\vert x_i)}
\]</span></p>
<h2 id="variational-learning-4">Variational learning</h2>
<p><span class="math display">\[
\mathcal{L}(\theta, \psi) = \sum_{x_i \in \mathcal{D}} \mathbb{E}_{z\sim q_\psi} \log \frac{p_\theta(z,  x_i)}{q_\psi(z\vert x_i)}
\]</span></p>
<h2 id="variational-learning-5">Variational learning</h2>
<p><span class="math display">\[
\mathcal{L}(\theta, \psi) = \sum_{x_i \in \mathcal{D}} \mathbb{E}_{z\sim q_\psi(z\vert x_i)} \log p(x_i\vert z) - \operatorname{KL}[q_\psi(z\vert x_i)\vert p_\theta(z)]
\]</span></p>
<h2 id="variational-learning-6">Variational learning</h2>
<p><span class="math display">\[
\mathcal{L}(\theta, \psi) = \sum_{x_i \in \mathcal{D}} \underbrace{\mathbb{E}_{z\sim q_\psi(z\vert x_i)} \log p(x_i\vert z)}_\text{reconstruction} - \operatorname{KL}[q_\psi(z\vert x_i)\vert p_\theta(z)]
\]</span></p>
<h2 id="discussion-of-max-likelihood">Discussion of max likelihood</h2>
<ul>
<li>trained so that <span class="math inline">\(p_\theta(x)\)</span> matches data</li>
<li>evaluated by how useful <span class="math inline">\(p_\theta(z\vert x)\)</span> is</li>
<li>there is a mismatch</li>
</ul>
<h3 id="representation-learning-vs-max-likelihood">Representation learning vs max likelihood</h3>
<center>
<div class="centered" style="">
<img class="" src="https://i.imgur.com/SPx9AoA.png" width="" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<h3 id="representation-learning-vs-max-likelihood-1">Representation learning vs max likelihood</h3>
<center>
<div class="centered" style="">
<img class="" src="https://i.imgur.com/EqHhQVh.png" width="" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<h3 id="representation-learning-vs-max-likelihood-2">Representation learning vs max likelihood</h3>
<center>
<div class="centered" style="">
<img class="" src="https://i.imgur.com/L0n5kSI.png" width="" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<h3 id="representation-learning-vs-max-likelihood-3">Representation learning vs max likelihood</h3>
<center>
<div class="centered" style="">
<img class="" src="https://i.imgur.com/wuAdSbB.png" width="" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<h3 id="representation-learning-vs-max-likelihood-4">Representation learning vs max likelihood</h3>
<center>
<div class="centered" style="">
<img class="" src="https://i.imgur.com/DwGlp8k.png" width="" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<h3 id="representation-learning-vs-max-likelihood-5">Representation learning vs max likelihood</h3>
<center>
<div class="centered" style="">
<img class="" src="https://i.imgur.com/yuoEcbt.png" width="" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<h2 id="discussion-of-max-likelihood-1">Discussion of max likelihood</h2>
<ul>
<li>max likelihood may not produce good representations</li>
<li>Why do variational methods find good representations?</li>
<li>Are there alternative principles?</li>
</ul>

