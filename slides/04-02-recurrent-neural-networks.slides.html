<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="dcterms.date" content="2022-02-15">
  <title>Recurrent Neural Networks</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@3.9.2/css/reveal.css">
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@3.9.2/css/theme/black.css" id="theme">
  <link rel="stylesheet" href="https://inverseprobability.com/assets/css/talks.css"/>
  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'https://unpkg.com/reveal.js@3.9.2/css/print/pdf.css' : 'https://unpkg.com/reveal.js@3.9.2/css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!--[if lt IE 9]>
  <script src="https://unpkg.com/reveal.js@3.9.2/lib/js/html5shiv.js"></script>
  <![endif]-->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_SVG" type="text/javascript"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    TeX: {
         extensions: ["color.js"]
      }
    });
  </script>
  <script src="../assets/js/figure-animate.js"></script>
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Recurrent Neural Networks</h1>
  <p class="author" style="text-align:center"><a href="https://www.inference.vc/about/">Ferenc Huszár</a></p>
  <p class="date" style="text-align:center"><time>2022-02-15</time></p>
</section>

<section class="slide level2">

<!-- Do not edit this file locally. -->
<!---->
<!-- Do not edit this file locally. -->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
</section>
<section id="different-from-what-weve-seen-before" class="slide level2">
<h2>Different from what we’ve seen before:</h2>
<ul>
<li>different input type (sequences)</li>
<li>different network building blocks
<ul>
<li>multiplicative interactions</li>
<li>gating</li>
<li>skip connections</li>
</ul></li>
<li>different objective
<ul>
<li>maximum likelihood</li>
<li>generative modelling</li>
</ul></li>
</ul>
</section>
<section id="modelling-sequences" class="slide level2">
<h2>Modelling sequences</h2>
<ul>
<li>input to the network: <span class="math inline">\(x_1, x_2, \ldots, x_T\)</span></li>
<li>sequences of different length</li>
<li>sometimes ‘EOS’ symbol</li>
<li>sequence classification (e.g. text classification)</li>
<li>sequence generation (e.g. language generation)</li>
<li>sequence-to-sequence (e.g. translation)</li>
</ul>
</section>
<section id="recurrent-neural-network" class="slide level2">
<h2>Recurrent Neural Network</h2>
<p><img data-src="https://i.imgur.com/UJYrL7I.png" /></p>
</section>
<section id="rnn-unrolled-through-time" class="slide level2">
<h2>RNN: Unrolled through time</h2>
<p><img data-src="https://i.imgur.com/YnkgS5P.png" /></p>
</section>
<section id="rnn-different-uses" class="slide level2">
<h2>RNN: different uses</h2>
<p><img data-src="https://i.imgur.com/WGl90lv.jpg" /></p>
<p>figure from <a href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/">Andrej Karpathy’s blog post</a></p>
</section>
<section id="generating-sequences" class="slide level2">
<h2>Generating sequences</h2>
<p>Goal: model the distribution of sequences</p>
<p><span class="math display">\[
p(x_{1:T}) = p(x_1, \ldots, x_T)
\]</span></p>
<p>Idea: model it one-step-at-a-time:</p>
<p><span class="math display">\[
p(x_{1:T}) = p(x_T\vert x_{1:T-1}) p(x_{T-1} \vert x_{1:T-2}) \cdots p(x_1)
\]</span></p>
</section>
<section id="modeling-sequence-distributions" class="slide level2">
<h2>Modeling sequence distributions</h2>
<p><img data-src="https://i.imgur.com/WfPwnjZ.png" /></p>
</section>
<section id="training-maximum-likelihood" class="slide level2">
<h2>Training: maximum likelihood</h2>
<p><img data-src="https://i.imgur.com/Z8sLsQI.png" /></p>
</section>
<section id="sampling-sequences" class="slide level2">
<h2>Sampling sequences</h2>
<p><img data-src="https://i.imgur.com/c9WcaD0.png" /></p>
</section>
<section id="char-rnn-shakespeare" class="slide level2">
<h2>Char-RNN: Shakespeare</h2>
<p>from <a href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/">Andrej Karpathy’s 2015 blog post</a></p>
<p><img data-src="https://i.imgur.com/cN25jUL.png" /></p>
</section>
<section id="char-rnn-wikipedia" class="slide level2">
<h2>Char-RNN: Wikipedia</h2>
<p>from <a href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/">Andrej Karpathy’s 2015 blog post</a></p>
<p><img data-src="https://i.imgur.com/Nr0UjtR.png" /></p>
</section>
<section id="char-rnn-wikipedia-1" class="slide level2">
<h2>Char-RNN: Wikipedia</h2>
<p>from <a href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/">Andrej Karpathy’s 2015 blog post</a></p>
<p><img data-src="https://i.imgur.com/R91pDeJ.png" /></p>
</section>
<section id="char-rnn-example-random-xml" class="slide level2">
<h2>Char-RNN example: random XML</h2>
<p>from <a href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/">Andrej Karpathy’s 2015 blog post</a></p>
<p><img data-src="https://i.imgur.com/H3b3QjC.png" /></p>
</section>
<section id="char-rnn-example-latex" class="slide level2">
<h2>Char-RNN example: LaTeX</h2>
<p>from <a href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/">Andrej Karpathy’s 2015 blog post</a></p>
<p><img data-src="https://i.imgur.com/GgXRG4n.jpg" /></p>
</section>
<section id="but-it-was-not-that-easy" class="slide level2">
<h2>But, it was not that easy</h2>
<ul>
<li>vanilla RNNs forget too quickly</li>
<li>vanishing gradients problem</li>
<li>exploding gradients problem</li>
</ul>
</section>
<section id="vanishingexploding-gradients-problem" class="slide level2">
<h2>Vanishing/exploding gradients problem</h2>
<p>Vanilla RNN:</p>
<p><span class="math display">\[
\mathbf{h}_{t+1} = \sigma(W_h \mathbf{h}_t + W_x \mathbf{x}_t + \mathbf{b_h})
\]</span></p>
<p><span class="math display">\[
\hat{y} = \phi(W_y \mathbf{h}_{T} + \mathbf{b}_y)
\]</span></p>
</section>
<section id="the-gradients-of-the-loss-are" class="slide level2">
<h2>The gradients of the loss are</h2>
<p><span class="math display">\[\begin{align}
\frac{\partial \hat{L}}{\partial \mathbf{h}_t} &amp;= \frac{\partial \hat{L}}{\partial \mathbf{h}_T} \prod_{s=t}^{T-1} \frac{\partial h_{s+1}}{\partial h_s} \\
&amp;= \frac{\partial \hat{L}}{\mathbf{h}_T} \left( \prod_{s=t}^{T-1} D_s \right) W^{T-t}_h,
\end{align}\]</span></p>
<p>where * <span class="math inline">\(D_t = \operatorname{diag} \left[\sigma&#39;(W_t \mathbf{h}_{t-1} + + W_x \mathbf{x}_t + \mathbf{b_h})\right]\)</span> * if <span class="math inline">\(\sigma\)</span> is ReLU, <span class="math inline">\(\sigma&#39;(z) \in \{0, 1\}\)</span></p>
</section>
<section id="the-norm-of-the-gradient-is-upper-bounded" class="slide level2">
<h2>The norm of the gradient is upper bounded</h2>
<p><span class="math display">\[\begin{align}
\left\|\frac{\partial \hat{L}}{\partial \mathbf{h}_t}\right\| &amp;\leq \left\|\frac{\partial \hat{L}}{\mathbf{h}_T}\right\| \left\|W_h\right\|^{T-t} \prod_{s=t}^{T-1} \left\|D_s\right\|,
\end{align}\]</span></p>
<ul>
<li>the norm of <span class="math inline">\(D_s\)</span> is less than 1 (ReLU)</li>
<li>the norm of <span class="math inline">\(W_h\)</span> can cause gradients to explode</li>
</ul>
</section>
<section id="section" class="slide level2">
<h2></h2>
<p><img data-src="https://i.imgur.com/DVFyskJ.png" /></p>
</section>
<section id="more-typical-solution-gating" class="slide level2">
<h2>More typical solution: gating</h2>
<p>Vanilla RNN:</p>
<p><span class="math display">\[
\mathbf{h}_{t+1} = \sigma(W_h \mathbf{h}_t + W_x \mathbf{x}_t + \mathbf{b_h})
\]</span></p>
<p>Gated Recurrent Unit:</p>
<p><span class="math display">\[\begin{align}
\mathbf{h}_{t+1} &amp;= \mathbf{z}_t \odot \mathbf{h}_t + (1 - \mathbf{z}_t) \tilde{\mathbf{h}}_t \\
\tilde{\mathbf{h}}_t &amp;= \phi\left(W\mathbf{x}_t + U(\mathbf{r}_t \odot \mathbf{h}_t)\right)\\
\mathbf{r}_t &amp;= \sigma(W_r\mathbf{x}_t + U_r\mathbf{h}_t)\\
\mathbf{z}_t &amp;= \sigma(W_z\mathbf{x}_t + U_z\mathbf{h}_t)\\
\end{align}\]</span></p>
</section>
<section id="gru-diagram" class="slide level2">
<h2>GRU diagram</h2>
<p><img data-src="https://i.imgur.com/TrhwIcC.png" /></p>
</section>
<section id="lstm-long-short-term-memory" class="slide level2">
<h2>LSTM: Long Short-Term Memory</h2>
<ul>
<li>by Hochreiter and Schmidhuber (1997)</li>
<li>improved/tweaked several times since</li>
<li>more gates to control behaviour</li>
<li>2009: Alex Graves, ICDAR connected handwriting recognition competition</li>
<li>2013: sets new record in natural speech dataset</li>
<li>2014: GRU proposed (simplified LSTM)</li>
<li>2016: neural machine translation</li>
</ul>
</section>
<section id="rnns-for-images" class="slide level2">
<h2>RNNs for images</h2>
<p><img data-src="https://karpathy.github.io/assets/rnn/house_read.gif" /></p>
<p>(<a href="https://arxiv.org/abs/1412.7755">Ba et al, 2014</a>)</p>
</section>
<section id="rnns-for-images-1" class="slide level2">
<h2>RNNs for images</h2>
<p><img data-src="https://karpathy.github.io/assets/rnn/house_generate.gif" /> (<a href="https://arxiv.org/abs/1502.04623">Gregor et al, 2015</a>)</p>
</section>
<section id="rnns-for-painting" class="slide level2">
<h2>RNNs for painting</h2>
<p><img data-src="https://i.imgur.com/DhbBAl2.png" /></p>
<p>(<a href="https://learning-to-paint.github.io/">Mellor et al, 2019</a>)</p>
</section>
<section id="rnns-for-painting-1" class="slide level2">
<h2>RNNs for painting</h2>
<p><img data-src="https://i.imgur.com/KKg33WR.jpg" /></p>
</section>
<section id="spatial-lstms" class="slide level2">
<h2>Spatial LSTMs</h2>
<p><img data-src="https://i.imgur.com/4fOP3FR.png" /></p>
<p>(<a href="https://arxiv.org/pdf/1506.03478.pdf">Theis et al, 2015</a>)</p>
</section>
<section id="spatial-lstms-generating-textures" class="slide level2">
<h2>Spatial LSTMs generating textures</h2>
<p><img data-src="https://i.imgur.com/uLYyB3l.jpg" /></p>
</section>
<section id="seq2seq-sequence-to-sequence" class="slide level2">
<h2>Seq2Seq: sequence-to-sequence</h2>
<p><img data-src="https://i.imgur.com/Ki8xpvY.png" /></p>
<p>(<a href="https://arxiv.org/pdf/1409.3215.pdf">Sutskever et al, 2014</a>)</p>
</section>
<section id="seq2seq-neural-machine-translation" class="slide level2">
<h2>Seq2Seq: neural machine translation</h2>
<p><img data-src="https://i.imgur.com/WrZg5r4.png" /></p>
</section>
<section id="show-and-tell-image2seq" class="slide level2">
<h2>Show and Tell: “Image2Seq”</h2>
<p><img data-src="https://i.imgur.com/hyUtUjl.png" /></p>
<p>(<a href="https://arxiv.org/pdf/1411.4555.pdf">Vinyals et al, 2015</a>)</p>
</section>
<section id="show-and-tell-image2seq-1" class="slide level2">
<h2>Show and Tell: “Image2Seq”</h2>
<p><img data-src="https://i.imgur.com/MSU5mIw.jpg" /></p>
<p>(<a href="https://arxiv.org/pdf/1411.4555.pdf">Vinyals et al, 2015</a>)</p>
</section>
<section id="sentence-to-parsing-tree-seq2tree" class="slide level2">
<h2>Sentence to Parsing tree “Seq2Tree”</h2>
<p><img data-src="https://i.imgur.com/ywwmSCK.png" /></p>
<p>(<a href="https://arxiv.org/abs/1412.7449">Vinyals et al, 2014</a>)</p>
</section>
<section id="general-algorithms-as-seq2seq" class="slide level2">
<h2>General algorithms as Seq2Seq</h2>
<p>travelling salesman</p>
<p><img data-src="https://i.imgur.com/B8jsaMt.png" /></p>
<p>(<a href="https://arxiv.org/abs/1506.03134">Vinyals et al, 2015</a>)</p>
</section>
<section id="general-algorithms-as-seq2seq-1" class="slide level2">
<h2>General algorithms as Seq2Seq</h2>
<p>convex hull and triangulation</p>
<p><img data-src="https://i.imgur.com/mTQhCTi.png" /></p>
</section>
<section id="pointer-networks" class="slide level2">
<h2>Pointer networks</h2>
<p><img data-src="https://i.imgur.com/JhFpOkZ.png" /></p>
</section>
<section id="revisiting-the-basic-idea" class="slide level2">
<h2>Revisiting the basic idea</h2>
<p><img data-src="https://i.imgur.com/Ki8xpvY.png" /></p>
<p>“Asking the network too much”</p>
</section>
<section id="attention-layer" class="slide level2">
<h2>Attention layer</h2>
<p><img data-src="https://i.imgur.com/nskRYts.png" /></p>
</section>
<section id="attention-layer-1" class="slide level2">
<h2>Attention layer</h2>
<p>Attention weights:</p>
<p><span class="math display">\[
\alpha_{t,s} = \frac{e^{\mathbf{e}^T_t \mathbf{d}_s}}{\sum_u e^{\mathbf{e}^T_t \mathbf{d}_s}} 
\]</span></p>
<p>Context vector:</p>
<p><span class="math display">\[
\mathbf{c}_s = \sum_{t=1}^T \alpha_{t,s} \mathbf{e}_t
\]</span></p>
</section>
<section id="attention-layer-visualised" class="slide level2">
<h2>Attention layer visualised</h2>
<p><img data-src="https://i.imgur.com/MVt50yl.png%20=500x" /></p>
</section>
<section class="slide level2">

<p><img data-src="https://i.imgur.com/uNwTRux.png" /></p>
</section>
<section id="to-engage-with-this-material-at-home" class="slide level2">
<h2>To engage with this material at home</h2>
<p>Try the <a href="https://github.com/udacity/deep-learning-v2-pytorch/blob/master/recurrent-neural-networks/char-rnn/Character_Level_RNN_Exercise.ipynb">char-RNN Exercise</a> from Udacity.</p>
</section>
<section id="side-note-dealing-with-depth" class="slide level2">
<h2>Side note: dealing with depth</h2>
<p><img data-src="https://i.imgur.com/sTaW6fT.png" /></p>
</section>
<section id="side-note-dealing-with-depth-1" class="slide level2">
<h2>Side note: dealing with depth</h2>
<p><img data-src="https://i.imgur.com/2oCXEIh.png" /></p>
</section>
<section id="side-note-dealing-with-depth-2" class="slide level2">
<h2>Side note: dealing with depth</h2>
<p><img data-src="https://i.imgur.com/w8BmEfS.png%20=260x" /></p>
</section>
<section id="deep-residual-networks-resnets" class="slide level2">
<h2>Deep Residual Networks (ResNets)</h2>
<p><img data-src="https://i.imgur.com/hJK6Rx4.png" /></p>
</section>
<section id="deep-residual-networks-resnets-1" class="slide level2">
<h2>Deep Residual Networks (ResNets)</h2>
<p><img data-src="https://i.imgur.com/wjBWNn9.png" /></p>
</section>
<section id="resnets" class="slide level2">
<h2>ResNets</h2>
<ul>
<li>allow for much deeper networks (101, 152 layer)</li>
<li>performance increases with depth</li>
<li>new record in benchmarks (ImageNet, COCO)</li>
<li>used almost everywhere now</li>
</ul>
</section>
<section id="resnets-behave-like-ensembles" class="slide level2">
<h2>Resnets behave like ensembles</h2>
<p><img data-src="https://i.imgur.com/LNPB4e8.png" /></p>
<p>from (<a href="https://arxiv.org/pdf/1605.06431.pdf">Veit et al, 2016</a>)</p>
</section>
<section id="densenets" class="slide level2">
<h2>DenseNets</h2>
<p><img data-src="https://i.imgur.com/Eyyx1uK.png" /></p>
</section>
<section id="densenets-1" class="slide level2">
<h2>DenseNets</h2>
<p><img data-src="https://i.imgur.com/a5dQUl8.png" /></p>
</section>
<section id="back-to-rnns" class="slide level2">
<h2>Back to RNNs</h2>
<ul>
<li>like ResNets, LSTMs and GRU create “shortcuts”</li>
<li>allows information to skip processing</li>
<li>data-dependent gating</li>
<li>data-dependent shortcuts</li>
</ul>
</section>
<section id="section-1" class="slide level2">
<h2></h2>
<p>{Different from what we had before:</p>
<ul>
<li>different input type (sequences)</li>
<li>different network building blocks
<ul>
<li>multiplicative interactions</li>
<li>gating</li>
<li>skip connections</li>
</ul></li>
<li>different objective
<ul>
<li>maximum likelihood</li>
<li>generative modelling</li>
</ul></li>
</ul>
</section>
<section class="slide level2">

</section>
    </div>
  </div>

  <script src="https://unpkg.com/reveal.js@3.9.2/lib/js/head.min.js"></script>
  <script src="https://unpkg.com/reveal.js@3.9.2/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Push each slide change to the browser history
        history: true,
        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // Optional reveal.js plugins
        dependencies: [
          { src: 'https://unpkg.com/reveal.js@3.9.2/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'https://unpkg.com/reveal.js@3.9.2/plugin/zoom-js/zoom.js', async: true },
          { src: 'https://unpkg.com/reveal.js@3.9.2/plugin/math/math.js', async: true },
          { src: 'https://unpkg.com/reveal.js@3.9.2/plugin/notes/notes.js', async: true }
        ]
      });
    </script>
    </body>
</html>
