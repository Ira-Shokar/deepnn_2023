<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="dcterms.date" content="2021-02-23">
  <title>Transformers</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@3.9.2/css/reveal.css">
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@3.9.2/css/theme/black.css" id="theme">
  <link rel="stylesheet" href="../assets/css/talks-small.css"/>
  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'https://unpkg.com/reveal.js@3.9.2/css/print/pdf.css' : 'https://unpkg.com/reveal.js@3.9.2/css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!--[if lt IE 9]>
  <script src="https://unpkg.com/reveal.js@3.9.2/lib/js/html5shiv.js"></script>
  <![endif]-->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_SVG" type="text/javascript"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    TeX: {
         extensions: ["color.js"]
      }
    });
  </script>
  <script src="../assets/js/figure-animate.js"></script>
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Transformers</h1>
  <p class="author" style="text-align:center"><a href="http://niclane.org/">Nic Lane</a></p>
  <p class="date" style="text-align:center"><time>2021-02-23</time></p>
  <p class="venue" style="text-align:center">Zoom</p>
</section>

<section class="slide level2">

<!-- Do not edit this file locally. -->
<!---->
<!-- Do not edit this file locally. -->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
</section>
<section id="plan-for-the-day" class="slide level2">
<h2>Plan for the Day</h2>
<ul>
<li><strong>Attention, Attention!</strong></li>
<li>Transformers: Attention is All You Need.</li>
<li>Transformer extensions:
<ul>
<li>Reformers - alternative attention operator</li>
<li>Linformers - linear complexity</li>
<li>Transformer training difficulty<br />
</li>
<li>Switch Transformers</li>
</ul></li>
<li>Going beyond NLP
<ul>
<li>Image is Worth 16x16 Words</li>
<li>AlphaFold 2</li>
</ul></li>
</ul>
</section>
<section id="think-back-to-last-week-seq2seq" class="slide level2">
<h2>Think back to last week: Seq2Seq</h2>
<ul>
<li><strong>Encoder</strong> compresses inut into a number of hidden notes - the state vector.</li>
<li><strong>Decoder</strong> , initialized with the context vector, generates the output (translation)</li>
</ul>
<center>
<div class="centered" style="">
<img class="" src="../slides/diagrams/deepnn/encoder-decoder-example.png" width="1300px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<ul>
<li><strong>Memory, and gradient stability, remain the key limitation even when LSTMs and GRUs are used.</strong></li>
</ul>
</section>
<section id="neural-machine-translation-by-jointly-learning-to-align-and-translate" class="slide level2">
<h2>Neural Machine Translation by Jointly Learning to Align and Translate</h2>
<center>
<table>
<tr>
<ul>
<div class="centered" style="">
<img class="" src="../slides/diagrams/deepnn/encoder-decoder-example.png" width="1200px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</ul>
<ul>
<div class="centered" style="">
<img class="" src="../slides/diagrams/deepnn/LSTM.png" width="700px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</ul>
</tr>
</table>
<br />

</center>
</section>
<section id="learning-alignment---the-first-attention" class="slide level2">
<h2>Learning Alignment - the first Attention</h2>
<span style="text-align:right">
<div class="centered" style="">
<img class="" src="../slides/diagrams/deepnn/rnnAttention.png" width="600px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<p></span></p>
<ul>
<li>Fixed-length context vector was a key performance bottleneck in 2015.</li>
<li>Bahdanau et al. (2015) propose to (soft-)search for parts of a source <br>sentence that are relevant to predicting a target word.</li>
<li>The method, based on the Cho et al. (2014) Encoder–Decoder:
</li>
<ul>
<li><strong>Encoder:</strong> bi-directional RNN.</li>
<li><strong>Decoder:</strong> gated RNN.</li>
<li><strong>Innovation:</strong> each time-step gets its own separate context vector: <span class="math display">\[\large c_t = \sum_{j=1}^{\top} ( \alpha_{t,j} h_t)\]</span> <span class="math display">\[\large \alpha_{t,j} = \frac{exp(e_{t,j})} {\sum_{k=1}^{\top}e_{t,k}}\]</span> <span class="math display">\[\large e_{t,j} = f(s_{t-1}, h_j)\]</span></li>
</ul></li>
</ul>
<p><em>Function <span class="math inline">\(f\)</span> implemented as a fully connected layer.</em></p>
</section>
<section id="attention-attention" class="slide level2">
<h2>Attention, Attention!</h2>
<center>
<div class="centered" style="">
<img class="" src="../slides/diagrams/deepnn/learned_attention.png" width="1100px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</section>
<section id="plan-for-the-day-1" class="slide level2">
<h2>Plan for the Day</h2>
<ul>
<li>Attention, Attention!</li>
<li><strong>Transformers: Attention is All You Need.</strong></li>
<li>Transformer extensions:
<ul>
<li>Reformers - alternative attention operator</li>
<li>Linformers - linear complexity</li>
<li>Transformer training difficulty<br />
</li>
<li>Switch Transformers</li>
</ul></li>
<li>Going beyond NLP
<ul>
<li>Image is Worth 16x16 Words</li>
<li>AlphaFold 2</li>
</ul></li>
</ul>
</section>
<section id="attention-is-all-you-need" class="slide level2">
<h2>Attention is All You Need</h2>
<center>
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/deepnn/attention.jpg" width="1300px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</section>
<section id="attention-is-all-you-need-1" class="slide level2">
<h2>Attention is All You Need</h2>
<span style="text-align:right">
<div class="centered" style="">
<img class="" src="../slides/diagrams/deepnn/scaled.png" width="1200px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<p></span></p>
<ul>
<li>Scaled dot-product attention (no parameters):</li>
</ul>
<p><span class="math display">\[ \text{attention}(Q,K,V)=\text{softmax}\left(\frac{QK^{\top}}{\sqrt{d_{K}}}\right) V\]</span></p>
</section>
<section id="attention-is-all-you-need-2" class="slide level2">
<h2>Attention is All You Need</h2>
<center>
<div class="centered" style="">
<img class="" src="../slides/diagrams/deepnn/QKV.png" width="1600px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</section>
<section id="attention-is-all-you-need-3" class="slide level2">
<h2>Attention is All You Need</h2>
<span style="text-align:right">
<div class="centered" style="">
<img class="" src="inputs3/multihead.png" width="1100px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<p></span></p>
<ul>
<li>Scaled dot-product attention (no parameters):</li>
</ul>
<p><span class="math display">\[ \text{attention}(Q,K,V)=\text{softmax}\left(\frac{QK^{\top}}{\sqrt{d_{K}}}\right) V\]</span></p>
<ul>
<li>Multi-head attention (parameters: <span class="math inline">\(W_0, W_1^Q, W_1^K, W_1^V, \dots\)</span>): <span class="math inline">\(\text{MultiHead}(Q,K,V)=\text{concat}(\text{head}_{1},\dots,\text{head}_{h}) W_{0}\)</span> <span class="math inline">\(\text{head}_i=\text{attention}(QW_i^Q, KW_i^K, VW_i^V)\)</span></li>
</ul>
</section>
<section id="attention-is-all-you-need-the-transformer" class="slide level2">
<h2>Attention is All You Need: the Transformer</h2>
<center>
<table>
<tr>
<td>
<div class="centered" style="">
<img class="" src="../slides/diagrams/deepnn/rnnAttention.png" width="575px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td>
<div class="centered" style="">
<img class="" src="../slides/diagrams/deepnn/transformer.png" width="675px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</center>
</section>
<section id="attention-is-all-you-need-4" class="slide level2">
<h2>Attention is All You Need</h2>
<center>
<em>The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German <br> and English-to-French newstest 2014 tests at a fraction of the training cost.</em>
</center>
<center>
<div class="centered" style="">
<img class="" src="../slides/diagrams/deepnn/transformer_perf.png" width="1500px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</section>
<section id="plan-for-the-day-2" class="slide level2">
<h2>Plan for the Day</h2>
<ul>
<li>Attention, Attention!</li>
<li>Transformers: Attention is All You Need.</li>
<li><strong>Transformer extensions:</strong>
<ul>
<li>Reformers - alternative attention operator</li>
<li>Linformers - linear complexity</li>
<li>Transformer training difficulty<br />
</li>
<li>Switch Transformers</li>
</ul></li>
<li>Going beyond NLP
<ul>
<li>Image is Worth 16x16 Words</li>
<li>AlphaFold 2</li>
</ul></li>
</ul>
</section>
<section id="reformers" class="slide level2">
<h2>Reformers</h2>
<center>
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/deepnn/reformers.jpg" width="1300px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</section>
<section id="reformers-locality-sensitive-hashing" class="slide level2">
<h2>Reformers: Locality Sensitive Hashing</h2>
<center>
<em>Rather than attending all-to-all, split the sequence up. Kitaev et al. (2020) employ a hashing scheme first proposed by Andoni et al. (2015).</em>
</center>
<center>
<div class="centered" style="">
<img class="" src="../slides/diagrams/deepnn/reformer_mechanism.png" width="1500px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</section>
<section id="reformers-performance" class="slide level2">
<h2>Reformers performance</h2>
<center>
<div class="centered" style="">
<img class="" src="../slides/diagrams/deepnn/reformer_performance.png" width="1400px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<center>
<em>Loss: log-likelihood and perplexity expressed in bits per dimension.</em>
</center>
</section>
<section id="linformers" class="slide level2">
<h2>Linformers</h2>
<center>
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/deepnn/linformers.jpg" width="1300px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</section>
<section id="linformers-1" class="slide level2">
<h2>Linformers</h2>
<ul>
<li><p>Basic attention (no parameters):</p>
<p><span class="math display">\[ \text{attention}(Q,K,V)=\underbrace{\text{softmax}\left(\frac{QK^{\top}}{\sqrt{d_{K}}}\right)}_\text{P} V\]</span></p></li>
<li><p>Wang et al. prove theoretically and check empirically that P is low rank.</p></li>
</ul>
<center>
<div class="centered" style="">
<img class="" src="../slides/diagrams/deepnn/linformer_eigen.png" width="1600px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</section>
<section id="linformers-2" class="slide level2">
<h2>Linformers</h2>
<center>
<em>The idea is very simple - add a simple projection between the weighted K, Q and their joint dot product. <br>This fixes to a constant the dimension of the matrices entering the self-attention mechanism.</em>
</center>
<center>
<div class="centered" style="">
<img class="" src="../slides/diagrams/deepnn/linformer_arch.png" width="1200px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</section>
<section id="linformers-performance" class="slide level2">
<h2>Linformers performance</h2>
<center>
<div class="centered" style="">
<img class="" src="../slides/diagrams/deepnn/linformer_performance.png" width="1400px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</section>
<section id="training-transformers" class="slide level2">
<h2>Training Transformers</h2>
<center>
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/deepnn/training.jpg" width="1300px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</section>
<section id="layer-normalization-in-transformers" class="slide level2">
<h2>Layer Normalization in Transformers</h2>
<center>
<div class="centered" style="">
<img class="" src="../slides/diagrams/deepnn/layerNorm.png" width="1400px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</section>
<section id="training-transformers-1" class="slide level2">
<h2>Training Transformers</h2>
<table>
<tr>
<th>
<center>
Unbalanced Gradients
</center>
</th>
<th>
<center>
Amplification-induced Instability
</center>
</th>
</tr>
<tr>
<td>
<div class="centered" style="">
<img class="" src="../slides/diagrams/deepnn/training_sec3.png" width="1200px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td>
<div class="centered" style="">
<img class="" src="../slides/diagrams/deepnn/training_sec4.png" width="1050px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</section>
<section id="switch-transformers" class="slide level2">
<h2>Switch Transformers</h2>
<center>
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/deepnn/switch.jpg" width="1300px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</section>
<section id="switch-transformers-1" class="slide level2">
<h2>Switch Transformers</h2>
<center>
<div class="centered" style="">
<img class="" src="../slides/diagrams/deepnn/switch.png" width="1200px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</section>
<section id="switch-transformers---speed-up" class="slide level2">
<h2>Switch Transformers - speed-up</h2>
<center>
<div class="centered" style="">
<img class="" src="../slides/diagrams/deepnn/switch_speed.png" width="1200px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</section>
<section id="switch-transformers---multi-task-performance" class="slide level2">
<h2>Switch Transformers - multi-task performance</h2>
<center>
<div class="centered" style="">
<img class="" src="../slides/diagrams/deepnn/switch_multitask.png" width="1900px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</section>
<section id="plan-for-the-day-3" class="slide level2">
<h2>Plan for the Day</h2>
<ul>
<li>Attention, Attention!</li>
<li>Transformers: Attention is All You Need.</li>
<li>Transformer extensions:
<ul>
<li>Reformers - alternative attention operator</li>
<li>Linformers - linear complexity</li>
<li>Transformer training difficulty<br />
</li>
<li>Switch Transformers</li>
</ul></li>
<li><strong>Going beyond NLP</strong>
<ul>
<li>Image is Worth 16x16 Words</li>
<li>AlphaFold 2</li>
</ul></li>
</ul>
</section>
<section id="image-is-worth-16x16-words-embedding-and-architecture" class="slide level2">
<h2>Image is Worth 16x16 Words Embedding and Architecture</h2>
<center>
<div class="centered" style="">
<img class="" src="../slides/diagrams/deepnn/image16wordsModel.png" width="1400px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</section>
<section id="image-is-worth-16x16-words" class="slide level2">
<h2>Image is Worth 16x16 Words</h2>
<center>
<div class="centered" style="">
<img class="" src="../slides/diagrams/deepnn/image16wordsViz.png" width="2000px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</section>
<section id="image-is-worth-16x16-words-1" class="slide level2">
<h2>Image is Worth 16x16 Words</h2>
<center>
<div class="centered" style="">
<img class="" src="../slides/diagrams/deepnn/image16wordsRes.png" width="1100px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<ul>
<center>
<div class="centered" style="">
<img class="" src="../slides/diagrams/deepnn/image16wordsRes2.png" width="800px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</ul>
<ul>
<center>
<div class="centered" style="">
<img class="" src="../slides/diagrams/deepnn/image16wordsRes3.png" width="800px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</ul>
</section>
<section id="alphafold---the-protein-folding-problem" class="slide level2">
<h2>AlphaFold - the protein folding problem</h2>
<center>
<div class="centered" style="">
<img class="" src="../slides/diagrams/deepnn/protein.gif" width="1600px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</section>
<section id="convolutional-solution-alphafold-1" class="slide level2">
<h2>Convolutional solution: AlphaFold 1</h2>
<table>
<tr>
<td>
<center>
<div class="centered" style="">
<img class="" src="../slides/diagrams/deepnn/AlphaFold1a.png" width="1100px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</td>
<td>
<center>
<div class="centered" style="">
<img class="" src="../slides/diagrams/deepnn/AlphaFold1.png" width="1000px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</td>
</tr>
</table>
</section>
<section id="transformer-based-solution-alphafold-2" class="slide level2">
<h2>Transformer-based solution: AlphaFold 2</h2>
<table>
<tr>
<td>
<center>
<div class="centered" style="">
<img class="" src="../slides/diagrams/deepnn/AlphaFold.png" width="1800px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</td>
<td>
<center>
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/deepnn/AlphaFold_performance.jpg" width="1200px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</td>
</tr>
</table>
</section>
<section id="summary-of-the-day" class="slide level2">
<h2>Summary of the Day</h2>
<ul>
<li>Attention, Attention!</li>
<li>Transformers: Attention is All You Need.</li>
<li>Transformer extensions:
<ul>
<li>Reformers - alternative attention operator</li>
<li>Linformers - linear complexity</li>
<li>Transformer training difficulty<br />
</li>
<li>Switch Transformers</li>
</ul></li>
<li>Going beyond NLP
<ul>
<li>Image is Worth 16x16 Words</li>
<li>AlphaFold 2</li>
</ul></li>
</ul>
</section>
    </div>
  </div>

  <script src="https://unpkg.com/reveal.js@3.9.2/lib/js/head.min.js"></script>
  <script src="https://unpkg.com/reveal.js@3.9.2/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Push each slide change to the browser history
        history: true,
        // Transition style
        transition: 'None', // none/fade/slide/convex/concave/zoom
        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // Optional reveal.js plugins
        dependencies: [
          { src: 'https://unpkg.com/reveal.js@3.9.2/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'https://unpkg.com/reveal.js@3.9.2/plugin/zoom-js/zoom.js', async: true },
          { src: 'https://unpkg.com/reveal.js@3.9.2/plugin/math/math.js', async: true },
          { src: 'https://unpkg.com/reveal.js@3.9.2/plugin/notes/notes.js', async: true }
        ]
      });
    </script>
    </body>
</html>
