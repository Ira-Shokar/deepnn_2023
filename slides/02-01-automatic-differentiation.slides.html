<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="dcterms.date" content="2022-01-27">
  <title>More Generalisation and Automatic Differentiation</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@3.9.2/css/reveal.css">
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@3.9.2/css/theme/black.css" id="theme">
  <link rel="stylesheet" href="https://inverseprobability.com/assets/css/talks.css"/>
  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'https://unpkg.com/reveal.js@3.9.2/css/print/pdf.css' : 'https://unpkg.com/reveal.js@3.9.2/css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!--[if lt IE 9]>
  <script src="https://unpkg.com/reveal.js@3.9.2/lib/js/html5shiv.js"></script>
  <![endif]-->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_SVG" type="text/javascript"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    TeX: {
         extensions: ["color.js"]
      }
    });
  </script>
  <script src="../assets/js/figure-animate.js"></script>
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">More Generalisation and Automatic Differentiation</h1>
  <p class="author" style="text-align:center"><a href="https://www.inference.vc/about/">Ferenc Huszár</a></p>
  <p class="date" style="text-align:center"><time>2022-01-27</time></p>
  <p class="venue" style="text-align:center">LT1, William Gates Building</p>
</section>

<section class="slide level2">

<!-- Do not edit this file locally. -->
<!---->
<!-- Do not edit this file locally. -->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
</section>
<section id="reddit-group" class="slide level2">
<h2>Reddit Group</h2>
<p><a href="https://www.reddit.com/r/CST_DeepNN/">https://www.reddit.com/r/CST_DeepNN/</a></p>
<p>{ ## Approximation</p>
</section>
<section class="slide level2">

<h3 id="basic-multilayer-perceptron">Basic Multilayer Perceptron</h3>
<p><span class="math display">\[\begin{align}
f_l(x) &amp;= \phi(W_l f_{l-1}(x) + b_l)\\
f_0(x) &amp;= x
\end{align}\]</span></p>
</section>
<section class="slide level2">

<h3 id="basic-multilayer-perceptron-1">Basic Multilayer Perceptron</h3>
<p><span class="math display">\[
\small
f_L(x) = \phi\left(b_L + W_L \phi\left(b_{L-1} + W_{L-1} \phi\left( \cdots \phi\left(b_1 + W_1 x\right) \cdots \right)\right)\right)
\]</span></p>
</section>
<section class="slide level2">

<h3 id="rectified-linear-unit">Rectified Linear Unit</h3>
<p><span class="math display">\[
\phi(x) = \left\{\matrix{0&amp;\text{when }x\leq 0\\x&amp;\text{when }x&gt;0}\right.
\]</span></p>
<p><img data-src="https://i.imgur.com/SxKdrzb.png" /></p>
</section>
<section class="slide level2">

<h3 id="what-can-these-networks-represent">What can these networks represent?</h3>
<p><span class="math display">\[
\operatorname{ReLU}(\mathbf{w}_1x - \mathbf{b}_1)
\]</span></p>
<p><img data-src="https://i.imgur.com/rN5wRVJ.png" /></p>
</section>
<section class="slide level2">

<h3 id="what-can-these-networks-represent-1">What can these networks represent?</h3>
<p><span class="math display">\[
f(x) = \mathbf{w}^T_2 \operatorname{ReLU}(\mathbf{w}_1x - \mathbf{b}_1)
\]</span></p>
<p><img data-src="https://i.imgur.com/kX3nuYg.png" /></p>
</section>
<section class="slide level2">

<h3 id="single-hidden-layer">Single hidden layer</h3>
<p>number of kinks <span class="math inline">\(\approx O(\)</span> width of network <span class="math inline">\()\)</span></p>
</section>
<section class="slide level2">

<h3 id="example-sawtooth-network">Example: “sawtooth” network</h3>
<p><span class="math display">\[\begin{align}
f_l(x) &amp;= 2\vert f_{l-1}(x)\vert - 2 \\
f_0(x) &amp;= x
\end{align}\]</span></p>
</section>
<section class="slide level2">

<h3 id="sawtooth-network">Sawtooth network</h3>
<p><span class="math display">\[\begin{align}
f_l(x) &amp;= 2 \operatorname{ReLU}(f_{l-1}(x)) + 2 \operatorname{ReLU}(-f_{l-1}(x)) - 2\\
f_0(x) &amp;= x
\end{align}\]</span></p>
</section>
<section class="slide level2">

<h3 id="layer-network"><span class="math inline">\(0\)</span>-layer network</h3>
<p><img data-src="https://i.imgur.com/pucqIVN.png" /></p>
</section>
<section class="slide level2">

<h3 id="layer-network-1"><span class="math inline">\(1\)</span>-layer network</h3>
<p><img data-src="https://i.imgur.com/YOTtTY7.png" /></p>
</section>
<section class="slide level2">

<h3 id="layer-network-2"><span class="math inline">\(2\)</span>-layer network</h3>
<p><img data-src="https://i.imgur.com/reii7O5.png" /></p>
</section>
<section class="slide level2">

<h3 id="layer-network-3"><span class="math inline">\(3\)</span>-layer network</h3>
<p><img data-src="https://i.imgur.com/J6KiUHI.png" /></p>
</section>
<section class="slide level2">

<h3 id="layer-network-4"><span class="math inline">\(4\)</span>-layer network</h3>
<p><img data-src="https://i.imgur.com/fHTZhU0.png" /></p>
</section>
<section class="slide level2">

<h3 id="layer-network-5"><span class="math inline">\(5\)</span>-layer network</h3>
<p><img data-src="https://i.imgur.com/ni4QV2b.png" /></p>
</section>
<section class="slide level2">

<h3 id="deep-relu-networks">Deep ReLU networks</h3>
<p>number of kinks <span class="math inline">\(\approx O(2^\text{depth of network})\)</span></p>
</section>
<section class="slide level2">

<h3 id="in-higher-dimensions">In higher dimensions</h3>
<p><img data-src="https://i.imgur.com/0NVHFEN.png" /></p>
</section>
<section class="slide level2">

<h3 id="in-higher-dimensions-1">In higher dimensions</h3>
<p><img data-src="https://i.imgur.com/DJtv5Yj.jpg" /></p>
</section>
<section class="slide level2">

<h3 id="approximation-summary">Approximation: summary</h3>
<ul>
<li>depth increases model complexity more than width</li>
<li>model clas defined by deep networks is VERY LARGE</li>
<li>both an advantage, but and cause for concern</li>
<li>“complex models don’t generalize”</li>
</ul>
</section>
<section id="generalization" class="slide level2">
<h2>Generalization</h2>
</section>
<section id="generalization-1" class="slide level2">
<h2>Generalization</h2>
<p><img data-src="https://i.imgur.com/Tu5SHpr.png" /></p>
</section>
<section id="generalization-2" class="slide level2">
<h2>Generalization</h2>
<p><img data-src="https://i.imgur.com/8bkhxAv.png" /></p>
</section>
<section id="generalization-3" class="slide level2">
<h2>Generalization</h2>
<p><img data-src="https://i.imgur.com/YHedAr6.png" /></p>
</section>
<section id="generalization-deep-nets" class="slide level2">
<h2>Generalization: deep nets</h2>
<p><img data-src="https://i.imgur.com/bfyRBsx.png" /></p>
</section>
<section id="generalization-deep-nets-1" class="slide level2">
<h2>Generalization: deep nets</h2>
<p><img data-src="https://i.imgur.com/fzLYvHe.png" /></p>
</section>
<section id="generalization-summary" class="slide level2">
<h2>Generalization: summary</h2>
<ul>
<li><strong>classical view:</strong> generalization is property of model class and loss function</li>
<li><strong>new view:</strong> it is also a property of the optimization algorithm</li>
</ul>
</section>
<section id="generalization-4" class="slide level2">
<h2>Generalization</h2>
<ul>
<li>optimization is core to deep learning</li>
<li>new tools and insights:
<ul>
<li>infinite width neural networks</li>
<li>neural tangent kernel <a href="https://arxiv.org/abs/1806.07572">(Jacot et al, 2018)</a></li>
<li>deep linear models (<a href="https://arxiv.org/abs/1905.13655">e.g. Arora et al, 2019</a>)</li>
<li>importance of initialization</li>
<li>effect of gradient noise</li>
</ul></li>
</ul>
</section>
<section id="gradient-based-optimization" class="slide level2">
<h2>Gradient-based optimization</h2>
<p><span class="math display">\[
\mathcal{L}(\theta) = \sum_{n=1}^N \ell(y_n, f(x_n, \theta))
\]</span></p>
<p><span class="math display">\[
\theta_{t+1} = \theta_t - \eta \nabla_\theta \mathcal{L}(\theta)
\]</span></p>
</section>
<section class="slide level2">

<h3 id="basic-multilayer-perceptron-2">Basic Multilayer Perceptron</h3>
<p><span class="math display">\[
\small
f_L(x) = \phi\left(b_L + W_L \phi\left(b_{L-1} + W_{L-1} \phi\left( \cdots \phi\left(b_1 + W_1 x\right) \cdots \right)\right)\right)
\]</span></p>
</section>
<section id="general-deep-function-composition" class="slide level2">
<h2>General deep function composition</h2>
<p><span class="math display">\[
f_L(f_{L-1}(\cdots f_1(\mathbb{w})))
\]</span></p>
<p>How do I calculate the derivative of <span class="math inline">\(f_L(\mathbb{w})\)</span> with respect to <span class="math inline">\(\mathbb{w}\)</span>?</p>
</section>
<section id="chain-rule" class="slide level2">
<h2>Chain rule</h2>
<p><span class="math display">\[
\frac{\partial \mathbf{f}_L}{\partial \mathbb{w}} = \frac{\partial \mathbf{f}_L}{\partial \mathbf{f}_{L-1}} \frac{\partial \mathbf{f}_{L-1}}{\partial \mathbf{f}_{L-2}} \frac{\partial \mathbf{f}_{L-2}}{\partial \mathbf{f}_{L-3}} \cdots \frac{\partial \mathbf{f}_3}{\partial \mathbf{f}_{2}} \frac{\partial \mathbf{f}_2}{\partial \mathbf{f}_{1}} \frac{\partial \mathbf{f}_1}{\partial \mathbf{w}} 
\]</span></p>
</section>
<section id="how-to-evaluate-this" class="slide level2">
<h2>How to evaluate this?</h2>
<p><span class="math display">\[
\frac{\partial \mathbf{f}_L}{\partial \mathbb{w}} = \frac{\partial \mathbf{f}_L}{\partial \mathbf{f}_{L-1}} \frac{\partial \mathbf{f}_{L-1}}{\partial \mathbf{f}_{L-2}} \frac{\partial \mathbf{f}_{L-2}}{\partial \mathbf{f}_{L-3}} \cdots \frac{\partial \mathbf{f}_3}{\partial \mathbf{f}_{2}} \frac{\partial \mathbf{f}_2}{\partial \mathbf{f}_{1}} \frac{\partial \mathbf{f}_1}{\partial \mathbf{w}} 
\]</span></p>
<p><span class="math display">\[
\small
\frac{\partial \mathbf{f}_L}{\partial \mathbb{w}} = \frac{\partial \mathbf{f}_L}{\partial \mathbf{f}_{L-1}} \left( \frac{\partial \mathbf{f}_{L-1}}{\partial \mathbf{f}_{L-2}} \left( \frac{\partial \mathbf{f}_{L-2}}{\partial \mathbf{f}_{L-3}} \cdots \left( \frac{\partial \mathbf{f}_3}{\partial \mathbf{f}_{2}} \left( \frac{\partial \mathbf{f}_2}{\partial \mathbf{f}_{1}} \frac{\partial \mathbf{f}_1}{\partial \mathbf{w}} \right) \right) \cdots \right) \right)
\]</span></p>
</section>
<section id="or-like-this" class="slide level2">
<h2>Or like this?</h2>
<p><span class="math display">\[
\frac{\partial \mathbf{f}_L}{\partial \mathbb{w}} = \frac{\partial \mathbf{f}_L}{\partial \mathbf{f}_{L-1}} \frac{\partial \mathbf{f}_{L-1}}{\partial \mathbf{f}_{L-2}} \frac{\partial \mathbf{f}_{L-2}}{\partial \mathbf{f}_{L-3}} \cdots \frac{\partial \mathbf{f}_3}{\partial \mathbf{f}_{2}} \frac{\partial \mathbf{f}_2}{\partial \mathbf{f}_{1}} \frac{\partial \mathbf{f}_1}{\partial \mathbf{w}} 
\]</span></p>
<p><span class="math display">\[
\small
\frac{\partial \mathbf{f}_L}{\partial \mathbb{w}} = \left( \left( \cdots \left( \left( \frac{\partial \mathbf{f}_L}{\partial \mathbf{f}_{L-1}} \frac{\partial \mathbf{f}_{L-1}}{\partial \mathbf{f}_{L-2}}  \right) \frac{\partial \mathbf{f}_{L-2}}{\partial \mathbf{f}_{L-3}} \right) \cdots \frac{\partial \mathbf{f}_3}{\partial \mathbf{f}_{2}} \right) \frac{\partial \mathbf{f}_2}{\partial \mathbf{f}_{1}} \right) \frac{\partial \mathbf{f}_1}{\partial \mathbf{w}} 
\]</span></p>
</section>
<section id="or-in-a-funky-way" class="slide level2">
<h2>Or in a funky way?</h2>
<p><span class="math display">\[
\frac{\partial \mathbf{f}_L}{\partial \mathbb{w}} = \frac{\partial \mathbf{f}_L}{\partial \mathbf{f}_{L-1}} \frac{\partial \mathbf{f}_{L-1}}{\partial \mathbf{f}_{L-2}} \frac{\partial \mathbf{f}_{L-2}}{\partial \mathbf{f}_{L-3}} \cdots \frac{\partial \mathbf{f}_3}{\partial \mathbf{f}_{2}} \frac{\partial \mathbf{f}_2}{\partial \mathbf{f}_{1}} \frac{\partial \mathbf{f}_1}{\partial \mathbf{w}} 
\]</span></p>
<p><span class="math display">\[
\small
\frac{\partial \mathbf{f}_L}{\partial \mathbb{w}} = \frac{\partial \mathbf{f}_L}{\partial \mathbf{f}_{L-1}} \left( \left( \left( \frac{\partial \mathbf{f}_{L-1}}{\partial \mathbf{f}_{L-2}}  \right) \frac{\partial \mathbf{f}_{L-2}}{\partial \mathbf{f}_{L-3}} \right) \left( \left( \cdots \frac{\partial \mathbf{f}_3}{\partial \mathbf{f}_{2}} \right) \frac{\partial \mathbf{f}_2}{\partial \mathbf{f}_{1}} \right) \right)\frac{\partial \mathbf{f}_1}{\partial \mathbf{w}} 
\]</span></p>
</section>
<section id="automatic-differentiation" class="slide level2">
<h2>Automatic differentiation</h2>
<h3 id="forward-mode">Forward-mode</h3>
<p><span class="math display">\[
\small
\frac{\partial \mathbf{f}_L}{\partial \mathbb{w}} = \frac{\partial \mathbf{f}_L}{\partial \mathbf{f}_{L-1}} \left( \frac{\partial \mathbf{f}_{L-1}}{\partial \mathbf{f}_{L-2}} \left( \frac{\partial \mathbf{f}_{L-2}}{\partial \mathbf{f}_{L-3}} \cdots \left( \frac{\partial \mathbf{f}_3}{\partial \mathbf{f}_{2}} \left( \frac{\partial \mathbf{f}_2}{\partial \mathbf{f}_{1}} \frac{\partial \mathbf{f}_1}{\partial \mathbf{w}} \right) \right) \cdots \right) \right)
\]</span></p>
<p>Cost: <span class="math display">\[
\small
d_0d_1d_2 + d_0d_2d_3 + \ldots + d_0d_{L-1}d_L = d_0 \sum_{l=2}^{L}d_ld_{l-1}
\]</span></p>
</section>
<section id="automatic-differentiation-1" class="slide level2">
<h2>Automatic differentiation</h2>
<h3 id="reverse-mode">Reverse-mode</h3>
<p><span class="math display">\[
\small
\frac{\partial \mathbf{f}_L}{\partial \mathbb{w}} = \left( \left( \cdots \left( \left( \frac{\partial \mathbf{f}_L}{\partial \mathbf{f}_{L-1}} \frac{\partial \mathbf{f}_{L-1}}{\partial \mathbf{f}_{L-2}}  \right) \frac{\partial \mathbf{f}_{L-2}}{\partial \mathbf{f}_{L-3}} \right) \cdots \frac{\partial \mathbf{f}_3}{\partial \mathbf{f}_{2}} \right) \frac{\partial \mathbf{f}_2}{\partial \mathbf{f}_{1}} \right) \frac{\partial \mathbf{f}_1}{\partial \mathbf{w}} 
\]</span></p>
<p>Cost: <span class="math display">\[
\small
d_Ld_{L-1}d_{L-2} + d_{L}d_{L-2}d_{L-3} + \ldots + d_Ld_{1}d_0 = d_L \sum_{l=0}^{L-2}d_ld_{l+1}
\]</span></p>
</section>
<section id="automatic-differentiation-2" class="slide level2">
<h2>Automatic differentiation</h2>
<ul>
<li>in deep learning we’re most interested in scalar objectives</li>
<li><span class="math inline">\(d_L=1\)</span>, consequently, backward mode is always optimal</li>
<li>in the context of neural networks: <strong>backpropagation</strong></li>
<li>backprop has higher memory cost than forwardprop</li>
</ul>
</section>
<section id="example-calculating-a-hessian" class="slide level2">
<h2>Example: calculating a Hessian</h2>
<p><span class="math display">\[
H(\mathbb{w}) = \frac{\partial^2}{\partial\mathbf{w}\partial\mathbf{w}^T} L(\mathbf{w})
= \frac{\partial}{\partial\mathbf{w}} \mathbf{g}(\mathbf{w})
\]</span></p>
</section>
<section id="example-hessian-vector-product" class="slide level2">
<h2>Example: Hessian-vector product</h2>
<p><span class="math display">\[
\mathbf{v}^TH(\mathbf{w}) = \frac{\partial}{\partial\mathbf{w}} \left( \mathbf{v}^T \mathbf{g}(\mathbf{w}) \right)
\]</span></p>
<p>}</p>
</section>
    </div>
  </div>

  <script src="https://unpkg.com/reveal.js@3.9.2/lib/js/head.min.js"></script>
  <script src="https://unpkg.com/reveal.js@3.9.2/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Push each slide change to the browser history
        history: true,
        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // Optional reveal.js plugins
        dependencies: [
          { src: 'https://unpkg.com/reveal.js@3.9.2/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'https://unpkg.com/reveal.js@3.9.2/plugin/zoom-js/zoom.js', async: true },
          { src: 'https://unpkg.com/reveal.js@3.9.2/plugin/math/math.js', async: true },
          { src: 'https://unpkg.com/reveal.js@3.9.2/plugin/notes/notes.js', async: true }
        ]
      });
    </script>
    </body>
</html>
