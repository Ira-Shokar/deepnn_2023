<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="dcterms.date" content="2021-01-26">
  <title>Generalization and Neural Networks</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="reveal.js/css/reveal.css">
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="reveal.js/css/theme/black.css" id="theme">
  <link rel="stylesheet" href="../assets/css/talks.css"/>
  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'reveal.js/css/print/pdf.css' : 'reveal.js/css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!--[if lt IE 9]>
  <script src="reveal.js/lib/js/html5shiv.js"></script>
  <![endif]-->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_SVG" type="text/javascript"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    TeX: {
         extensions: ["color.js"]
      }
    });
  </script>
  <script src="../assets/js/figure-animate.js"></script>
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Generalization and Neural Networks</h1>
  <p class="author" style="text-align:center"><a href="http://inverseprobability.com">Neil D. Lawrence</a></p>
  <p class="date" style="text-align:center"><time>2021-01-26</time></p>
  <p class="venue" style="text-align:center">Zoom</p>
</section>

<section class="slide level2">

<!-- Do not edit this file locally. -->
<!---->
<!-- Do not edit this file locally. -->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
<!--setupplotcode{import seaborn as sns
sns.set_style('darkgrid')
sns.set_context('paper')
sns.set_palette('colorblind')}-->
</section>
<section id="quadratic-loss-and-linear-system" class="slide level2">
<h2>Quadratic Loss and Linear System</h2>
</section>
<section id="expected-loss" class="slide level2">
<h2>Expected Loss</h2>
<p><span class="math display">\[
R(\mathbf{ w}) = \int L(y, x, \mathbf{ w}) \mathbb{P}(y, x) \text{d}y
\text{d}x.
\]</span></p>
</section>
<section id="sample-based-approximations" class="slide level2">
<h2>Sample Based Approximations</h2>
<ul>
<li><p>Sample based approximation: replace true expectation with sum over samples. <span class="math display">\[
\int f(z) p(z) \text{d}z\approx \frac{1}{s}\sum_{i=1}^s f(z_i).
\]</span></p></li>
<li><p>Allows us to approximate true integral with a sum <span class="math display">\[
R(\mathbf{ w}) \approx \frac{1}{n}\sum_{i=1}^{n} L(y_i, x_i, \mathbf{ w}).
\]</span></p></li>
</ul>
</section>
<section id="quadratic-loss" class="slide level2">
<h2>Quadratic Loss</h2>
<p><span class="math display">\[
L(f) = \sum_{i=1}^n\left(y_i - f(\mathbf{ x}_i))^2
\]</span></p>
</section>
<section id="linear-model" class="slide level2">
<h2>Linear Model</h2>
<p><span class="math display">\[
L(f) = \sum_{i=1}^n\left(y_i - \mathbf{ w}^\top \mathbf{ x}_i)^2
\]</span></p>
<!-- SECTION Multiple Input Solution with Linear Algebra -->
</section>
<section id="multiple-input-solution-with-linear-algebra" class="slide level2">
<h2>Multiple Input Solution with Linear Algebra</h2>
<!-- SECTION Design Matrix -->
</section>
<section id="design-matrix" class="slide level2">
<h2>Design Matrix</h2>
</section>
<section id="writing-the-objective-with-linear-algebra" class="slide level2">
<h2>Writing the Objective with Linear Algebra</h2>
<p><span class="math display">\[
L(\mathbf{ w}) = \sum_{i=1}^n(y_i - f(\mathbf{ x}_i; \mathbf{ w}))^2,
\]</span></p>
<p><span class="math display">\[
f(\mathbf{ x}_i; \mathbf{ w}) = \designVector_i^\top \mathbf{ w}.
\]</span></p>
<p><span class="math display">\[
a = \sum_{i=1}^{k} b^2_i = \mathbf{b}^\top\mathbf{b},
\]</span></p>
<p><span class="math display">\[
\mathbf{ y}= \begin{bmatrix}y_1\\ y_2\\ \vdots \\ y_n\end{bmatrix}
\]</span></p>
<p><span class="math display">\[
\mathbf{ f}(\mathbf{ x}_1; \mathbf{ w}) = \begin{bmatrix}f(\mathbf{ x}_1; \mathbf{ w})\\ f(\mathbf{ x}_2; \mathbf{ w})\\ \vdots \\ f(\mathbf{ x}_n; \mathbf{ w})\end{bmatrix}.
\]</span></p>
<p><span class="math display">\[
\mathbf{ f}= \begin{bmatrix}f_1\\f_2\\
\vdots \\ f_n\end{bmatrix}.
\]</span></p>
<p><span class="math display">\[
L(\mathbf{ w}) = (\mathbf{ y}- \mathbf{ f})^\top(\mathbf{ y}- \mathbf{ f})
\]</span></p>
<p><span class="math display">\[
f_i = \designVector_i^\top\mathbf{ w}.
\]</span> {This operation tells us that each element of the vector <span class="math inline">\(\mathbf{ f}\)</span> (our vector valued function) is given by an inner product between <span class="math inline">\(\mathbf{ x}_i\)</span> and <span class="math inline">\(\mathbf{ w}\)</span>. In other words it is a series of inner products. Let’s look at the definition of matrix multiplication, it takes the form <span class="math display">\[
\mathbf{c} = \mathbf{B}\mathbf{a},
\]</span> {where <span class="math inline">\(\mathbf{c}\)</span> might be a <span class="math inline">\(k\)</span> dimensional vector (which we can intepret as a <span class="math inline">\(k\times 1\)</span> dimensional matrix), and <span class="math inline">\(\mathbf{B}\)</span> is a <span class="math inline">\(k\times k\)</span> dimensional matrix and <span class="math inline">\(\mathbf{a}\)</span> is a <span class="math inline">\(k\)</span> dimensional vector (<span class="math inline">\(k\times 1\)</span> dimensional matrix).}</p>
<p><span class="math display">\[
L(\mathbf{ w}) = (\mathbf{ y}- \mathbf{ f})^\top(\mathbf{ y}- \mathbf{ f})
\]</span></p>
<!-- SECTION Objective Optimisation -->
</section>
<section id="objective-optimisation" class="slide level2">
<h2>Objective Optimisation</h2>
</section>
<section id="multivariate-derivatives" class="slide level2">
<h2>Multivariate Derivatives</h2>
<ul>
<li>We will need some multivariate calculus.</li>
<li>For now some simple multivariate differentiation: <span class="math display">\[\frac{\text{d}{\mathbf{a}^{\top}}{\mathbf{ w}}}{\text{d}\mathbf{ w}}=\mathbf{a}\]</span> and <span class="math display">\[\frac{\mathbf{ w}^{\top}\mathbf{A}\mathbf{ w}}{\text{d}\mathbf{ w}}=\left(\mathbf{A}+\mathbf{A}^{\top}\right)\mathbf{ w}\]</span> or if <span class="math inline">\(\mathbf{A}\)</span> is symmetric (<em>i.e.</em> <span class="math inline">\(\mathbf{A}=\mathbf{A}^{\top}\)</span>) <span class="math display">\[\frac{\text{d}\mathbf{ w}^{\top}\mathbf{A}\mathbf{ w}}{\text{d}\mathbf{ w}}=2\mathbf{A}\mathbf{ w}.\]</span></li>
</ul>
</section>
<section id="differentiate-the-objective" class="slide level2">
<h2>Differentiate the Objective</h2>
<p><span style="text-align:left">Differentiating with respect to the vector <span class="math inline">\(\mathbf{ w}\)</span> we obtain</span> <span class="math display">\[
\frac{\partial L\left(\mathbf{ w},\sigma^2 \right)}{\partial
\mathbf{ w}}=\frac{1}{\sigma^2} \sum _{i=1}^{n}\mathbf{ x}_i y_i-\frac{1}{\sigma^2}
\left[\sum _{i=1}^{n}\mathbf{ x}_i\mathbf{ x}_i^{\top}\right]\mathbf{ w}
\]</span> Leading to <span class="math display">\[
\mathbf{ w}^{*}=\left[\sum
_{i=1}^{n}\mathbf{ x}_i\mathbf{ x}_i^{\top}\right]^{-1}\sum
_{i=1}^{n}\mathbf{ x}_iy_i,
\]</span></p>
</section>
<section id="differentiate-the-objective-1" class="slide level2">
<h2>Differentiate the Objective</h2>
<p>Rewrite in matrix notation: <span class="math display">\[
\sum_{i=1}^{n}\mathbf{ x}_i\mathbf{ x}_i^\top = \boldsymbol{ \Phi}^\top \boldsymbol{ \Phi}
\]</span> <span class="math display">\[
\sum_{i=1}^{n}\mathbf{ x}_iy_i = \boldsymbol{ \Phi}^\top \mathbf{ y}
\]</span></p>
<!-- SECTION Update Equation for Global Optimum -->
</section>
<section id="update-equation-for-global-optimum" class="slide level2">
<h2>Update Equation for Global Optimum</h2>
</section>
<section id="update-equations" class="slide level2">
<h2>Update Equations</h2>
<ul>
<li>Update for <span class="math inline">\(\mathbf{ w}^{*}\)</span>. <span class="math display">\[\mathbf{ w}^{*} = \left(\boldsymbol{ \Phi}^\top \boldsymbol{ \Phi}\right)^{-1} \boldsymbol{ \Phi}^\top \mathbf{ y}\]</span></li>
<li>The equation for <span class="math inline">\(\left.\sigma^2\right.^{*}\)</span> may also be found <span class="math display">\[\left.\sigma^2\right.^{{*}}=\frac{\sum_{i=1}^{n}\left(y_i-\left.\mathbf{ w}^{*}\right.^{\top}\mathbf{ x}_i\right)^{2}}{n}.\]</span></li>
</ul>
</section>
<section id="solving-the-multivariate-system" class="slide level2">
<h2>Solving the Multivariate System</h2>
</section>
<section id="hessian-matrix" class="slide level2">
<h2>Hessian Matrix</h2>
<p><span class="math display">\[
\frac{\text{d}^2}{\text{d}\mathbf{ w}\text{d}\mathbf{ w}^\top} L(\mathbf{ w}) = 2\boldsymbol{ \Phi}^\top\boldsymbol{ \Phi}\mathbf{ w}.
\]</span></p>
<p>of unit length <span class="math inline">\(\mathbf{u}^\top\mathbf{u}\)</span> we have that,} <span class="math display">\[
\mathbf{u}^\top\mathbf{A} \mathbf{u} &gt; 0 \forall \mathbf{u} \text{with} $\mathbf{u}^\top\mathbf{u}=1$
\]</span></p>
</section>
<section id="eigendecomposition-of-hessian" class="slide level2">
<h2>Eigendecomposition of Hessian</h2>
<p><span class="math display">\[
\mathbf{A}\mathbf{u} = \lambda\mathbf{u}
\]</span></p>
<ul>
<li><span class="math inline">\(\boldsymbol{ \Phi}^\top\boldsymbol{ \Phi}\mathbf{ w}= \lambda\mathbf{u}\)</span></li>
</ul>
</section>
<section id="shallow-and-deep-learning" class="slide level2">
<h2>Shallow and Deep Learning</h2>
</section>
<section id="deep-neural-network" class="slide level2">
<h2>Deep Neural Network</h2>
<object class="svgplot " data="../slides/diagrams/deepgp/deep-nn1.svg" width="50%" style=" ">
</object>
</section>
<section id="deep-neural-network-1" class="slide level2">
<h2>Deep Neural Network</h2>
<object class="svgplot " data="../slides/diagrams/deepgp/deep-nn2.svg" width="50%" style=" ">
</object>
</section>
<section id="mathematically" class="slide level2">
<h2>Mathematically</h2>
<p><span class="math display">\[
\begin{align*}
    \mathbf{ h}_{1} &amp;= \phi\left(\mathbf{W}_1 \mathbf{ x}\right)\\
    \mathbf{ h}_{2} &amp;=  \phi\left(\mathbf{W}_2\mathbf{ h}_{1}\right)\\
    \mathbf{ h}_{3} &amp;= \phi\left(\mathbf{W}_3 \mathbf{ h}_{2}\right)\\
    \mathbf{ w}&amp;= \mathbf{ w}_4 ^\top\mathbf{ h}_{3}
\end{align*}
\]</span></p>
</section>
<section id="neural-network-prediction-function" class="slide level2">
<h2>Neural Network Prediction Function</h2>
<p><span class="math display">\[
 f(\mathbf{ x}; \mathbf{W})  =  \mathbf{ w}_4 ^\top\phi\left(\mathbf{W}_3 \phi\left(\mathbf{W}_2\phi\left(\mathbf{W}_1 \mathbf{ x}\right)\right)\right)
\]</span></p>
<!-- SECTION Nigeria NMIS Data -->
</section>
<section id="nigeria-nmis-data" class="slide level2">
<h2>Nigeria NMIS Data</h2>
</section>
<section id="nigeria-nmis-data-notebook" class="slide level2">
<h2>Nigeria NMIS Data: Notebook</h2>
<div class="figure">
<div id="nigerian-health-facilities-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/ml/nigerian-health-facilities.png" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Location of the over thirty four thousand health facilities registered in the NMIS data across Nigeria. Each facility plotted according to its latitude and longitude.
</aside>
</section>
<section id="multivariate-regression-on-nigeria-nmis-data" class="slide level2">
<h2>Multivariate Regression on Nigeria NMIS Data</h2>
<ul>
<li>Regress from features <code>num_nurses_fulltime</code>, <code>num_nursemidwives_fulltime</code>, <code>latitude</code> and <code>longitude</code> to <code>num_doctors_fulltime</code>.</li>
</ul>
</section>
<section id="residuals" class="slide level2">
<h2>Residuals</h2>
<div class="figure">
<div id="nigeria-nmis-num-doctors-residuals-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/ml/nigeria-nmis-num-doctors-residuals.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Residual values for the ratings from the prediction of the movie rating given the data from the film.
</aside>
</section>
<section id="solution-with-qr-decomposition" class="slide level2">
<h2>Solution with QR Decomposition</h2>
<p><span class="math display">\[
\boldsymbol{ \Phi}^\top \boldsymbol{ \Phi}\boldsymbol{\beta} =
\boldsymbol{ \Phi}^\top \mathbf{ y}
\]</span> <span class="math display">\[
(\mathbf{Q}\mathbf{R})^\top
(\mathbf{Q}\mathbf{R})\boldsymbol{\beta} = (\mathbf{Q}\mathbf{R})^\top
\mathbf{ y}
\]</span> <span class="math display">\[
\mathbf{R}^\top (\mathbf{Q}^\top \mathbf{Q}) \mathbf{R}
\boldsymbol{\beta} = \mathbf{R}^\top \mathbf{Q}^\top \mathbf{ y}
\]</span></p>
</section>
<section id="section" class="slide level2">
<h2></h2>
<p><span class="math display">\[
\mathbf{R}^\top \mathbf{R} \boldsymbol{\beta} = \mathbf{R}^\top \mathbf{Q}^\top
\mathbf{ y}
\]</span> <span class="math display">\[
\mathbf{R} \boldsymbol{\beta} = \mathbf{Q}^\top \mathbf{ y}
\]</span></p>
</section>
<section id="section-1" class="slide level2">
<h2></h2>
<ul>
<li>More nummerically stable.</li>
<li>Avoids the intermediate computation of <span class="math inline">\(\boldsymbol{ \Phi}^\top\boldsymbol{ \Phi}\)</span>.</li>
</ul>
</section>
<section id="basis-function-models" class="slide level2">
<h2>Basis Function Models</h2>
</section>
<section id="polynomial-basis" class="slide level2">
<h2>Polynomial Basis</h2>
<ul>
<li></li>
</ul>
<p><span class="math display">\[
\phi_j(x) = x^j
\]</span></p>
<script>
showDivs(0, 'polynomial_basis');
</script>
<p><small></small> <input id="range-polynomial_basis" type="range" min="0" max="4" value="0" onchange="setDivs('polynomial_basis')" oninput="setDivs('polynomial_basis')"> <button onclick="plusDivs(-1, 'polynomial_basis')">❮</button> <button onclick="plusDivs(1, 'polynomial_basis')">❯</button></p>
<div class="polynomial_basis" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/polynomial_basis000.svg" width="80%" style=" ">
</object>
</div>
<div class="polynomial_basis" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/polynomial_basis001.svg" width="80%" style=" ">
</object>
</div>
<div class="polynomial_basis" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/polynomial_basis002.svg" width="80%" style=" ">
</object>
</div>
<div class="polynomial_basis" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/polynomial_basis003.svg" width="80%" style=" ">
</object>
</div>
<div class="polynomial_basis" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/polynomial_basis004.svg" width="80%" style=" ">
</object>
</div>
</section>
<section id="functions-derived-from-polynomial-basis" class="slide level2">
<h2>Functions Derived from Polynomial Basis</h2>
<p><span class="math display">\[
f(x) = {\color{cyan}{w_0}} + {\color{green}{w_1 x}} + {\color{yellow}{w_2 x^2}} + {\color{magenta}{w_3 x^3}} + {\color{red}{w_4 x^4}}
\]</span></p>
<script>
showDivs(0, 'polynomial_function');
</script>
<p><small></small> <input id="range-polynomial_function" type="range" min="0" max="3" value="0" onchange="setDivs('polynomial_function')" oninput="setDivs('polynomial_function')"> <button onclick="plusDivs(-1, 'polynomial_function')">❮</button> <button onclick="plusDivs(1, 'polynomial_function')">❯</button></p>
<div class="polynomial_function" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/polynomial_function000.svg" width="80%" style=" ">
</object>
</div>
<div class="polynomial_function" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/polynomial_function001.svg" width="80%" style=" ">
</object>
</div>
<div class="polynomial_function" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/polynomial_function002.svg" width="80%" style=" ">
</object>
</div>
<ul>
<li>_2 ^2 + _3 ^3 + _4 ^4 $$ are <em>linear</em> in the parameters, <span class="math inline">\(\mathbf{ w}\)</span>, but <em>non-linear</em> in the input <span class="math inline">\(x^3\)</span>. Here we are showing a polynomial basis for a 1-dimensional input, <span class="math inline">\(x\)</span>, but basis functions can also be constructed for multidimensional inputs, <span class="math inline">\(\mathbf{ x}\)</span>.}</li>
</ul>
</section>
<section id="olympic-marathon-data" class="slide level2">
<h2>Olympic Marathon Data</h2>
<table>
<tr>
<td width="70%">
<ul>
<li>Gold medal times for Olympic Marathon since 1896.</li>
<li>Marathons before 1924 didn’t have a standardised distance.</li>
<li>Present results using pace per km.</li>
<li>In 1904 Marathon was badly organised leading to very slow times.</li>
</ul>
</td>
<td width="30%">
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/Stephen_Kiprotich.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<small>Image from Wikimedia Commons <a href="http://bit.ly/16kMKHQ" class="uri">http://bit.ly/16kMKHQ</a></small>
</td>
</tr>
</table>
</section>
<section id="olympic-marathon-data-1" class="slide level2">
<h2>Olympic Marathon Data</h2>
<div class="figure">
<div id="olympic-marathon-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/datasets/olympic-marathon.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Olympic marathon pace times since 1896.
</aside>
</section>
<section id="polynomial-fits-to-olympic-marthon-data" class="slide level2">
<h2>Polynomial Fits to Olympic Marthon Data</h2>
</section>
<section id="linear-fit" class="slide level2">
<h2>Linear Fit</h2>
<div class="figure">
<div id="olympic-marathon-polynomial-2-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/ml/olympic-marathon-polynomial-2.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Fit of a 1 degree polynomial (a linear model) to the olympic marathon data.
</aside>
</section>
<section id="cubic-fit" class="slide level2">
<h2>Cubic Fit</h2>
<div class="figure">
<div id="olympic-marathon-polynomial-4-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/ml/olympic-marathon-polynomial-4.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Fit of a 3 degree polynomial (a cubic model) to the olympic marathon data.
</aside>
</section>
<section id="th-degree-polynomial-fit" class="slide level2">
<h2>9th Degree Polynomial Fit</h2>
<div class="figure">
<div id="olympic-marathon-polynomial-10-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/ml/olympic-marathon-polynomial-10.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Fit of a 9 degree polynomial to the olympic marathon data.
</aside>
</section>
<section id="th-degree-polynomial-fit-1" class="slide level2">
<h2>26th Degree Polynomial Fit</h2>
<div class="figure">
<div id="olympic-marathon-polynomial-27-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/ml/olympic-marathon-polynomial-27.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Fit of a 26 degree polynomial to the olympic marathon data.
</aside>
</section>
<section id="th-degree-polynomial-fit-2" class="slide level2">
<h2>16th Degree Polynomial Fit</h2>
<div class="figure">
<div id="olympic-marathon-polynomial-17-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/ml/olympic-marathon-polynomial-17.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Fit of a 16 degree polynomial to the olympic marathon data.
</aside>
</section>
<section id="the-bootstrap" class="slide level2">
<h2>The Bootstrap</h2>
<p><span class="math display">\[
\mathbf{ y}, \mathbf{X}\sim \mathbb{P}(y, \mathbf{ x})
\]</span></p>
<p>```python def bootstrap(X): “Return a bootstrap sample from a data set.” n = X.shape[0] ind = np.random.choice(n, n, replace=True) # Sample randomly with replacement. return X[ind, :]</p>
</section>
<section id="bootstrap-and-olympic-marathon-data" class="slide level2">
<h2>Bootstrap and Olympic Marathon Data</h2>
</section>
<section id="linear-fit-1" class="slide level2">
<h2>Linear Fit</h2>
<div class="figure">
<div id="olympic-marathon-bootstrap-polynomial-2-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/ml/olympic-marathon-bootstrap-polynomial-2.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Fit of a 1 degree polynomial (a linear model) to the olympic marathon data.
</aside>
</section>
<section id="cubic-fit-1" class="slide level2">
<h2>Cubic Fit</h2>
<div class="figure">
<div id="olympic-marathon-bootstrap-polynomial-4-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/ml/olympic-marathon-bootstrap-polynomial-4.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Fit of a 3 degree polynomial (a cubic model) to the olympic marathon data.
</aside>
</section>
<section id="th-degree-polynomial-fit-3" class="slide level2">
<h2>9th Degree Polynomial Fit</h2>
<div class="figure">
<div id="olympic-marathon-bootstrap-polynomial-10-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/ml/olympic-marathon-bootstrap-polynomial-10.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Fit of a 9 degree polynomial to the olympic marathon data.
</aside>
</section>
<section id="th-degree-polynomial-fit-4" class="slide level2">
<h2>16th Degree Polynomial Fit</h2>
<div class="figure">
<div id="olympic-marathon-bootstrap-polynomial-17-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/ml/olympic-marathon-bootstrap-polynomial-17.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Fit of a 16 degree polynomial to the olympic marathon data.
</aside>
</section>
<section id="bias-variance-decomposition" class="slide level2">
<h2>Bias Variance Decomposition</h2>
<p>Generalisation error <span class="math display">\[
\mathbb{E}\left[ \left(y- f^*(\mathbf{ x})\right)^2 \right].
\]</span> Decompose as <span class="math display">\[
\mathbb{E}\left[ \left(y- f(\mathbf{ x})\right)^2 \right] = \text{bias}\left[f^*(\mathbf{ x})\right]^2 + \text{variance}\left[f^*(\mathbf{ x})\right] +\sigma^2,
\]</span></p>
</section>
<section id="bias" class="slide level2">
<h2>Bias</h2>
<ul>
<li><p>Given by <span class="math display">\[
\text{bias}\left[f^*(\mathbf{ x})\right] =
\mathbb{E}\left[f^*(\mathbf{ x})\right] * f(\mathbf{ x})
\]</span></p></li>
<li><p>Error due to bias comes from a model that’s too simple.</p></li>
</ul>
</section>
<section id="variance" class="slide level2">
<h2>Variance</h2>
<ul>
<li><p>Given by <span class="math display">\[
\text{variance}\left[f^*(\mathbf{ x})\right] = \mathbb{E}\left[\left(f^*(\mathbf{ x}) - \mathbb{E}\left[f^*(\mathbf{ x})\right]\right)^2\right].
\]</span></p></li>
<li><p>Slight variations in the training set cause changes in the prediction. Error due to variance is error in the model due to an overly complex model.</p></li>
</ul>
</section>
<section id="regularisation" class="slide level2">
<h2>Regularisation</h2>
<p>Linear system, solve:</p>
<p><span class="math display">\[
\boldsymbol{ \Phi}^\top\boldsymbol{ \Phi}\mathbf{ w}= \boldsymbol{ \Phi}^\top\mathbf{ y}
\]</span> But if <span class="math inline">\(\boldsymbol{ \Phi}^\top\boldsymbol{ \Phi}\)</span> then this is not wel posed.</p>
</section>
<section id="tikhonov-regularisation" class="slide level2">
<h2>Tikhonov Regularisation</h2>
<ul>
<li>Updated objective: <span class="math display">\[
L(\mathbf{ w}) = (\mathbf{ y}- \mathbf{ f})^\top(\mathbf{ y}- \mathbf{ f}) + \alpha\lTwoNorm{\mathbf{W}}^2
\]</span></li>
<li>Hessian: <span class="math display">\[
$\boldsymbol{ \Phi}^\top\boldsymbol{ \Phi}$ + \alpha \mathbf{I}
\]</span></li>
</ul>
</section>
<section id="splines-functions-hilbert-kernels" class="slide level2">
<h2>Splines, Functions, Hilbert Kernels</h2>
<ul>
<li>Can also regularise the function <span class="math inline">\(f(\cdot)\)</span> directly.</li>
<li>This approach taken in <em>splines</em> and <span class="citation" data-cites="Wahba:book90">Wahba (1990)</span> and kernels <span class="citation" data-cites="Scholkopf:learning01">Schölkopf and Smola (2001)</span>.</li>
<li>Mathematically more elegant, but algorithmically less flexible and harder to scale.</li>
</ul>
</section>
<section id="training-with-noise" class="slide level2">
<h2>Training with Noise</h2>
<ul>
<li>Other regularisation approaches such as <em>dropout</em> <span class="citation" data-cites="Srivastava:dropout14">(Srivastava et al., 2014)</span></li>
<li>Often perturbing the neural network structure or inputs.</li>
<li>Can have elegant interpretations (see e.g. <span class="citation" data-cites="Bishop:noise95">Bishop (1995)</span>)</li>
<li>Also interpreted as <em>ensemble</em> or <em>Bayesian</em> methods.</li>
</ul>
</section>
<section id="bayesian-interpretation-of-regularisation" class="slide level2">
<h2>Bayesian Interpretation of Regularisation</h2>
<p>Bootstrap Predication and Bayesian Misspecified Models: <a href="https://www.jstor.org/stable/3318894#metadata_info_tab_contents" class="uri">https://www.jstor.org/stable/3318894#metadata_info_tab_contents</a></p>
<p>Edwin Fong and Chris Holmes: On the Marginal Likelihood and Cross Validation <a href="https://arxiv.org/abs/1905.08737" class="uri">https://arxiv.org/abs/1905.08737</a></p>
</section>
<section id="overparameterised-systems" class="slide level2">
<h2>Overparameterised Systems</h2>
<ul>
<li>Neural networks are highly overparameterised.</li>
<li>If we <em>could</em> examine their Hessian at “optimum”
<ul>
<li>Very low (or negative) eigenvalues.</li>
<li>Error function is not sensitive to changes in parameters.</li>
<li>Implies parmeters are <em>badly determined</em></li>
</ul></li>
</ul>
</section>
<section id="whence-generalisation" class="slide level2">
<h2>Whence Generalisation?</h2>
<ul>
<li>Not enough regularisation in our objective functions to explain.</li>
<li>Neural network models are <em>not</em> using traditional generalisation approaches.</li>
<li>The ability of these models to generalise <em>must</em> be coming somehow from the algorithm*</li>
<li>How to explain it and control it is perhaps the most interesting theoretical question for neural networks.</li>
</ul>
</section>
<section id="double-descent" class="slide level2">
<h2>Double Descent</h2>
<div class="figure">
<div id="double-descent-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/ml/double-descent.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
<em>Left</em> traditional perspective on generalisation. There is a sweet spot of operation where the training error is still non-zero. Overfitting occurs when the variance increases. <em>Right</em> The double descent phenomenon, the modern models operate in an interpolation regime where they reconstruct the training data fully, but are well regularised in their interpolations for test data. Figure from <span class="citation" data-cites="Belkin:reconciling19">Belkin et al. (2019)</span>.
</aside>
</section>
<section id="neural-tangent-kernel" class="slide level2">
<h2>Neural Tangent Kernel</h2>
<ul>
<li>Consider very wide neural networks.</li>
<li>Consider particular initialisation.</li>
<li>Deep neural network is regularising with a particular <em>kernel</em>.</li>
<li>This is known as the neural tangent kernel <span class="citation" data-cites="Jacot-ntk18">(Jacot et al., 2018)</span>.</li>
</ul>
</section>
<section id="regularisation-in-optimisation" class="slide level2">
<h2>Regularisation in Optimisation</h2>
<ul>
<li>Gradient flow methods allow us to study nature of optima.</li>
<li>In particular systems, with given initialisations, we can show L1 and L2 norms are minimised.</li>
<li>In other cases the rank of <span class="math inline">\(\mathbf{W}\)</span> is minimised.</li>
<li>Questions remain over the nature of this regularisation in neural networks.</li>
</ul>
</section>
<section id="deep-linear-models" class="slide level2">
<h2>Deep Linear Models</h2>
<p><span class="math display">\[
f(\mathbf{ x}; \mathbf{W}) = \mathbf{W}_4 \mathbf{W}_3 \mathbf{W}_2 \mathbf{W}_1 \mathbf{ x}.
\]</span> {In these models, a gradient flow analysis shows that the model finds solutions where the linear mapping, <span class="math display">\[
\mathbf{W}= \mathbf{W}_4 \mathbf{W}_3 \mathbf{W}_2 \mathbf{W}_1 
\]</span></p>
</section>
<section id="thanks" class="slide level2 scrollable">
<h2 class="scrollable">Thanks!</h2>
<ul>
<li><p>twitter: <a href="https://twitter.com/lawrennd">@lawrennd</a></p></li>
<li><p>podcast: <a href="http://thetalkingmachines.com">The Talking Machines</a></p></li>
<li><p>newspaper: <a href="http://www.theguardian.com/profile/neil-lawrence">Guardian Profile Page</a></p></li>
<li><p>blog posts:</p>
<p><a href="http://inverseprobability.com/2014/07/01/open-data-science">Open Data Science</a></p></li>
</ul>
</section>
<section id="references" class="slide level2 scrollable">
<h2 class="scrollable">References</h2>
<p>bootstrap</p>
<p>David Hogg’s lecture <a href="https://speakerdeck.com/dwhgg/linear-regression-with-huge-numbers-of-parameters" class="uri">https://speakerdeck.com/dwhgg/linear-regression-with-huge-numbers-of-parameters</a></p>
<p>The Deep Bootstrap <a href="https://twitter.com/PreetumNakkiran/status/1318007088321335297?s=20" class="uri">https://twitter.com/PreetumNakkiran/status/1318007088321335297?s=20</a></p>
<p>Aki Vehtari on Leave One Out Uncertainty: <a href="https://arxiv.org/abs/2008.10296" class="uri">https://arxiv.org/abs/2008.10296</a> (check for his references).</p>
<div id="refs" class="references hanging-indent" role="doc-bibliography">
<div id="ref-Belkin:reconciling19">
<p>Belkin, M., Hsu, D., Ma, S., Soumik Mandal, 2019. Reconciling modern machine-learning practice and the classical bias-variance trade-off. Proc. Natl. Acad. Sci. USA 116, 15849–15854.</p>
</div>
<div id="ref-Bishop:noise95">
<p>Bishop, C.M., 1995. Training with noise is equivalent to Tikhonov regularization. Neural Computation 7, 108–116. <a href="https://doi.org/10.1162/neco.1995.7.1.108">https://doi.org/10.1162/neco.1995.7.1.108</a></p>
</div>
<div id="ref-Jacot-ntk18">
<p>Jacot, A., Gabriel, F., Hongler, C., 2018. Neural tangent kernel: Convergence and generalization in neural networks, in: Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., Garnett, R. (Eds.), Advances in Neural Information Processing Systems. Curran Associates, Inc., pp. 8571–8580.</p>
</div>
<div id="ref-Scholkopf:learning01">
<p>Schölkopf, B., Smola, A.J., 2001. Learning with kernels. mit, Cambridge, MA.</p>
</div>
<div id="ref-Srivastava:dropout14">
<p>Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., Salakhutdinov, R., 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research 15, 1929–1958.</p>
</div>
<div id="ref-Wahba:book90">
<p>Wahba, G., 1990. Spline models for observational data, First. ed. SIAM. <a href="https://doi.org/10.1137/1.9781611970128">https://doi.org/10.1137/1.9781611970128</a></p>
</div>
</div>
</section>
    </div>
  </div>

  <script src="reveal.js/lib/js/head.min.js"></script>
  <script src="reveal.js/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Push each slide change to the browser history
        history: true,
        // Transition style
        transition: 'None', // none/fade/slide/convex/concave/zoom
        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // Optional reveal.js plugins
        dependencies: [
          { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true },
          { src: 'reveal.js/plugin/math/math.js', async: true },
          { src: 'reveal.js/plugin/notes/notes.js', async: true }
        ]
      });
    </script>
    </body>
</html>
