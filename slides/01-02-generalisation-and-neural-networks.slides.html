<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="dcterms.date" content="2022-01-25">
  <title>Generalization and Neural Networks</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@3.9.2/css/reveal.css">
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@3.9.2/css/theme/black.css" id="theme">
  <link rel="stylesheet" href="https://inverseprobability.com/assets/css/talks.css"/>
  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'https://unpkg.com/reveal.js@3.9.2/css/print/pdf.css' : 'https://unpkg.com/reveal.js@3.9.2/css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!--[if lt IE 9]>
  <script src="https://unpkg.com/reveal.js@3.9.2/lib/js/html5shiv.js"></script>
  <![endif]-->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_SVG" type="text/javascript"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    TeX: {
         extensions: ["color.js"]
      }
    });
  </script>
  <script src="../assets/js/figure-animate.js"></script>
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Generalization and Neural Networks</h1>
  <p class="author" style="text-align:center"><a href="http://inverseprobability.com">Neil D. Lawrence</a></p>
  <p class="date" style="text-align:center"><time>2022-01-25</time></p>
  <p class="venue" style="text-align:center">LT1, William Gates Building</p>
</section>

<section class="slide level2">

<!-- Do not edit this file locally. -->
<!---->
<!-- Do not edit this file locally. -->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
<!--setupplotcode{import seaborn as sns
sns.set_style('darkgrid')
sns.set_context('paper')
sns.set_palette('colorblind')}-->
</section>
<section id="quadratic-loss-and-linear-system" class="slide level2">
<h2>Quadratic Loss and Linear System</h2>
</section>
<section id="expected-loss" class="slide level2">
<h2>Expected Loss</h2>
<p><span class="math display">\[
R(\mathbf{ w}) = \int L(y, x, \mathbf{ w}) \mathbb{P}(y, x) \text{d}y
\text{d}x.
\]</span></p>
</section>
<section id="sample-based-approximations" class="slide level2">
<h2>Sample Based Approximations</h2>
<ul>
<li><p>Sample based approximation: replace true expectation with sum over samples. <span class="math display">\[
\int f(z) p(z) \text{d}z\approx \frac{1}{s}\sum_{i=1}^s f(z_i).
\]</span></p></li>
<li><p>Allows us to approximate true integral with a sum <span class="math display">\[
R(\mathbf{ w}) \approx \frac{1}{n}\sum_{i=1}^{n} L(y_i, x_i, \mathbf{ w}).
\]</span></p></li>
</ul>
</section>
<section id="quadratic-loss" class="slide level2">
<h2>Quadratic Loss</h2>
<p><span class="math display">\[
L(\mathbf{ w}) = \sum_{i=1}^n\left(y_i - f(\mathbf{ x}_i, \mathbf{ w})\right)^2
\]</span></p>
</section>
<section id="linear-model" class="slide level2">
<h2>Linear Model</h2>
<p><span class="math display">\[
L(\mathbf{ w}) = \sum_{i=1}^n\left(y_i - \mathbf{ w}^\top \mathbf{ x}_i\right)^2
\]</span></p>
</section>
<section id="bracket-expansion" class="slide level2">
<h2>Bracket Expansion</h2>
<p><span class="math display">\[
\begin{align*}
  L(\mathbf{ w},\sigma^2)  = &amp; \sum
_{i=1}^{n}y_i^{2}- 2\sum
_{i=1}^{n}y_i\mathbf{ w}^{\top}\mathbf{ x}_i\\&amp;+\sum
_{i=1}^{n}\mathbf{ w}^{\top}\mathbf{ x}_i\mathbf{ x}_i^{\top}\mathbf{ w}\\
    = &amp; \sum_{i=1}^{n}y_i^{2}-
2 \mathbf{ w}^\top\sum_{i=1}^{n}\mathbf{ x}_iy_i\\&amp;+
\mathbf{ w}^{\top}\left[\sum
_{i=1}^{n}\mathbf{ x}_i\mathbf{ x}_i^{\top}\right]\mathbf{ w}.
\end{align*}
\]</span></p>
<!-- SECTION Design Matrix -->
</section>
<section id="design-matrix" class="slide level2">
<h2>Design Matrix</h2>
<p><span class="math display">\[
\boldsymbol{ \Phi}
= \begin{bmatrix} 
\mathbf{ x}_1^\top \\\ 
\mathbf{ x}_2^\top \\\ 
\vdots \\\
\mathbf{ x}_n^\top
\end{bmatrix} = \begin{bmatrix}
1 &amp; x_1 \\\
1 &amp; x_2 \\\
\vdots
&amp; \vdots \\\
1 &amp; x_n
\end{bmatrix}
\]</span></p>
</section>
<section id="writing-the-objective-with-linear-algebra" class="slide level2">
<h2>Writing the Objective with Linear Algebra</h2>
<p><span class="math display">\[
L(\mathbf{ w}) = \sum_{i=1}^n(y_i - f(\mathbf{ x}_i; \mathbf{ w}))^2,
\]</span></p>
<p><span class="math display">\[
f(\mathbf{ x}_i; \mathbf{ w}) = \mathbf{ x}_i^\top \mathbf{ w}.
\]</span></p>
</section>
<section id="stacking-vectors" class="slide level2">
<h2>Stacking Vectors</h2>
<p><span class="math display">\[
\mathbf{ f}= \begin{bmatrix}f_1\\f_2\\
\vdots \\ f_n\end{bmatrix}.
\]</span></p>
<p><span class="math display">\[
L(\mathbf{ w}) = (\mathbf{ y}- \mathbf{ f})^\top(\mathbf{ y}- \mathbf{ f})
\]</span></p>
</section>
<section id="section" class="slide level2">
<h2></h2>
<p><span class="math display">\[
\mathbf{ f}= \boldsymbol{ \Phi}\mathbf{ w}.
\]</span></p>
<p><span class="math display">\[
L(\mathbf{ w}) = (\mathbf{ y}- \mathbf{ f})^\top(\mathbf{ y}- \mathbf{ f})
\]</span></p>
<!-- SECTION Objective Optimisation -->
</section>
<section id="objective-optimisation" class="slide level2">
<h2>Objective Optimisation</h2>
</section>
<section id="multivariate-derivatives" class="slide level2">
<h2>Multivariate Derivatives</h2>
<ul>
<li>We will need some multivariate calculus.</li>
<li>For now some simple multivariate differentiation: <span class="math display">\[\frac{\text{d}{\mathbf{a}^{\top}}{\mathbf{ w}}}{\text{d}\mathbf{ w}}=\mathbf{a}\]</span> and <span class="math display">\[\frac{\mathbf{ w}^{\top}\mathbf{A}\mathbf{ w}}{\text{d}\mathbf{ w}}=\left(\mathbf{A}+\mathbf{A}^{\top}\right)\mathbf{ w}\]</span> or if <span class="math inline">\(\mathbf{A}\)</span> is symmetric (<em>i.e.</em> <span class="math inline">\(\mathbf{A}=\mathbf{A}^{\top}\)</span>) <span class="math display">\[\frac{\text{d}\mathbf{ w}^{\top}\mathbf{A}\mathbf{ w}}{\text{d}\mathbf{ w}}=2\mathbf{A}\mathbf{ w}.\]</span></li>
</ul>
</section>
<section id="differentiate-the-objective" class="slide level2">
<h2>Differentiate the Objective</h2>
<div style="text-align:left">
Differentiating with respect to the vector <span class="math inline">\(\mathbf{ w}\)</span> we obtain
</div>
<p><span class="math display">\[
\frac{\partial L\left(\mathbf{ w},\sigma^2 \right)}{\partial
\mathbf{ w}}=\frac{1}{\sigma^2} \sum _{i=1}^{n}\mathbf{ x}_i y_i-\frac{1}{\sigma^2}
\left[\sum _{i=1}^{n}\mathbf{ x}_i\mathbf{ x}_i^{\top}\right]\mathbf{ w}
\]</span> Leading to <span class="math display">\[
\mathbf{ w}^{*}=\left[\sum
_{i=1}^{n}\mathbf{ x}_i\mathbf{ x}_i^{\top}\right]^{-1}\sum
_{i=1}^{n}\mathbf{ x}_iy_i,
\]</span></p>
</section>
<section id="differentiate-the-objective-1" class="slide level2">
<h2>Differentiate the Objective</h2>
<p>Rewrite in matrix notation: <span class="math display">\[
\sum_{i=1}^{n}\mathbf{ x}_i\mathbf{ x}_i^\top = \boldsymbol{ \Phi}^\top \boldsymbol{ \Phi}
\]</span> <span class="math display">\[
\sum_{i=1}^{n}\mathbf{ x}_iy_i = \boldsymbol{ \Phi}^\top \mathbf{ y}
\]</span></p>
<!-- SECTION Update Equation for Global Optimum -->
</section>
<section id="update-equation-for-global-optimum" class="slide level2">
<h2>Update Equation for Global Optimum</h2>
</section>
<section id="update-equations" class="slide level2">
<h2>Update Equations</h2>
<ul>
<li>Solve the matrix equation for <span class="math inline">\(\mathbf{ w}\)</span>. <span class="math display">\[\boldsymbol{ \Phi}^\top \boldsymbol{ \Phi}\mathbf{ w}=  \boldsymbol{ \Phi}^\top \mathbf{ y}\]</span></li>
</ul>
</section>
<section id="hessian-matrix" class="slide level2">
<h2>Hessian Matrix</h2>
<p><span class="math display">\[
\frac{\text{d}^2}{\text{d}\mathbf{ w}\text{d}\mathbf{ w}^\top} L(\mathbf{ w}) = 2\boldsymbol{ \Phi}^\top\boldsymbol{ \Phi}.
\]</span></p>
</section>
<section id="eigendecomposition-of-hessian" class="slide level2">
<h2>Eigendecomposition of Hessian</h2>
<p><span class="math display">\[
\mathbf{A}\mathbf{u} = \lambda\mathbf{u}
\]</span></p>
<p><span class="math display">\[
\boldsymbol{ \Phi}^\top\boldsymbol{ \Phi}\mathbf{u}_i = \lambda_i \mathbf{u}_i
\]</span></p>
<!-- SECTION Nigeria NMIS Data -->
</section>
<section id="nigeria-nmis-data" class="slide level2">
<h2>Nigeria NMIS Data</h2>
</section>
<section id="nigeria-nmis-data-notebook" class="slide level2">
<h2>Nigeria NMIS Data: Notebook</h2>
<div class="figure">
<div id="nigerian-health-facilities-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.com/deepnn/./slides/diagrams//ml/nigerian-health-facilities.png" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Location of the over thirty-four thousand health facilities registered in the NMIS data across Nigeria. Each facility plotted according to its latitude and longitude.
</aside>
</section>
<section id="multivariate-regression-on-nigeria-nmis-data" class="slide level2">
<h2>Multivariate Regression on Nigeria NMIS Data</h2>
<ul>
<li>Regress from features <code>num_nurses_fulltime</code>, <code>num_nursemidwives_fulltime</code>, <code>latitude</code> and <code>longitude</code> to <code>num_doctors_fulltime</code>.</li>
</ul>
</section>
<section id="residuals" class="slide level2">
<h2>Residuals</h2>
<div class="figure">
<div id="nigeria-nmis-num-doctors-residuals-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.com/deepnn/./slides/diagrams//ml/nigeria-nmis-num-doctors-residuals.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Residual values for the ratings from the prediction of the movie rating given the data from the film.
</aside>
</section>
<section id="shallow-and-deep-learning" class="slide level2">
<h2>Shallow and Deep Learning</h2>
</section>
<section id="deep-neural-network" class="slide level2">
<h2>Deep Neural Network</h2>
<object class="svgplot " data="https://mlatcl.github.com/deepnn/./slides/diagrams//deepgp/deep-nn1.svg" width="50%" style=" ">
</object>
</section>
<section id="deep-neural-network-1" class="slide level2">
<h2>Deep Neural Network</h2>
<object class="svgplot " data="https://mlatcl.github.com/deepnn/./slides/diagrams//deepgp/deep-nn2.svg" width="50%" style=" ">
</object>
</section>
<section id="mathematically" class="slide level2">
<h2>Mathematically</h2>
<p><span class="math display">\[
\begin{align*}
    \mathbf{ h}_{1} &amp;= \phi\left(\mathbf{W}_1 \mathbf{ x}\right)\\
    \mathbf{ h}_{2} &amp;=  \phi\left(\mathbf{W}_2\mathbf{ h}_{1}\right)\\
    \mathbf{ h}_{3} &amp;= \phi\left(\mathbf{W}_3 \mathbf{ h}_{2}\right)\\
    f&amp;= \mathbf{ w}_4 ^\top\mathbf{ h}_{3}
\end{align*}
\]</span></p>
</section>
<section id="neural-network-prediction-function" class="slide level2">
<h2>Neural Network Prediction Function</h2>
<p><span class="math display">\[
 f(\mathbf{ x}; \mathbf{W})  =  \mathbf{ w}_4 ^\top\phi\left(\mathbf{W}_3 \phi\left(\mathbf{W}_2\phi\left(\mathbf{W}_1 \mathbf{ x}\right)\right)\right)
\]</span></p>
</section>
<section id="basis-function-models" class="slide level2">
<h2>Basis Function Models</h2>
</section>
<section id="polynomial-basis" class="slide level2">
<h2>Polynomial Basis</h2>
<p><span class="math display">\[
\phi_j(x) = x^j
\]</span></p>
<script>
showDivs(0, 'polynomial_basis');
</script>
<p><small></small> <input id="range-polynomial_basis" type="range" min="0" max="4" value="0" onchange="setDivs('polynomial_basis')" oninput="setDivs('polynomial_basis')"> <button onclick="plusDivs(-1, 'polynomial_basis')">❮</button> <button onclick="plusDivs(1, 'polynomial_basis')">❯</button></p>
<div class="polynomial_basis" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.com/deepnn/./slides/diagrams//ml/polynomial_basis000.svg" width="80%" style=" ">
</object>
</div>
<div class="polynomial_basis" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.com/deepnn/./slides/diagrams//ml/polynomial_basis001.svg" width="80%" style=" ">
</object>
</div>
<div class="polynomial_basis" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.com/deepnn/./slides/diagrams//ml/polynomial_basis002.svg" width="80%" style=" ">
</object>
</div>
<div class="polynomial_basis" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.com/deepnn/./slides/diagrams//ml/polynomial_basis003.svg" width="80%" style=" ">
</object>
</div>
<div class="polynomial_basis" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.com/deepnn/./slides/diagrams//ml/polynomial_basis004.svg" width="80%" style=" ">
</object>
</div>
</section>
<section id="functions-derived-from-polynomial-basis" class="slide level2">
<h2>Functions Derived from Polynomial Basis</h2>
<p><span class="math display">\[
f(x) = {\color{cyan}{w_0}} + {\color{green}{w_1 x}} + {\color{yellow}{w_2 x^2}} + {\color{magenta}{w_3 x^3}} + {\color{red}{w_4 x^4}}
\]</span></p>
<script>
showDivs(0, 'polynomial_function');
</script>
<p><small></small> <input id="range-polynomial_function" type="range" min="0" max="3" value="0" onchange="setDivs('polynomial_function')" oninput="setDivs('polynomial_function')"> <button onclick="plusDivs(-1, 'polynomial_function')">❮</button> <button onclick="plusDivs(1, 'polynomial_function')">❯</button></p>
<div class="polynomial_function" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.com/deepnn/./slides/diagrams//ml/polynomial_function000.svg" width="80%" style=" ">
</object>
</div>
<div class="polynomial_function" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.com/deepnn/./slides/diagrams//ml/polynomial_function001.svg" width="80%" style=" ">
</object>
</div>
<div class="polynomial_function" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.com/deepnn/./slides/diagrams//ml/polynomial_function002.svg" width="80%" style=" ">
</object>
</div>
<ul>
<li>_2 ^2 + _3 ^3 + _4 ^4 $$ are <em>linear</em> in the parameters, <span class="math inline">\(\mathbf{ w}\)</span>, but <em>non-linear</em> in the input <span class="math inline">\(x^3\)</span>. Here we are showing a polynomial basis for a 1-dimensional input, <span class="math inline">\(x\)</span>, but basis functions can also be constructed for multidimensional inputs, <span class="math inline">\(\mathbf{ x}\)</span>.}</li>
</ul>
</section>
<section id="olympic-marathon-data" class="slide level2">
<h2>Olympic Marathon Data</h2>
<table>
<tr>
<td width="70%">
<ul>
<li>Gold medal times for Olympic Marathon since 1896.</li>
<li>Marathons before 1924 didn’t have a standardised distance.</li>
<li>Present results using pace per km.</li>
<li>In 1904 Marathon was badly organised leading to very slow times.</li>
</ul>
</td>
<td width="30%">
<div class="centered centered" style="">
<img class="" src="https://mlatcl.github.com/deepnn/./slides/diagrams//Stephen_Kiprotich.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<small>Image from Wikimedia Commons <a href="http://bit.ly/16kMKHQ" class="uri">http://bit.ly/16kMKHQ</a></small>
</td>
</tr>
</table>
</section>
<section id="olympic-marathon-data-1" class="slide level2">
<h2>Olympic Marathon Data</h2>
<div class="figure">
<div id="olympic-marathon-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.com/deepnn/./slides/diagrams//datasets/olympic-marathon.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Olympic marathon pace times since 1896.
</aside>
</section>
<section id="polynomial-fits-to-olympic-marthon-data" class="slide level2">
<h2>Polynomial Fits to Olympic Marthon Data</h2>
<ul>
<li>Fit linear model with polynomial basis to marathon data.</li>
<li>Try different numbers of basis functions (different degress of polynomial).</li>
<li>Check the quality of fit.</li>
</ul>
</section>
<section id="linear-fit" class="slide level2">
<h2>Linear Fit</h2>
<p><span class="math display">\[f(x, \mathbf{ w}) = w_0 + w_1x\]</span></p>
<div class="figure">
<div id="olympic-marathon-polynomial-2-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.com/deepnn/./slides/diagrams//ml/olympic-marathon-polynomial-2.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Fit of a 1 degree polynomial (a linear model) to the olympic marathon data.
</aside>
</section>
<section id="cubic-fit" class="slide level2">
<h2>Cubic Fit</h2>
<p><span class="math display">\[f(x, \mathbf{ w}) = w_0 + w_1 x+ w_2 x^2 + w_3 x^3\]</span></p>
<div class="figure">
<div id="olympic-marathon-polynomial-4-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.com/deepnn/./slides/diagrams//ml/olympic-marathon-polynomial-4.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Fit of a 3 degree polynomial (a cubic model) to the olympic marathon data.
</aside>
</section>
<section id="th-degree-polynomial-fit" class="slide level2">
<h2>9th Degree Polynomial Fit</h2>
<p><span class="math display">\[f(x, \mathbf{ w}) = w_0 + w_1 x+ w_2 x^2 + \dots + w_9 x^9\]</span></p>
<div class="figure">
<div id="olympic-marathon-polynomial-10-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.com/deepnn/./slides/diagrams//ml/olympic-marathon-polynomial-10.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Fit of a 9 degree polynomial to the olympic marathon data.
</aside>
</section>
<section id="th-degree-polynomial-fit-1" class="slide level2">
<h2>16th Degree Polynomial Fit</h2>
<p><span class="math display">\[f(x, \mathbf{ w}) = w_0 + w_1 x+ w_2 x^2 + \dots + w_{16} x^{16}\]</span></p>
<div class="figure">
<div id="olympic-marathon-polynomial-17-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.com/deepnn/./slides/diagrams//ml/olympic-marathon-polynomial-17.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Fit of a 16 degree polynomial to the olympic marathon data.
</aside>
</section>
<section id="th-degree-polynomial-fit-2" class="slide level2">
<h2>26th Degree Polynomial Fit</h2>
<p><span class="math display">\[f(x, \mathbf{ w}) = w_0 + w_1 x+ w_2 x^2 + \dots + w_{26} x^{26}\]</span></p>
<div class="figure">
<div id="olympic-marathon-polynomial-27-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.com/deepnn/./slides/diagrams//ml/olympic-marathon-polynomial-27.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Fit of a 26 degree polynomial to the olympic marathon data.
</aside>
</section>
<section id="the-bootstrap" class="slide level2">
<h2>The Bootstrap</h2>
<p><span class="math display">\[
\mathbf{ y}, \mathbf{X}\sim \mathbb{P}(y, \mathbf{ x})
\]</span></p>
</section>
<section id="resample-dataset" class="slide level2">
<h2>Resample Dataset</h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bootstrap(X):</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;Return a bootstrap sample from a data set.&quot;</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> X.shape[<span class="dv">0</span>]</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    ind <span class="op">=</span> np.random.choice(n, n, replace<span class="op">=</span><span class="va">True</span>) <span class="co"># Sample randomly with replacement.</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> X[ind, :]</span></code></pre></div>
</section>
<section id="bootstrap-and-olympic-marathon-data" class="slide level2">
<h2>Bootstrap and Olympic Marathon Data</h2>
</section>
<section id="linear-fit-1" class="slide level2">
<h2>Linear Fit</h2>
<p><span class="math display">\[f(x, \mathbf{ w}) = w_0 + w_1 x\]</span></p>
<div class="figure">
<div id="olympic-marathon-bootstrap-polynomial-2-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.com/deepnn/./slides/diagrams//ml/olympic-marathon-bootstrap-polynomial-2.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Fit of a 1 degree polynomial (a linear model) to the olympic marathon data.
</aside>
</section>
<section id="cubic-fit-1" class="slide level2">
<h2>Cubic Fit</h2>
<p><span class="math display">\[f(x, \mathbf{ w}) = w_0 + w_1 x+ w_2 x^2 + w_{3} x^3\]</span></p>
<div class="figure">
<div id="olympic-marathon-bootstrap-polynomial-4-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.com/deepnn/./slides/diagrams//ml/olympic-marathon-bootstrap-polynomial-4.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Fit of a 3 degree polynomial (a cubic model) to the olympic marathon data.
</aside>
</section>
<section id="th-degree-polynomial-fit-3" class="slide level2">
<h2>9th Degree Polynomial Fit</h2>
<p><span class="math display">\[f(x, \mathbf{ w}) = w_0 + w_1 x+ w_2 x^2 + \dots + w_{9} x^{9}\]</span></p>
<div class="figure">
<div id="olympic-marathon-bootstrap-polynomial-10-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.com/deepnn/./slides/diagrams//ml/olympic-marathon-bootstrap-polynomial-10.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Fit of a 9 degree polynomial to the olympic marathon data.
</aside>
</section>
<section id="th-degree-polynomial-fit-4" class="slide level2">
<h2>16th Degree Polynomial Fit</h2>
<p><span class="math display">\[f(x, \mathbf{ w}) = w_0 + w_1 x+ w_2 x^2 + \dots + w_{16} x^{16}\]</span></p>
<div class="figure">
<div id="olympic-marathon-bootstrap-polynomial-17-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.com/deepnn/./slides/diagrams//ml/olympic-marathon-bootstrap-polynomial-17.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Fit of a 16 degree polynomial to the olympic marathon data.
</aside>
</section>
<section id="bias-variance-decomposition" class="slide level2">
<h2>Bias Variance Decomposition</h2>
<p>Generalisation error <span class="math display">\[\begin{align*}
R(\mathbf{ w}) = &amp; \int \left(y- f^*(\mathbf{ x})\right)^2 \mathbb{P}(y, \mathbf{ x}) \text{d}y\text{d}\mathbf{ x}\\
&amp; \triangleq &amp; \mathbb{E}\left[ \left(y- f^*(\mathbf{ x})\right)^2 \right].
\end{align*}\]</span></p>
</section>
<section id="decompose" class="slide level2">
<h2>Decompose</h2>
<p>Decompose as <span class="math display">\[
\begin{align*}
\mathbb{E}\left[ \left(y- f(\mathbf{ x})\right)^2 \right] = &amp; \text{bias}\left[f^*(\mathbf{ x})\right]^2 \\
&amp; + \text{variance}\left[f^*(\mathbf{ x})\right] \\ \\ &amp;+\sigma^2,
\end{align*}
\]</span></p>
</section>
<section id="bias" class="slide level2">
<h2>Bias</h2>
<ul>
<li><p>Given by <span class="math display">\[
\text{bias}\left[f^*(\mathbf{ x})\right] =
\mathbb{E}\left[f^*(\mathbf{ x})\right] - f(\mathbf{ x})
\]</span></p></li>
<li><p>Error due to bias comes from a model that’s too simple.</p></li>
</ul>
</section>
<section id="variance" class="slide level2">
<h2>Variance</h2>
<ul>
<li><p>Given by <span class="math display">\[
\text{variance}\left[f^*(\mathbf{ x})\right] = \mathbb{E}\left[\left(f^*(\mathbf{ x}) - \mathbb{E}\left[f^*(\mathbf{ x})\right]\right)^2\right].
\]</span></p></li>
<li><p>Slight variations in the training set cause changes in the prediction. Error due to variance is error in the model due to an overly complex model.</p></li>
</ul>
</section>
<section id="regularisation" class="slide level2">
<h2>Regularisation</h2>
<p>Linear system, solve:</p>
<p><span class="math display">\[
\boldsymbol{ \Phi}^\top\boldsymbol{ \Phi}\mathbf{ w}= \boldsymbol{ \Phi}^\top\mathbf{ y}
\]</span> But if <span class="math inline">\(\boldsymbol{ \Phi}^\top\boldsymbol{ \Phi}\)</span> then this is not well posed.</p>
</section>
<section id="tikhonov-regularisation" class="slide level2">
<h2>Tikhonov Regularisation</h2>
<ul>
<li>Updated objective: <span class="math display">\[
L(\mathbf{ w}) = (\mathbf{ y}- \mathbf{ f})^\top(\mathbf{ y}- \mathbf{ f}) + \alpha\left\Vert \mathbf{W} \right\Vert_2^2
\]</span></li>
<li>Hessian: <span class="math display">\[
\boldsymbol{ \Phi}^\top\boldsymbol{ \Phi}+ \alpha \mathbf{I}
\]</span></li>
</ul>
</section>
<section id="splines-functions-hilbert-kernels" class="slide level2">
<h2>Splines, Functions, Hilbert Kernels</h2>
<ul>
<li>Can also regularise the function <span class="math inline">\(f(\cdot)\)</span> directly.</li>
<li>This approach taken in <em>splines</em> and <span class="citation" data-cites="Wahba:book90">Wahba (1990)</span> and kernels <span class="citation" data-cites="Scholkopf:learning01">Schölkopf and Smola (2001)</span>.</li>
<li>Mathematically more elegant, but algorithmically less flexible and harder to scale.</li>
</ul>
</section>
<section id="training-with-noise" class="slide level2">
<h2>Training with Noise</h2>
<ul>
<li>Other regularisation approaches such as <em>dropout</em> <span class="citation" data-cites="Srivastava:dropout14">(<strong>Srivastava:dropout14?</strong>)</span></li>
<li>Often perturbing the neural network structure or inputs.</li>
<li>Can have elegant interpretations (see e.g. <span class="citation" data-cites="Bishop:noise95">Bishop (1995)</span>)</li>
<li>Also interpreted as <em>ensemble</em> or <em>Bayesian</em> methods.</li>
</ul>
<!--include{_ml/includes/bayesian-interpretation-of-regularisation.md}-->
</section>
<section id="overparameterised-systems" class="slide level2">
<h2>Overparameterised Systems</h2>
<ul>
<li>Neural networks are highly overparameterised.</li>
<li>If we <em>could</em> examine their Hessian at “optimum”
<ul>
<li>Very low (or negative) eigenvalues.</li>
<li>Error function is not sensitive to changes in parameters.</li>
<li>Implies parmeters are <em>badly determined</em></li>
</ul></li>
</ul>
</section>
<section id="whence-generalisation" class="slide level2">
<h2>Whence Generalisation?</h2>
<ul>
<li>Not enough regularisation in our objective functions to explain.</li>
<li>Neural network models are <em>not</em> using traditional generalisation approaches.</li>
<li>The ability of these models to generalise <em>must</em> be coming somehow from the algorithm*</li>
<li>How to explain it and control it is perhaps the most interesting theoretical question for neural networks.</li>
</ul>
</section>
<section id="double-descent" class="slide level2">
<h2>Double Descent</h2>
<div class="figure">
<div id="double-descent-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.com/deepnn/./slides/diagrams//ml/double-descent.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
<em>Left</em> traditional perspective on generalisation. There is a sweet spot of operation where the training error is still non-zero. Overfitting occurs when the variance increases. <em>Right</em> The double descent phenomenon, the modern models operate in an interpolation regime where they reconstruct the training data fully, but are well regularised in their interpolations for test data. Figure from <span class="citation" data-cites="Belkin:reconciling19">Belkin et al. (2019)</span>.
</aside>
</section>
<section id="neural-tangent-kernel" class="slide level2">
<h2>Neural Tangent Kernel</h2>
<ul>
<li>Consider very wide neural networks.</li>
<li>Consider particular initialisation.</li>
<li>Deep neural network is regularising with a particular <em>kernel</em>.</li>
<li>This is known as the neural tangent kernel <span class="citation" data-cites="Jacot-ntk18">(Jacot et al., 2018)</span>.</li>
</ul>
</section>
<section id="regularisation-in-optimisation" class="slide level2">
<h2>Regularisation in Optimisation</h2>
<ul>
<li>Gradient flow methods allow us to study nature of optima.</li>
<li>In particular systems, with given initialisations, we can show L1 and L2 norms are minimised.</li>
<li>In other cases the rank of <span class="math inline">\(\mathbf{W}\)</span> is minimised.</li>
<li>Questions remain over the nature of this regularisation in neural networks.</li>
</ul>
</section>
<section id="deep-linear-models" class="slide level2">
<h2>Deep Linear Models</h2>
<p><span class="math display">\[
f(\mathbf{ x}; \mathbf{W}) = \mathbf{W}_4 \mathbf{W}_3 \mathbf{W}_2 \mathbf{W}_1 \mathbf{ x}.
\]</span> {In these models, a gradient flow analysis shows that the model finds solutions where the linear mapping, <span class="math display">\[
\mathbf{W}= \mathbf{W}_4 \mathbf{W}_3 \mathbf{W}_2 \mathbf{W}_1 
\]</span></p>
</section>
<section id="thanks" class="slide level2 scrollable">
<h2 class="scrollable">Thanks!</h2>
<ul>
<li><p>twitter: <a href="https://twitter.com/lawrennd">@lawrennd</a></p></li>
<li><p>podcast: <a href="http://thetalkingmachines.com">The Talking Machines</a></p></li>
<li><p>newspaper: <a href="http://www.theguardian.com/profile/neil-lawrence">Guardian Profile Page</a></p></li>
<li><p>blog posts:</p>
<p><a href="http://inverseprobability.com/2014/07/01/open-data-science">Open Data Science</a></p></li>
</ul>
</section>
<section id="references" class="slide level2 scrollable">
<h2 class="scrollable">References</h2>
<p>bootstrap</p>
<p>David Hogg’s lecture <a href="https://speakerdeck.com/dwhgg/linear-regression-with-huge-numbers-of-parameters" class="uri">https://speakerdeck.com/dwhgg/linear-regression-with-huge-numbers-of-parameters</a></p>
<p>The Deep Bootstrap <a href="https://twitter.com/PreetumNakkiran/status/1318007088321335297?s=20" class="uri">https://twitter.com/PreetumNakkiran/status/1318007088321335297?s=20</a></p>
<p>Aki Vehtari on Leave One Out Uncertainty: <a href="https://arxiv.org/abs/2008.10296" class="uri">https://arxiv.org/abs/2008.10296</a> (check for his references).</p>
<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-Belkin:reconciling19" class="csl-entry" role="doc-biblioentry">
Belkin, M., Hsu, D., Ma, S., Soumik Mandal, and, 2019. Reconciling modern machine-learning practice and the classical bias-variance trade-off. Proc. Natl. Acad. Sci. USA 116, 15849–15854.
</div>
<div id="ref-Bishop:noise95" class="csl-entry" role="doc-biblioentry">
Bishop, C.M., 1995. Training with noise is equivalent to <span>T</span>ikhonov regularization. Neural Computation 7, 108–116. <a href="https://doi.org/10.1162/neco.1995.7.1.108">https://doi.org/10.1162/neco.1995.7.1.108</a>
</div>
<div id="ref-Jacot-ntk18" class="csl-entry" role="doc-biblioentry">
Jacot, A., Gabriel, F., Hongler, C., 2018. Neural tangent kernel: Convergence and generalization in neural networks, in: Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., Garnett, R. (Eds.), Advances in Neural Information Processing Systems. Curran Associates, Inc., pp. 8571–8580.
</div>
<div id="ref-Scholkopf:learning01" class="csl-entry" role="doc-biblioentry">
Schölkopf, B., Smola, A.J., 2001. Learning with kernels. mit, Cambridge, MA.
</div>
<div id="ref-Wahba:book90" class="csl-entry" role="doc-biblioentry">
Wahba, G., 1990. Spline models for observational data, First. ed. SIAM. <a href="https://doi.org/10.1137/1.9781611970128">https://doi.org/10.1137/1.9781611970128</a>
</div>
</div>
</section>
    </div>
  </div>

  <script src="https://unpkg.com/reveal.js@3.9.2/lib/js/head.min.js"></script>
  <script src="https://unpkg.com/reveal.js@3.9.2/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Push each slide change to the browser history
        history: true,
        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // Optional reveal.js plugins
        dependencies: [
          { src: 'https://unpkg.com/reveal.js@3.9.2/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'https://unpkg.com/reveal.js@3.9.2/plugin/zoom-js/zoom.js', async: true },
          { src: 'https://unpkg.com/reveal.js@3.9.2/plugin/math/math.js', async: true },
          { src: 'https://unpkg.com/reveal.js@3.9.2/plugin/notes/notes.js', async: true }
        ]
      });
    </script>
    </body>
</html>
