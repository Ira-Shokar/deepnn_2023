<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="dcterms.date" content="2022-01-25">
  <title>Generalization and Neural Networks</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@3.9.2/css/reveal.css">
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@3.9.2/css/theme/black.css" id="theme">
  <link rel="stylesheet" href="https://inverseprobability.com/assets/css/talks.css"/>
  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'https://unpkg.com/reveal.js@3.9.2/css/print/pdf.css' : 'https://unpkg.com/reveal.js@3.9.2/css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!--[if lt IE 9]>
  <script src="https://unpkg.com/reveal.js@3.9.2/lib/js/html5shiv.js"></script>
  <![endif]-->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_SVG" type="text/javascript"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    TeX: {
         extensions: ["color.js"]
      }
    });
  </script>
  <script src="../assets/js/figure-animate.js"></script>
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Generalization and Neural Networks</h1>
  <p class="author" style="text-align:center"><a href="http://inverseprobability.com">Neil D. Lawrence</a></p>
  <p class="date" style="text-align:center"><time>2022-01-25</time></p>
  <p class="venue" style="text-align:center">LT1, William Gates Building</p>
</section>

<section class="slide level2">

<!-- Do not edit this file locally. -->
<!---->
<!-- Do not edit this file locally. -->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
<!--setupplotcode{import seaborn as sns
sns.set_style('darkgrid')
sns.set_context('paper')
sns.set_palette('colorblind')}-->
</section>
<section id="quadratic-loss-and-linear-system" class="slide level2">
<h2>Quadratic Loss and Linear System</h2>
</section>
<section id="expected-loss" class="slide level2">
<h2>Expected Loss</h2>
<p><span class="math display">\[
R(\mathbf{ w}) = \int L(y, x, \mathbf{ w}) \mathbb{P}(y, x) \text{d}y
\text{d}x.
\]</span></p>
</section>
<section id="sample-based-approximations" class="slide level2">
<h2>Sample Based Approximations</h2>
<ul>
<li><p>Sample based approximation: replace true expectation with sum over samples. <span class="math display">\[
\int f(z) p(z) \text{d}z\approx \frac{1}{s}\sum_{i=1}^s f(z_i).
\]</span></p></li>
<li><p>Allows us to approximate true integral with a sum <span class="math display">\[
R(\mathbf{ w}) \approx \frac{1}{n}\sum_{i=1}^{n} L(y_i, x_i, \mathbf{ w}).
\]</span></p></li>
</ul>
</section>
<section id="olympic-marathon-data" class="slide level2">
<h2>Olympic Marathon Data</h2>
<table>
<tr>
<td width="70%">
<ul>
<li>Gold medal times for Olympic Marathon since 1896.</li>
<li>Marathons before 1924 didn’t have a standardised distance.</li>
<li>Present results using pace per km.</li>
<li>In 1904 Marathon was badly organised leading to very slow times.</li>
</ul>
</td>
<td width="30%">
<div class="centered centered" style="">
<img class="" src="https://mlatcl.github.io/deepnn/./slides/diagrams//Stephen_Kiprotich.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<small>Image from Wikimedia Commons <a href="http://bit.ly/16kMKHQ" class="uri">http://bit.ly/16kMKHQ</a></small>
</td>
</tr>
</table>
</section>
<section id="olympic-marathon-data-1" class="slide level2">
<h2>Olympic Marathon Data</h2>
<div class="figure">
<div id="olympic-marathon-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//datasets/olympic-marathon.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Olympic marathon pace times since 1896.
</aside>
</section>
<section id="polynomial-fits-to-olympic-marthon-data" class="slide level2">
<h2>Polynomial Fits to Olympic Marthon Data</h2>
<ul>
<li>Fit linear model with polynomial basis to marathon data.</li>
<li>Try different numbers of basis functions (different degress of polynomial).</li>
<li>Check the quality of fit.</li>
</ul>
</section>
<section id="linear-fit" class="slide level2">
<h2>Linear Fit</h2>
<p><span class="math display">\[f(x, \mathbf{ w}) = w_0 + w_1x\]</span></p>
<div class="figure">
<div id="olympic-marathon-polynomial-2-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//ml/olympic-marathon-polynomial-2.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Fit of a 1 degree polynomial (a linear model) to the olympic marathon data.
</aside>
</section>
<section id="cubic-fit" class="slide level2">
<h2>Cubic Fit</h2>
<p><span class="math display">\[f(x, \mathbf{ w}) = w_0 + w_1 x+ w_2 x^2 + w_3 x^3\]</span></p>
<div class="figure">
<div id="olympic-marathon-polynomial-4-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//ml/olympic-marathon-polynomial-4.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Fit of a 3 degree polynomial (a cubic model) to the olympic marathon data.
</aside>
</section>
<section id="th-degree-polynomial-fit" class="slide level2">
<h2>9th Degree Polynomial Fit</h2>
<p><span class="math display">\[f(x, \mathbf{ w}) = w_0 + w_1 x+ w_2 x^2 + \dots + w_9 x^9\]</span></p>
<div class="figure">
<div id="olympic-marathon-polynomial-10-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//ml/olympic-marathon-polynomial-10.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Fit of a 9 degree polynomial to the olympic marathon data.
</aside>
</section>
<section id="th-degree-polynomial-fit-1" class="slide level2">
<h2>16th Degree Polynomial Fit</h2>
<p><span class="math display">\[f(x, \mathbf{ w}) = w_0 + w_1 x+ w_2 x^2 + \dots + w_{16} x^{16}\]</span></p>
<div class="figure">
<div id="olympic-marathon-polynomial-17-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//ml/olympic-marathon-polynomial-17.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Fit of a 16 degree polynomial to the olympic marathon data.
</aside>
</section>
<section id="th-degree-polynomial-fit-2" class="slide level2">
<h2>26th Degree Polynomial Fit</h2>
<p><span class="math display">\[f(x, \mathbf{ w}) = w_0 + w_1 x+ w_2 x^2 + \dots + w_{26} x^{26}\]</span></p>
<div class="figure">
<div id="olympic-marathon-polynomial-27-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//ml/olympic-marathon-polynomial-27.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Fit of a 26 degree polynomial to the olympic marathon data.
</aside>
</section>
<section id="the-bootstrap" class="slide level2">
<h2>The Bootstrap</h2>
<p><span class="math display">\[
\mathbf{ y}, \mathbf{X}\sim \mathbb{P}(y, \mathbf{ x})
\]</span></p>
</section>
<section id="resample-dataset" class="slide level2">
<h2>Resample Dataset</h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bootstrap(X):</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;Return a bootstrap sample from a data set.&quot;</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> X.shape[<span class="dv">0</span>]</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    ind <span class="op">=</span> np.random.choice(n, n, replace<span class="op">=</span><span class="va">True</span>) <span class="co"># Sample randomly with replacement.</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> X[ind, :]</span></code></pre></div>
</section>
<section id="bootstrap-and-olympic-marathon-data" class="slide level2">
<h2>Bootstrap and Olympic Marathon Data</h2>
</section>
<section id="linear-fit-1" class="slide level2">
<h2>Linear Fit</h2>
<p><span class="math display">\[f(x, \mathbf{ w}) = w_0 + w_1 x\]</span></p>
<div class="figure">
<div id="olympic-marathon-bootstrap-polynomial-2-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//ml/olympic-marathon-bootstrap-polynomial-2.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Fit of a 1 degree polynomial (a linear model) to the olympic marathon data.
</aside>
</section>
<section id="cubic-fit-1" class="slide level2">
<h2>Cubic Fit</h2>
<p><span class="math display">\[f(x, \mathbf{ w}) = w_0 + w_1 x+ w_2 x^2 + w_{3} x^3\]</span></p>
<div class="figure">
<div id="olympic-marathon-bootstrap-polynomial-4-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//ml/olympic-marathon-bootstrap-polynomial-4.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Fit of a 3 degree polynomial (a cubic model) to the olympic marathon data.
</aside>
</section>
<section id="th-degree-polynomial-fit-3" class="slide level2">
<h2>9th Degree Polynomial Fit</h2>
<p><span class="math display">\[f(x, \mathbf{ w}) = w_0 + w_1 x+ w_2 x^2 + \dots + w_{9} x^{9}\]</span></p>
<div class="figure">
<div id="olympic-marathon-bootstrap-polynomial-10-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//ml/olympic-marathon-bootstrap-polynomial-10.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Fit of a 9 degree polynomial to the olympic marathon data.
</aside>
</section>
<section id="th-degree-polynomial-fit-4" class="slide level2">
<h2>16th Degree Polynomial Fit</h2>
<p><span class="math display">\[f(x, \mathbf{ w}) = w_0 + w_1 x+ w_2 x^2 + \dots + w_{16} x^{16}\]</span></p>
<div class="figure">
<div id="olympic-marathon-bootstrap-polynomial-17-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//ml/olympic-marathon-bootstrap-polynomial-17.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Fit of a 16 degree polynomial to the olympic marathon data.
</aside>
</section>
<section id="bias-variance-decomposition" class="slide level2">
<h2>Bias Variance Decomposition</h2>
<p>Generalisation error <span class="math display">\[\begin{align*}
R(\mathbf{ w}) = &amp; \int \left(y- f^*(\mathbf{ x})\right)^2 \mathbb{P}(y, \mathbf{ x}) \text{d}y\text{d}\mathbf{ x}\\
&amp; \triangleq \mathbb{E}\left[ \left(y- f^*(\mathbf{ x})\right)^2 \right].
\end{align*}\]</span></p>
</section>
<section id="decompose" class="slide level2">
<h2>Decompose</h2>
<p>Decompose as <span class="math display">\[
\begin{align*}
\mathbb{E}\left[ \left(y- f(\mathbf{ x})\right)^2 \right] = &amp; \text{bias}\left[f^*(\mathbf{ x})\right]^2 \\
&amp; + \text{variance}\left[f^*(\mathbf{ x})\right] \\ \\ &amp;+\sigma^2,
\end{align*}
\]</span></p>
</section>
<section id="bias" class="slide level2">
<h2>Bias</h2>
<ul>
<li><p>Given by <span class="math display">\[
\text{bias}\left[f^*(\mathbf{ x})\right] =
\mathbb{E}\left[f^*(\mathbf{ x})\right] - f(\mathbf{ x})
\]</span></p></li>
<li><p>Error due to bias comes from a model that’s too simple.</p></li>
</ul>
</section>
<section id="variance" class="slide level2">
<h2>Variance</h2>
<ul>
<li><p>Given by <span class="math display">\[
\text{variance}\left[f^*(\mathbf{ x})\right] = \mathbb{E}\left[\left(f^*(\mathbf{ x}) - \mathbb{E}\left[f^*(\mathbf{ x})\right]\right)^2\right].
\]</span></p></li>
<li><p>Slight variations in the training set cause changes in the prediction. Error due to variance is error in the model due to an overly complex model.</p></li>
</ul>
</section>
<section id="regularisation" class="slide level2">
<h2>Regularisation</h2>
<p>Linear system, solve:</p>
<p><span class="math display">\[
\designMatrix^\top\designMatrix \mathbf{ w}= \designMatrix^\top\mathbf{ y}
\]</span> But if <span class="math inline">\(\designMatrix^\top\designMatrix\)</span> then this is not well posed.</p>
</section>
<section id="tikhonov-regularisation" class="slide level2">
<h2>Tikhonov Regularisation</h2>
<ul>
<li>Updated objective: <span class="math display">\[
L(\mathbf{ w}) = (\mathbf{ y}- \mathbf{ f})^\top(\mathbf{ y}- \mathbf{ f}) + \alpha\left\Vert \mathbf{W} \right\Vert_2^2
\]</span></li>
<li>Hessian: <span class="math display">\[
\designMatrix^\top\designMatrix + \alpha \mathbf{I}
\]</span></li>
</ul>
</section>
<section id="splines-functions-hilbert-kernels" class="slide level2">
<h2>Splines, Functions, Hilbert Kernels</h2>
<ul>
<li>Can also regularise the function <span class="math inline">\(f(\cdot)\)</span> directly.</li>
<li>This approach taken in <em>splines</em> and <span class="citation" data-cites="Wahba:book90">Wahba (1990)</span> and kernels <span class="citation" data-cites="Scholkopf:learning01">Schölkopf and Smola (2001)</span>.</li>
<li>Mathematically more elegant, but algorithmically less flexible and harder to scale.</li>
</ul>
</section>
<section id="training-with-noise" class="slide level2">
<h2>Training with Noise</h2>
<ul>
<li>Other regularisation approaches such as <em>dropout</em> <span class="citation" data-cites="Srivastava-dropout14">(Srivastava et al., 2014)</span></li>
<li>Often perturbing the neural network structure or inputs.</li>
<li>Can have elegant interpretations (see e.g. <span class="citation" data-cites="Bishop:noise95">Bishop (1995)</span>)</li>
<li>Also interpreted as <em>ensemble</em> or <em>Bayesian</em> methods.</li>
</ul>
<!--include{_ml/includes/bayesian-interpretation-of-regularisation.md}-->
</section>
<section id="overparameterised-systems" class="slide level2">
<h2>Overparameterised Systems</h2>
<ul>
<li>Neural networks are highly overparameterised.</li>
<li>If we <em>could</em> examine their Hessian at “optimum”
<ul>
<li>Very low (or negative) eigenvalues.</li>
<li>Error function is not sensitive to changes in parameters.</li>
<li>Implies parmeters are <em>badly determined</em></li>
</ul></li>
</ul>
</section>
<section id="whence-generalisation" class="slide level2">
<h2>Whence Generalisation?</h2>
<ul>
<li>Not enough regularisation in our objective functions to explain.</li>
<li>Neural network models are <em>not</em> using traditional generalisation approaches.</li>
<li>The ability of these models to generalise <em>must</em> be coming somehow from the algorithm*</li>
<li>How to explain it and control it is perhaps the most interesting theoretical question for neural networks.</li>
</ul>
</section>
<section id="double-descent" class="slide level2">
<h2>Double Descent</h2>
<div class="figure">
<div id="double-descent-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/deepnn/./slides/diagrams//ml/double-descent.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
<em>Left</em> traditional perspective on generalisation. There is a sweet spot of operation where the training error is still non-zero. Overfitting occurs when the variance increases. <em>Right</em> The double descent phenomenon, the modern models operate in an interpolation regime where they reconstruct the training data fully, but are well regularised in their interpolations for test data. Figure from <span class="citation" data-cites="Belkin:reconciling19">Belkin et al. (2019)</span>.
</aside>
</section>
<section id="neural-tangent-kernel" class="slide level2">
<h2>Neural Tangent Kernel</h2>
<ul>
<li>Consider very wide neural networks.</li>
<li>Consider particular initialisation.</li>
<li>Deep neural network is regularising with a particular <em>kernel</em>.</li>
<li>This is known as the neural tangent kernel <span class="citation" data-cites="Jacot-ntk18">(Jacot et al., 2018)</span>.</li>
</ul>
</section>
<section id="regularisation-in-optimisation" class="slide level2">
<h2>Regularisation in Optimisation</h2>
<ul>
<li>Gradient flow methods allow us to study nature of optima.</li>
<li>In particular systems, with given initialisations, we can show L1 and L2 norms are minimised.</li>
<li>In other cases the rank of <span class="math inline">\(\mathbf{W}\)</span> is minimised.</li>
<li>Questions remain over the nature of this regularisation in neural networks.</li>
</ul>
</section>
<section id="deep-linear-models" class="slide level2">
<h2>Deep Linear Models</h2>
<p><span class="math display">\[
f(\mathbf{ x}; \mathbf{W}) = \mathbf{W}_4 \mathbf{W}_3 \mathbf{W}_2 \mathbf{W}_1 \mathbf{ x}.
\]</span></p>
<p><span class="math display">\[
\mathbf{W}= \mathbf{W}_4 \mathbf{W}_3 \mathbf{W}_2 \mathbf{W}_1 
\]</span></p>
</section>
<section id="thanks" class="slide level2 scrollable">
<h2 class="scrollable">Thanks!</h2>
<ul>
<li><p>twitter: <a href="https://twitter.com/lawrennd">@lawrennd</a></p></li>
<li><p>podcast: <a href="http://thetalkingmachines.com">The Talking Machines</a></p></li>
<li><p>newspaper: <a href="http://www.theguardian.com/profile/neil-lawrence">Guardian Profile Page</a></p></li>
<li><p>blog posts:</p>
<p><a href="http://inverseprobability.com/2014/07/01/open-data-science">Open Data Science</a></p></li>
</ul>
</section>
<section id="references" class="slide level2 scrollable">
<h2 class="scrollable">References</h2>
<p>bootstrap</p>
<p>David Hogg’s lecture <a href="https://speakerdeck.com/dwhgg/linear-regression-with-huge-numbers-of-parameters" class="uri">https://speakerdeck.com/dwhgg/linear-regression-with-huge-numbers-of-parameters</a></p>
<p>The Deep Bootstrap <a href="https://twitter.com/PreetumNakkiran/status/1318007088321335297?s=20" class="uri">https://twitter.com/PreetumNakkiran/status/1318007088321335297?s=20</a></p>
<p>Aki Vehtari on Leave One Out Uncertainty: <a href="https://arxiv.org/abs/2008.10296" class="uri">https://arxiv.org/abs/2008.10296</a> (check for his references).</p>
<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-Belkin:reconciling19" class="csl-entry" role="doc-biblioentry">
Belkin, M., Hsu, D., Ma, S., Soumik Mandal, and, 2019. Reconciling modern machine-learning practice and the classical bias-variance trade-off. Proc. Natl. Acad. Sci. USA 116, 15849–15854.
</div>
<div id="ref-Bishop:noise95" class="csl-entry" role="doc-biblioentry">
Bishop, C.M., 1995. Training with noise is equivalent to <span>T</span>ikhonov regularization. Neural Computation 7, 108–116. <a href="https://doi.org/10.1162/neco.1995.7.1.108">https://doi.org/10.1162/neco.1995.7.1.108</a>
</div>
<div id="ref-Jacot-ntk18" class="csl-entry" role="doc-biblioentry">
Jacot, A., Gabriel, F., Hongler, C., 2018. Neural tangent kernel: Convergence and generalization in neural networks, in: Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., Garnett, R. (Eds.), Advances in Neural Information Processing Systems. Curran Associates, Inc., pp. 8571–8580.
</div>
<div id="ref-Scholkopf:learning01" class="csl-entry" role="doc-biblioentry">
Schölkopf, B., Smola, A.J., 2001. Learning with kernels. mit, Cambridge, MA.
</div>
<div id="ref-Srivastava-dropout14" class="csl-entry" role="doc-biblioentry">
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., Salakhutdinov, R., 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research 15, 1929–1958.
</div>
<div id="ref-Wahba:book90" class="csl-entry" role="doc-biblioentry">
Wahba, G., 1990. Spline models for observational data, First. ed. SIAM. <a href="https://doi.org/10.1137/1.9781611970128">https://doi.org/10.1137/1.9781611970128</a>
</div>
</div>
</section>
    </div>
  </div>

  <script src="https://unpkg.com/reveal.js@3.9.2/lib/js/head.min.js"></script>
  <script src="https://unpkg.com/reveal.js@3.9.2/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Push each slide change to the browser history
        history: true,
        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // Optional reveal.js plugins
        dependencies: [
          { src: 'https://unpkg.com/reveal.js@3.9.2/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'https://unpkg.com/reveal.js@3.9.2/plugin/zoom-js/zoom.js', async: true },
          { src: 'https://unpkg.com/reveal.js@3.9.2/plugin/math/math.js', async: true },
          { src: 'https://unpkg.com/reveal.js@3.9.2/plugin/notes/notes.js', async: true }
        ]
      });
    </script>
    </body>
</html>
